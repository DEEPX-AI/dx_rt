{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":"<p>DXRT is DEEPX Runtime SDK for AI inference based on DEEPX devices. It supports pre-built models from DEEPX model zoo, and compiled models by DXCOM(DEEPX Compiler SDK).  </p>"},{"location":"index.html#supported>_devices","title":"Supported devices","text":"<p>DXRT provides common inference framework based on 2 kinds of inference mode.  </p> <ul> <li>Accelerator mode : inference by PCIe interface  </li> <li>Standalone mode : inference by direct AXI/APB interface  </li> </ul> Device Mode DX_L1 Standalone DX_L2 Standalone DX_M1 Accelerator DX_H1 Accelerator"},{"location":"index.html#resources","title":"Resources","text":""},{"location":"index.html#model>_zoo>_prebuilt>_models","title":"Model zoo (Prebuilt models)","text":"<p>Official modelzoo is in preparation.  </p>"},{"location":"index.html#documents","title":"Documents","text":"<p>Official website documents are in preparation. Currently, documents are provided with limited rights. Please consult with our contact point person. In other way, you can generate documents from repository (using markdown files in <code>docs</code>).  </p> <ul> <li>python&gt;=3.9 is needed. <pre><code># install MkDocs\npip install mkdocs mkdocs-material mkdocs-video pymdown-extensions\n# generate html to directory \"docs_generated\"\nmkdocs build\n</code></pre> You can also generate API reference using doxygen. <pre><code># install doxygen\nsudo apt install doxygen graphviz\n# generate API reference html\ncd docs/cpp_api\ndoxygen Doxyfile\n</code></pre></li> </ul>"},{"location":"API-Reference.html","title":"API Reference","text":""},{"location":"API-Reference.html#install>_doxygen","title":"Install doxygen","text":"<p>Please ignore everything here. This version does not support doxygen yet. </p><pre><code>sudo apt install doxygen graphviz\n</code></pre>"},{"location":"API-Reference.html#generate>_api>_reference","title":"Generate API reference","text":"<pre><code>cd docs/cpp_api\ndoxygen Doxyfile\n</code></pre>"},{"location":"Build.html","title":"Build","text":""},{"location":"Build.html#compile>_definitions","title":"Compile Definitions","text":"<p>You can set global compile definitions in build.cfg to use in your application(It doesn't affect to DXRT libraries). Following means, <code>#define EXAMPLE_FLAG 1</code> in compile time. </p><pre><code>EXAMPLE_FLAG=1\n</code></pre>"},{"location":"Build.html#options","title":"Options","text":"<p>You can configure DXRT options in cmake/dxrt.cfg.cmake <code>USE_ORT</code> : Use ONNX runtime for NN ops that NPU does not support. <code>USE_DXRT_TEST</code> : (applicable for only DEEPX-internal version) Build DXRT Unit test. Prebult googletest libraries are linked if this option is <code>ON</code>. <code>USE_PYTHON</code> : Enable python RT.  </p> <pre><code>option(USE_ORT \"Use ONNX Runtime\" OFF)\noption(USE_DXRT_TEST \"Use DXRT Unit Test\" ON)\noption(USE_SHARED_DXRT_LIB \"Build for DXRT Shared Library\" ON)\noption(USE_PYTHON \"Use Python\" OFF)\n</code></pre>"},{"location":"Build.html#cross-compile>_setup","title":"Cross-compile Setup","text":"<p>DXRT supports 3 kinds of CPU Architecture - arm64, riscv64, x86_64. Set compiler toolchain path in <code>cmake/toolchain.&lt;CMAKE_SYSTEM_PROCESSOR&gt;.cmake</code> Example : arm64 cross-compile </p><pre><code>SET(CMAKE_C_COMPILER      /usr/bin/aarch64-linux-gnu-gcc )\nSET(CMAKE_CXX_COMPILER    /usr/bin/aarch64-linux-gnu-g++ )\nSET(CMAKE_LINKER          /usr/bin/aarch64-linux-gnu-ld  )\nSET(CMAKE_NM              /usr/bin/aarch64-linux-gnu-nm )\nSET(CMAKE_OBJCOPY         /usr/bin/aarch64-linux-gnu-objcopy )\nSET(CMAKE_OBJDUMP         /usr/bin/aarch64-linux-gnu-objdump )\nSET(CMAKE_RANLIB          /usr/bin/aarch64-linux-gnu-ranlib )\n</code></pre> Example : riscv64 cross-compile <pre><code>set(CMAKE_C_COMPILER /usr/bin/riscv64-linux-gnu-gcc)\nset(CMAKE_CXX_COMPILER /usr/bin/riscv64-linux-gnu-g++)\n</code></pre>"},{"location":"Build.html#3rd>_party>_libraries","title":"3rd Party Libraries","text":"<ol> <li>googleprotobuf, googletest     Prebuilt files are provided in <code>extern/include</code>, <code>extern/lib</code> directory. If you want to use your custom files, overwrite them.  </li> <li>OpenCV     Prebuilt files are not provided. CMake <code>find_package()</code> will find OpenCV packages automatically from your system. If you want to set specific OpenCV package path, configure following at <code>cmake/toolchain.&lt;CMAKE_SYSTEM_PROCESSOR&gt;.cmake</code> - just set the path which includes <code>OpenCVConfig.cmake</code> file.  <pre><code>set(OpenCV_DIR &lt;opencv build dir.&gt; )\n</code></pre> Example : OpenCV 4.5.5 Cross-compile for arm64 <pre><code>wget -O opencv.4.5.5.zip https://github.com/opencv/opencv/archive/4.5.5.zip \nwget -O opencv_contrib.4.5.5.zip https://github.com/opencv/opencv_contrib/archive/4.5.5.zip\nunzip opencv.4.5.5.zip\nunzip opencv_contrib.4.5.5.zip\ncd opencv-4.5.5\nmkdir build_arm64 ; cd build_arm64\ncmake -D CMAKE_TOOLCHAIN_FILE=../platforms/linux/aarch64-gnu.toolchain.cmake -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-4.5.5/modules -D CMAKE_INSTALL_PREFIX=../install_arm64 -DCMAKE_CXX_COMPILER=/usr/bin/aarch64-linux-gnu-g++ -DCMAKE_C_COMPILER=/usr/bin/aarch64-linux-gnu-gcc ../\nmake -j8\nsudo make install\n</code></pre> Example : OpenCV 4.5.5 Cross-compile for riscv64 <pre><code>wget -O opencv.4.5.5.zip https://github.com/opencv/opencv/archive/4.5.5.zip \nwget -O opencv_contrib.4.5.5.zip https://github.com/opencv/opencv_contrib/archive/4.5.5.zip\nunzip opencv.4.5.5.zip\nunzip opencv_contrib.4.5.5.zip\ncd opencv-4.5.5\nmkdir build_riscv64 ; cd build_riscv64\ncmake -D CMAKE_TOOLCHAIN_FILE=../platforms/linux/riscv64-gnu.toolchain.cmake -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-4.5.5/modules -D CMAKE_INSTALL_PREFIX=../install_riscv64 -DCMAKE_CXX_COMPILER=riscv64-linux-gnu-g++ -DCMAKE_C_COMPILER=riscv64-linux-gnu-gcc ../\nmake -j8\nsudo make install\n</code></pre> You can install opencv using <code>install.sh</code>, as followings. OpenCV 4.5.5 version will be installed in <code>extern/${cpu_arch}</code>.    <pre><code>./install.sh --arch riscv64 --opencv\n./install.sh --arch arm64 --opencv\n</code></pre></li> </ol>"},{"location":"Build.html#build>_guide","title":"Build guide","text":""},{"location":"Build.html#non-cross-compile>_case","title":"non-cross-compile case","text":"<p>To build DXRT for the host system, execute the build script with the appropriate installation path. It is recommended to set the installation path to a location commonly referenced in the OS, such as <code>/usr/local</code>. </p><pre><code>./build.sh --install /usr/local\n</code></pre>"},{"location":"Build.html#cross-compile>_case","title":"cross-compile case","text":"<p>To cross-compile DXRT for a target system, target CPU architecture should be configured. (If not specified, target CPU will be set to the host system's CPU) </p><pre><code>./build.sh --arch &lt;target_cpu&gt;\n</code></pre> <p>Please refer to following options. </p><pre><code>./build.sh [ options ]\n    --help     show this help\n    --clean    clean build\n    --verbose  show build commands\n    --type     cmake build type : [ Release, Debug, RelWithDebInfo ]\n    --arch     target CPU architecture : [ x86_64, arm64, riscv64 ]\n    --install  install path\n    --uninstall  uninstall dx-rt files\n</code></pre> Examples: <pre><code>./build.sh --arch arm64\n./build.sh --arch riscv64\n./build.sh --arch x86_64\n</code></pre> Build directories and related output files will be generated according to the target CPU (e.g., <code>build_arm64</code>, <code>build_riscv64</code>, ...). Demo application binary files can be found in <code>&lt;build directory&gt;</code>/release/bin/, and <code>bin</code>. <pre><code>&lt;build directory&gt;/release/bin/\n\u251c\u2500\u2500 cls_demo_bin\n\u251c\u2500\u2500 ddrnet\n\u251c\u2500\u2500 dxrt_test\n\u251c\u2500\u2500 face_recognition\n\u251c\u2500\u2500 helloworld\n\u251c\u2500\u2500 parse_model\n\u251c\u2500\u2500 pidnet\n\u251c\u2500\u2500 run_model\n\u251c\u2500\u2500 ssd\n\u251c\u2500\u2500 yolo\n\u251c\u2500\u2500 yolo_ddrnet\n\u2514\u2500\u2500 yolo_pidnet\n</code></pre>"},{"location":"Examples.html","title":"Examples","text":"<p>Please find our demo applications implemented based on the Inference Guide.</p>"},{"location":"Examples.html#parse>_model","title":"Parse Model","text":"<p>Parse model, and show detailed model information (from <code>app/basic/parse_model.cpp</code>)  </p> <p>-m, --model     model path  -h, --help      show help </p><pre><code>parse_model -m &lt;model_dir&gt;\n</code></pre> <pre><code>$./parse_model -m yolov5s\nmodelPath: yolov5s\nparse non-encryption format.\ncompiler version info : v0.5_transpose, 1.0, 0.0\nmodel name : yolov5s_512\nnumber of macs : 4096\nnumber of layers : 71\ntotal memory usage : 34628608 Bytes\n  - weight param. : 7342592 Bytes\n  - npu param. : 219648 Bytes\n  - input : 786432 Bytes\n  - output : 26279936 Bytes\ninput tensors\n  type : IM2COL\n  [0] \"INPUT\", UINT32, [1, 3, 512, 512, ], 786432 Bytes\noutput tensors\n  [0] \"378\", FLOAT, [64, 64, 255, ], 4194304 Bytes\n  [1] \"439\", FLOAT, [32, 32, 255, ], 1048576 Bytes\n  [2] \"500\", FLOAT, [16, 16, 255, ], 262144 Bytes\n</code></pre>"},{"location":"Examples.html#run>_model","title":"Run Model","text":"<p>Simple model run demo, which measure inference time, and check output data integrity (from <code>app/basic/run_model.cpp</code>)  </p> <p>-c, --config    config json file  -m, --model     model path  -i, --input     input data file  -o, --output    file to save output data  -r, --ref       reference output data file to compare  -l, --loop      loop test  -h, --help      show help </p><pre><code>run_model -m &lt;model_dir&gt; -i &lt;input bin.&gt; -o &lt;output bin.&gt; -r &lt;reference output bin.&gt; -l &lt;number of loops&gt;\n-----------------------------------\nloops : 100\n-----------------------------------\n...\n...\n-----------------------------------\n  Inference time : 13.872ms\n  FPS : 72.0879\n  Bit match test : SKIP\n-----------------------------------\n</code></pre> Multiple model run can be configured by using config json file. <pre><code>run_model -c &lt;json file&gt;\n</code></pre> Refer to following json example. <pre><code>{\n  \"models\": [\n    {\n      \"model\": {\n        \"path\": \"/dxrt/m1a/1705/yolov7_640_ppu\",\n        \"data\": [\n          {\n            \"input\": \"/dxrt/m1a/1705/yolov7_640_ppu/npu_input_0.bin\",\n            \"output\": \"/dxrt/m1a/1705/yolov7_640_ppu/npu_output_0.ppu.bin.rt\",\n            \"refoutput\": \"/dxrt/m1a/1705/yolov7_640_ppu/npu_output_0.ppu.bin\"\n          },\n          {\n            \"input\": \"/dxrt/m1a/1705/yolov7_640_ppu/npu_input_1.bin\",\n            \"output\": \"/dxrt/m1a/1705/yolov7_640_ppu/npu_output_1.ppu.bin.rt\",\n            \"refoutput\": \"/dxrt/m1a/1705/yolov7_640_ppu/npu_output_1.ppu.bin\"\n          }\n        ]\n      }\n    },\n    {\n      \"model\": {\n        \"path\": \"/dxrt/m1a/1705/yolov4\",\n        \"data\": [\n          {\n            \"input\": \"/dxrt/m1a/1705/yolov4/npu_input_0.bin\",\n            \"output\": \"/dxrt/m1a/1705/yolov4/npu_output_0.bin.rt\",\n            \"refoutput\": \"/dxrt/m1a/1705/yolov4/npu_output_0.bin\"\n          }\n        ]\n      }\n    },\n    {\n      \"model\": {\n        \"path\": \"/dxrt/m1a/1705/eyenix_ssdlite320\"\n      }\n    }\n  ],\n  \"loops_benchmark\": 30,\n  \"loops_datacheck\": 20,\n  \"loops_aging\": 50\n}\n</code></pre>"},{"location":"Examples.html#firmware>_interface>_dxrt-cli>_tool","title":"Firmware Interface DXRT-CLI tool","text":"<p>Read device status, and handle them by commandline interface (only for accelerator device)  </p> <p>-s, --status    get chip status  -i, --info      get device information  -r, --reset     reset chip  -d, --dump      dump device memory  -h, --help      show help </p><pre><code>dxrt-cli --status\n...\n...\nNPU0, DX_M1_4K @ 1000MHz, 850mV, 43'C\nNPU1, DX_M1_8K @ 666MHz, 750mV, 44'C\n</code></pre> <pre><code>dxrt-cli --reset\n...\n...\nReset NPU0\nReset NPU1\n</code></pre>"},{"location":"Examples.html#image>_classification","title":"Image Classification","text":"<p>Classification Demo for details, see <code>app/classification/classification.cpp</code> </p> <p>-m, --model     define model path   -i, --image     use image file input   -a, --async     asynchronous inference   -l, --loop      number of loops to repeat   -h, --help      show help </p><pre><code>classification -m &lt;model_dir&gt; -i &lt;image_dir&gt; \n</code></pre> <pre><code>$ sudo ./classification -m /dxrt/models/efficientnet-b0_argmax/ -b efficientnet.input.0.bin  \n\nmodelPath: /dxrt/models/efficientnet-b0_argmax/\nbinFile: efficientnet.input.0.bin\nasyncInference: 0\nmode: 0\nFound device : /dev/dx_dma0_npu_0\nDetected device id : b1000705\nFound device : /dev/dx_dma0_npu_1\nDetected device id : b1000705\n==== Accelerator mode ====\n  Task[0] /dxrt/models/efficientnet-b0_argmax/ NPUs, 8976640bytes\n            Device : NPU 0, /dev/dx_dma0_npu_0(IDLE), DX_M1_4K @ 1000MHz, PCIE type\n              - MemIf : /dev/dx_dma0_h2c_0, /dev/dx_dma0_c2h_0, c01000000(0) + 5f800000\n    [0]\n      inputs\n        Tensor: name INPUT, type UINT8 [1, 224, 224, 3,  ], data 0x556d114826a0, dest 0\n      outputs\n        Tensor: name argmax_output, type UINT16 [ ], data 0x556d11482460, dest 0\nTop1 Result : class 809\n</code></pre>"},{"location":"Examples.html#object>_detection","title":"Object Detection","text":"<p>YOLO Demo (from <code>app/object_detection/yolo_demo.cpp</code>) SSD Demo (from <code>app/object_detection/ssd_demo.cpp</code>)  </p> <p>-m, --model     define model path   -i, --image     use image file input   -v, --video     use video file input   -w, --write     write result frames to a video file   -c, --camera    use camera input   -x, --isp       use ISP input   -b, --bin       use binary file input   -s, --sim       use pre-defined npu output binary file input( perform post-proc. only )   -a, --async     asynchronous inference   -e, --ethernet  use ethernet input   -p, --param      pre/post-processing parameter selection   -l, --loop      loop test   -n, --numbuf    number of memory buffers for inference   -h, --help      show help  </p> <p>If the inference time is longer than the input frame rate, the incorrect result can be displayed.     In this case, increase the size of the buffer using <code>-n</code> option.(e.g. <code>-n 2</code>) </p>"},{"location":"Examples.html#prepost-processing>_parameters","title":"Pre/post-processing parameters","text":"<p>Pre/post-processing parameters not included in compiled model are required to execute the object detection application. Example parameters are described in <code>*_cfg.cpp</code>, and they all are listed as followings in <code>*_demo.cpp</code>. From the list, you can select what parameter to use in runtime by using <code>-p</code> argument. To configure your own parameters, you can modify examples, or append a new one to the list. </p><pre><code>// pre/post parameter table\nYoloParam yoloParams[] = {\n    [0] = yolov5s_320,\n    [1] = yolov5s_512,\n    [2] = yolox_s_512,\n    [3] = yolov5s_512_concat,\n    [4] = yolov7_640,\n    [5] = yolov7_512\n};\n</code></pre> <pre><code>// pre/post parameter table\nSsdParam ssdParams[] = {\n    [0] = mv1_ssd_300,\n    [1] = mv2_ssd_320,\n    [2] = mv1_ssd_512,\n};\n</code></pre> <pre><code>YoloParam yolov5s_512 = {\n    .imgSize = 512,\n    .confThreshold = 0.25,\n    .scoreThreshold = 0.3,\n    .iouThreshold = 0.4,\n    .numClasses = 80,\n    .layers = {\n        {\n            .numGridX = 64,\n            .numGridY = 64,\n            .numBoxes = 3,\n            .anchorWidth = { 10.0, 16.0, 33.0 },\n            .anchorHeight = { 13.0, 30.0, 23.0 },\n            .tensorIdx = { 0 },\n        },\n    ...\n    ...\n};\n</code></pre> <ul> <li> <p>object detection for single image </p><pre><code>./yolo -m &lt;model_dir&gt; -i &lt;image file&gt; -p &lt;param. index&gt;\n./ssd -m &lt;model_dir&gt; -i &lt;image file&gt; -p &lt;param. index&gt;\n</code></pre> For example, if you want to execute compiled YOLO model <code>A</code> with parameter <code>yolov5s_512</code>, <pre><code>./yolo -m A -i sample/1.jpg -p 1\n</code></pre> For example, if you want to execute compiled SSD model <code>B</code> with parameter <code>mv1_ssd_300</code>, <pre><code>./ssd -m B -i sample/1.jpg -p 0\n</code></pre> </li> <li> <p>object detection for single video file </p><pre><code>./yolo -m &lt;model_dir&gt; -v &lt;video file&gt; -a -p &lt;param. index&gt;\n./ssd -m &lt;model_dir&gt; -v &lt;video file&gt; -a -p &lt;param. index&gt;\n</code></pre> </li> <li>object detection for camera input (by OpenCV VideoCapture) <pre><code>./yolo -m &lt;model_dir&gt; -c -a -p &lt;param. index&gt;\n./ssd -m &lt;model_dir&gt; -c -a -p &lt;param. index&gt;\n</code></pre></li> <li> <p>multi channel object detection for camera and video file input (must supported OpenCV Library)    support up to 4 video sources </p><pre><code>./yolo_multi -c &lt;config_json_file&gt;\n</code></pre> <pre><code>$ ./yolo_multi_camera -h\n  yolo demo  \n  -c, --config    use config json file for run application\n                  e.g. sudo yolo_multi -c _multi_od_.json\n  -n, --numbuf    number of memory buffers for inference\n  -h, --help      show help\n</code></pre> Refer to following configuration json file. <code>test_od_demo.json</code> <pre><code>{\n    \"usage\": \"multi\",\n    \"model_path\": \"/model_path/yolov5s_512\",\n    \"model_name\": \"yolov5s_512\",\n\n    \"video_sources\": [\n                    [\"/video_path/video_sample0.mp4\", \"realtime\"],\n                    [\"/dev/video0\", \"camera\"],\n                    [\"/dev/video2\", \"camera\"],\n                    [\"/video_path/video_sample1.mp4\", \"offline\", 400]\n    ], \n\n    \"display_config\": {\n                    \"display_label\": \"output\",\n                    \"output_width\": 1920,\n                    \"output_height\": 1080,\n                    \"show_fps\": true\n    }\n}\n</code></pre> Set \"model_name\" for mapping pre/post-processing parameters. : <pre><code>// pre/post parameter table\nYoloParam yoloParams[] = {\n    [0] = yolov5s_320,\n    [1] = yolov5s_512, \n    [2] = yolox_s_512,\n    [3] = yolov5s_512_concat,\n    [4] = yolov7_640,\n    [5] = yolov7_512\n};\n</code></pre> <pre><code>...\n    \"model_path\": \"/model_path/yolov5s_512\",\n    \"model_name\": \"yolov5s_512\",\n...   \n</code></pre> \"realtime\" for real-time capture video frames. \"offline\" for pre-saved frames to reducing latency of capture.   Optional for \"offline\" next value, indicating number of frames to pre-saved. If you want to all frames of video, you can skip the value. :   <pre><code>...\n                    [\"/video_path/video_sample0.mp4\", \"realtime\"],\n                    [\"/video_path/video_sample1.mp4\", \"offline\", 400],\n                    [\"/video_path/video_sample2.mp4\", \"offline\"]\n...   \n</code></pre> Camera devices should have the source type set to \"camera\". :   <pre><code>...\n                    [\"/dev/video0\", \"camera\"],\n                    [\"/dev/video2\", \"camera\"],\n...\n</code></pre> Set display windows label and display resolution. :       <pre><code>...\n    \"display_config\": {\n                    \"display_label\": \"output\",\n                    \"output_width\": 1920,\n                    \"output_height\": 1080,\n                    \"show_fps\": true\n    }\n...\n</code></pre> </li> <li> <p>object detection for ISP input (by standalone device ISP) This is only available for standalone devices (e.g. DX-L1, DX_L2) </p><pre><code>./yolo -m &lt;model_dir&gt; -x -l 10000000 -p &lt;param. index&gt;\n</code></pre> </li> <li>object detection for ethernet input In this mode, device will work as a inference server by TCP/IP interface. So, \"od_client\" application should be executed from client side. Please refer to following. Server IP address is configured by '-p' option, and port number is set to 8080 currently. <pre><code>Server side\n./yolo -m &lt;model_dir&gt; -e -a -p &lt;param. index&gt;\n</code></pre> <pre><code>Client side\n./od_client -m &lt;model_dir&gt; -c -p &lt;IP address of server&gt;\n./od_client -m &lt;model_dir&gt; -v &lt;video file&gt; -p &lt;IP address of server&gt;\n</code></pre></li> <li>object detection for single binary file <pre><code>./yolo -m &lt;model_dir&gt; -b &lt;binary file&gt; -p &lt;param. index&gt;\n./ssd -m &lt;model_dir&gt; -b &lt;binary file&gt; -p &lt;param. index&gt;\n</code></pre> <pre><code>./yolo -m /dxrt/models/DX_M1/v2p0/yolov5s -i sample/1.jpg -p 1\nmodelPath: /dxrt/models/DX_M1/v2p0/yolov5s\nvideoFile:\nimgFile: sample/1.jpg\nbinFile:\nsimFile:\ncameraInput: 0\nasyncInference: 0\nmode: 0\nparse non-encryption format.\n            Device : NPU 0(0x55ae03311cc0), 1\n----------------------------------------------\nPCIE device 0 : /dev/xdma0_user, /dev/xdma0_c2h_0, /dev/xdma0_h2c_0\ncharacter device /dev/xdma0_user opened.\nMemory mapped at address 0x7f88a8dd7000.\n        Task[0] @ 2106400 (/dxrt/models/DX_M1/v2p0/yolov5s) : 1 NPUs, 34628608bytes\n          datainfo : rmap, filename /dxrt/models/DX_M1/v2p0/yolov5s/rmap.bin, type NONE [ ]\n          datainfo : weight, filename /dxrt/models/DX_M1/v2p0/yolov5s/weight.bin, type NONE [ ]\n            Device : NPU 0(0x55ae03311cc0), 1\n            input tensors\n              Tensor: name INPUT, type UINT32 [1, 3, 512, 512,  ], data 0x55ae03e6bec0, dest 0\n              Tensor: name INPUT, type UINT32 [1, 3, 512, 512,  ], data 0x55ae049ac480, dest 0\n            output tensors\n              Tensor: name 378, type FLOAT [64, 64, 255,  ], data 0x55ae0392bc10, dest 0\n              Tensor: name 439, type FLOAT [32, 32, 255,  ], data 0x55ae03d2bc20, dest 0\n              Tensor: name 500, type FLOAT [16, 16, 255,  ], data 0x55ae03e2bd70, dest 0\n              Tensor: name 378, type FLOAT [64, 64, 255,  ], data 0x55ae0446c090, dest 0\n              Tensor: name 439, type FLOAT [32, 32, 255,  ], data 0x55ae0486c1e0, dest 0\n              Tensor: name 500, type FLOAT [16, 16, 255,  ], data 0x55ae0496c330, dest 0\nYOLO created : 16128 boxes, 80 classes, 3 layers.\n  YoloParam:\n    - conf_threshold: 0.25, score_threshold: 0.3, iou_threshold: 0.4, num_classes: 80, num_layers: 3\n    - AnchorBoxParam: [ 16 x 16 x 3], width [116, 156, 373, ], height [90, 198, 326, ]\n    - AnchorBoxParam: [ 64 x 64 x 3], width [10, 16, 33, ], height [13, 30, 23, ]\n    - AnchorBoxParam: [ 32 x 32 x 3], width [30, 62, 59, ], height [61, 45, 119, ]\n    - classes: [person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, trafficlight, firehydrant, stopsign, parkingmeter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sportsball, kite, baseballbat, baseballglove, skateboard, surfboard, tennisracket, bottle, wineglass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hotdog, pizza, donut, cake, chair, couch, pottedplant, bed, diningtable, toilet, tv, laptop, mouse, remote, keyboard, cellphone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddybear, hairdrier, toothbrush, ]\n  OutputLayer: 64x64, 3 boxes, data align 0, data offset 0x0\n  OutputLayer: 32x32, 3 boxes, data align 0, data offset 0x0\n  OutputLayer: 16x16, 3 boxes, data align 0, data offset 0x0\n  OutputLayer: 64x64, 3 boxes, data align 0, data offset 0x0\n  OutputLayer: 32x32, 3 boxes, data align 0, data offset 0x0\n  OutputLayer: 16x16, 3 boxes, data align 0, data offset 0x0\n  Detected 10 boxes.\n    BBOX:person(0) 0.871739, (307.538, 139.761, 401.063, 363.528)\n    BBOX:bowl(45) 0.753045, (25.5786, 359.366, 78.6683, 392.692)\n    BBOX:bowl(45) 0.736314, (46.0351, 315.148, 107.061, 346.346)\n    BBOX:oven(69) 0.6947, (-0.0565262, 223.763, 153.919, 325.907)\n    BBOX:person(0) 0.635725, (-0.137587, 294.905, 48.2289, 331.544)\n    BBOX:bowl(45) 0.622758, (-0.811382, 328.491, 69.9861, 380.01)\n    BBOX:oven(69) 0.583807, (391.249, 245.999, 494.725, 359.424)\n    BBOX:bottle(39) 0.454198, (173.13, 269.961, 198.605, 321.687)\n    BBOX:pottedplant(58) 0.440339, (-0.421099, 86.0271, 51.0569, 206.691)\n    BBOX:bowl(45) 0.375233, (123.547, 219.058, 146.488, 233.043)\n[*** Name :                           main| min :        36222us| max :        36222us| avg :        36222us ***]\n*Max List* [36222,]\n[*** Name :                           post| min :          259us| max :          259us| avg :          259us ***]\n*Max List* [259,]\n[*** Name :                            pre| min :         1431us| max :         1431us| avg :         1431us ***]\n*Max List* [1431,]\n</code></pre> </li> </ul>"},{"location":"Examples.html#pose>_estimation","title":"Pose estimation","text":"<p>Pose estimation Demo with YOLO (from <code>app/pose_estimation/pose_demo.cpp</code>)  </p> <p>-m, --model     define model path   -i, --image     use image file input   -v, --video     use video file input   -w, --write     write result frames to a video file   -c, --camera    use camera input   -x, --isp       use ISP input   -b, --bin       use binary file input   -s, --sim       use pre-defined npu output binary file input( perform post-proc. only )   -a, --async     asynchronous inference   -e, --ethernet  use ethernet input   -p, --param      pre/post-processing parameter selection   -l, --loop      loop test   -n, --numbuf    number of memory buffers for inference   -h, --help      show help  </p>"},{"location":"Examples.html#prepost-processing>_parameters_1","title":"Pre/post-processing parameters","text":"<p>Pre/post-processing parameters follows same convention as in object detection.  </p> <ul> <li>pose estimation for single image <pre><code>./pose -m &lt;model_dir&gt; -i &lt;image file&gt; -p &lt;param. index&gt;\n</code></pre> For example, if you want to execute compiled YOLO model <code>A</code> with parameter <code>yolov5s_512</code>, <pre><code>./pose -m A -i sample/8.jpg -p 1\n</code></pre></li> <li>pose estimation for single video file <pre><code>./pose -m &lt;model_dir&gt; -v &lt;video file&gt; -a -p &lt;param. index&gt;\n</code></pre></li> <li>pose estimation for camera input (by OpenCV VideoCapture) <pre><code>./pose -m &lt;model_dir&gt; -c -a -p &lt;param. index&gt;\n</code></pre></li> <li>pose estimation for ISP input (by standalone device ISP) This is only available for standalone devices (e.g. DX-L1, DX_L2) <pre><code>./pose -m &lt;model_dir&gt; -x -l 10000000 -p &lt;param. index&gt;\n</code></pre> </li> </ul>"},{"location":"Examples.html#image>_segmentation","title":"Image Segmentation","text":"<p>DDRNet demo (from <code>app/segmentation/ddrnet.cpp</code>) PIDNet demo (from <code>app/segmentation/pidnet.cpp</code>)  </p> <p>-m, --model     define model path   -i, --image     use image file input   -v, --video     use video file input   -c, --camera    use camera input   -b, --bin       use binary file input   -s, --sim       use pre-defined npu output binary file input( perform post-proc. only )   -a, --async     asynchronous inference   -o, --iomode    I/O only mode (not perform inference directly)   -p, --pcie      Standalone mode: use pcie input   -l, --loop      number of loops to repeat   -h, --help      show help  </p> <ul> <li>segmentation for single image <pre><code>./ddrnet -m &lt;model_dir&gt; -i &lt;image file&gt;\n./pidnet -m &lt;model_dir&gt; -i &lt;image file&gt;\n</code></pre></li> <li>segmentation for single video file <pre><code>./ddrnet -m &lt;model_dir&gt; -v &lt;video file&gt; -a\n./pidnet -m &lt;model_dir&gt; -v &lt;video file&gt; -a\n</code></pre></li> <li>segmentation for camera input <pre><code>./ddrnet -m &lt;model_dir&gt; -c -a\n./pidnet -m &lt;model_dir&gt; -c -a\n</code></pre></li> <li>segmentation for single binary file <pre><code>./ddrnet -m &lt;model_dir&gt; -b &lt;binary file&gt;\n./pidnet -m &lt;model_dir&gt; -b &lt;binary file&gt;\n</code></pre> <pre><code>./pidnet -m /dxrt/models/DX_M1/v2p0/pidnet -i sample/7.png\ninputWidth: 1024\ninputHeight: 512\nmodelPath: /dxrt/models/DX_M1/v2p0/pidnet\nvideoFile:\nbinFile:\nsimFile:\ncameraInput: 0\nasyncInference: 0\nmode: 0\n            Device : NPU 0(0x55955bf416f0), 1\n----------------------------------------------\nPCIE device 0 : /dev/xdma0_user, /dev/xdma0_c2h_0, /dev/xdma0_h2c_0\ncharacter device /dev/xdma0_user opened.\nMemory mapped at address 0x7f1a6a535000.\n            Device : NPU 1(0x55955bf281e0), 1\n----------------------------------------------\nPCIE device 1 : /dev/xdma1_user, /dev/xdma1_c2h_0, /dev/xdma1_h2c_0\ncharacter device /dev/xdma1_user opened.\nMemory mapped at address 0x7f1a4de01000.\n        Task[0] @ 19eb0840 (/dxrt/models/DX_M1/v2p0/pidnet) : 2 NPUs, 69975552bytes\n          datainfo : rmap, filename /dxrt/models/DX_M1/v2p0/pidnet/rmap.bin, type NONE [ ]\n          datainfo : weight, filename /dxrt/models/DX_M1/v2p0/pidnet/weight.bin, type NONE [ ]\n            Device : NPU 0(0x55955bf416f0), 1\n            Device : NPU 1(0x55955bf281e0), 1\n            input tensors\n              Tensor: name INPUT, type UINT32 [1, 3, 512, 1024,  ], data 0x55955c26bfc0, dest 0\n              Tensor: name INPUT, type UINT32 [1, 3, 512, 1024,  ], data 0x55955c5ec2e0, dest 0\n              Tensor: name INPUT, type UINT32 [1, 3, 512, 1024,  ], data 0x55955c96c600, dest 0\n              Tensor: name INPUT, type UINT32 [1, 3, 512, 1024,  ], data 0x55955ccec920, dest 0\n              Tensor: name INPUT, type UINT32 [1, 3, 512, 1024,  ], data 0x55955d06cc40, dest 0\n              Tensor: name INPUT, type UINT32 [1, 3, 512, 1024,  ], data 0x55955d3ecfe0, dest 0\n              Tensor: name INPUT, type UINT32 [1, 3, 512, 1024,  ], data 0x55955d76d300, dest 0\n              Tensor: name INPUT, type UINT32 [1, 3, 512, 1024,  ], data 0x55955daed620, dest 0\n            output tensors\n              Tensor: name argmax_output, type UINT16 [1, 512, 1024,  ], data 0x55955c16be70, dest 0\n              Tensor: name argmax_output, type UINT16 [1, 512, 1024,  ], data 0x55955c4ec190, dest 0\n              Tensor: name argmax_output, type UINT16 [1, 512, 1024,  ], data 0x55955c86c4b0, dest 0\n              Tensor: name argmax_output, type UINT16 [1, 512, 1024,  ], data 0x55955cbec7d0, dest 0\n              Tensor: name argmax_output, type UINT16 [1, 512, 1024,  ], data 0x55955cf6caf0, dest 0\n              Tensor: name argmax_output, type UINT16 [1, 512, 1024,  ], data 0x55955d2ece90, dest 0\n              Tensor: name argmax_output, type UINT16 [1, 512, 1024,  ], data 0x55955d66d1b0, dest 0\n              Tensor: name argmax_output, type UINT16 [1, 512, 1024,  ], data 0x55955d9ed4d0, dest 0\noutputs.size(): 1\n              Tensor: name argmax_output, type UINT16 [1, 512, 1024,  ], data 0x55955c16be70, dest 0\n1024x512 &lt;- 676x338\n[*** Name :                           main| min :        71527us| max :        71527us| avg :        71527us ***]\n*Max List* [71527,]\n[*** Name :                     post-blend| min :          490us| max :          490us| avg :          490us ***]\n*Max List* [490,]\n[*** Name :                   post-segment| min :          413us| max :          413us| avg :          413us ***]\n*Max List* [413,]\n[*** Name :                            pre| min :         1548us| max :         1548us| avg :         1548us ***]\n*Max List* [1548,]\n</code></pre> </li> </ul>"},{"location":"Examples.html#face>_detectionrecognition","title":"Face Detection/Recognition","text":"<p>Face Recognition Demo Including Face Detection (<code>app/face_recognition/face_recognition_demo.cpp</code>)   </p> <p>-t, --threshold        Similarity Threshold  -m0, --fd_modelpath    face detection model include path  -m1, --lm_modelpath    face align model include path  -m2, --fr_modelpath    face recognition model include path  -p, --dbpath           face database directory  -l, --left             first image file to compare  -r, --right            second image file to compare  -i, --image            use image file input  -v, --video            use video file input  -c, --camera           use camera input  -a, --async            asynchronous inference  -d, --detect           include face detection (for dual image test)  -h, --help             show help  </p> <ul> <li>face recognition for single image <pre><code>./face_recognition -m0 &lt;fd_model_dir&gt; -m1 &lt;lm_model_dir&gt; -m2 &lt;fr_model_dir&gt; -p &lt;Face DB dir&gt; -i sample.jpg\n</code></pre></li> <li>face recognition for single video file <pre><code>./face_recognition -m0 &lt;fd_model_dir&gt; -m1 &lt;lm_model_dir&gt; -m2 &lt;fr_model_dir&gt; -p &lt;Face DB dir&gt; -v sample.mp4 -a\n</code></pre></li> <li>face recognition for camera input <pre><code>./face_recognition -m0 &lt;fd_model_dir&gt; -m1 &lt;lm_model_dir&gt; -m2 &lt;fr_model_dir&gt; -p &lt;Face DB dir&gt; -c -a\n</code></pre></li> <li>face recognition to compare right image to left image. you can use option -d for face detection <pre><code>./face_recognition -m0 &lt;fd_model_dir&gt; -m1 &lt;lm_model_dir&gt; -m2 &lt;fr_model_dir&gt; -r &lt;image_path&gt; -l &lt;image_path&gt;\n</code></pre> <pre><code>./face_recognition -m0 /fd_model -m1 /lm_model -m2 /fr_model -p /dxrt/face_db_2person -v /dxrt/videos/test.mp4\n</code></pre> </li> </ul>"},{"location":"Examples.html#multi-model","title":"Multi-Model","text":"<p>Object Detection With Image Segmentation Demo (from <code>app/hybrid/</code>) Currently, model path is configured as in <code>OD_MODEL_PATH</code>, <code>SEG_MODEL_PATH</code> in source code.  </p> <p>-i, --image     use image file input   -v, --video     use video file input   -c, --camera    use camera input   -a, --async     asynchronous inference   -h, --help      show help  </p> <ul> <li>YOLO + Segmentation for single image <pre><code>./yolo_ddrnet -m &lt;model_dir&gt; -i &lt;image file&gt;\n./yolo_pidnet -m &lt;model_dir&gt; -i &lt;image file&gt;\n</code></pre></li> <li>YOLO + Segmentation for single video file <pre><code>./yolo_ddrnet -m &lt;model_dir&gt; -v &lt;video file&gt; -a\n./yolo_pidnet -m &lt;model_dir&gt; -v &lt;video file&gt; -a\n</code></pre></li> <li>YOLO + Segmentation for camera input <pre><code>./yolo_ddrnet -m &lt;model_dir&gt; -c -a\n./yolo_pidnet -m &lt;model_dir&gt; -c -a\n</code></pre> <pre><code>./yolo_pidnet -a -v /dxrt/videos/stuttgart-00.avi\n</code></pre> </li> </ul>"},{"location":"Examples.html#imagenet>_classification","title":"ImageNet Classification","text":"<p>Image Classification Demo for ImageNet Dataset (from <code>app/imagenet_classification/imagenet_classification_demo.cpp</code>)   Currently, default model/image/label/grid paths are configured in source code. (<code>DEFAULT_MODEL_PATH</code>, <code>DEFAULT_IMAGE_PATH</code>, <code>DEFAULT_LABEL_PATH</code>, <code>DEFAULT_GRID_PATH</code>)  </p> <p>-m, --model        define model path  -i, --image        ImageNet image path  -l, --label        ImageNet label path  -g, --grid         ImageNet grid path  -h, --help         show help </p><pre><code>./imagenet_classification -m /dxrt/models/DX_M1/v2p0/efficientnet-b0_argmax/\n</code></pre> The label file is a JSON file that contains image names and their corresponding labels, as shown below. <pre><code>{\n  \"ILSVRC2012_val_00000001\": 65,\n  \"ILSVRC2012_val_00000002\": 970,\n  \"ILSVRC2012_val_00000003\": 230,\n  ...\n}\n</code></pre>"},{"location":"Examples.html#packet>_classification","title":"Packet Classification","text":"<p>Packet Classification Demo (from <code>app/packet_classification/packet_classification.cpp</code>)</p> <p>-m, --model        define model path  -a, --async        asynchronous inference  -b, --bin          use binary file input with preprocessed data  -h, --hazard       enable hazard mode</p> <ul> <li>packet classification for test packet <pre><code>./packet_classification -m &lt;model_dir&gt; -a -h\n</code></pre></li> <li>packet classification for single binary file <pre><code>./packet_classification -m &lt;model_dir&gt; -b sample/packet/attack_packet_hazard.bin\n</code></pre> Result <pre><code>Initialize device /dev/dx_dma0_npu_0\nMemory mapped at address 0x7f25f9bac000(100000)\nparse non-encryption format.\n  Task[0] /dxrt/m1_4k/khu-dasan-packet-224/1 NPUs, 9200000bytes\n            Device : NPU 0, /dev/dx_dma0_npu_0(IDLE), DX_M1_4K, PCIE type\n              - MemIf : /dev/dx_dma0_h2c_0, /dev/dx_dma0_c2h_0, 100000(0) + e0000000\n    [0]\n      inputs\n        Tensor: name INPUT, type UINT8 [1, 224, 224, 1,  ], data 0x557a3cded080, dest 0\n      outputs\n        Tensor: name 250, type INT32 [1, 1, 1, 2,  ], data 0x557a3ccb6250, dest 0\n[Packet Classification] result :: 0 [0:normal packet, 1:attack packet]\n</code></pre></li> </ul>"},{"location":"Getting-Started.html","title":"Getting Started","text":""},{"location":"Getting-Started.html#prerequisite","title":"Prerequisite","text":"<p>To proceed with the details below, installation must first be completed. Ensure that the necessary dependencies and build tools are installed on the host and target systems.  </p>"},{"location":"Getting-Started.html#0>_build","title":"0. Build","text":""},{"location":"Getting-Started.html#non-cross-compile>_case","title":"non-cross-compile case","text":"<p>To build DXRT for the host system, execute the build script with the appropriate installation path. It is recommended to set the installation path to a location commonly referenced in the OS, such as <code>/usr/local</code>. </p><pre><code>./build.sh --install /usr/local\n</code></pre>"},{"location":"Getting-Started.html#cross-compile>_case","title":"cross-compile case","text":"<p>To cross-compile DXRT for a target system, please refer to <code>Build</code> chapter. </p><pre><code>./build.sh --arch &lt;target_cpu&gt;\n</code></pre>"},{"location":"Getting-Started.html#1>_prepare>_a>_model","title":"1. Prepare a model","text":"<p>Pick one prebuilt model from ModelZoo. Since ModelZoo is still not open, prebuilt model files will be provided separately. </p><pre><code>(You will see the file structure as below )  \nyolov5s_640\n\u251c\u2500\u2500 rmap.bin\n\u251c\u2500\u2500 rmap.info\n\u2514\u2500\u2500 weight.bin\n</code></pre>"},{"location":"Getting-Started.html#2>_run>_simple>_cli>_to>_device","title":"2. Run simple CLI to device","text":"<p>Check basic device interface as following. </p><pre><code>sudo ./bin/dxrt-cli --status\n</code></pre> <pre><code>      Memory @ c01000000 ~ c60400000(0 ~ 5f400000), 5f400000, cur c01000000, [0, 5f400000, 0],\n      Memory @ c60400000 ~ c80000000(0 ~ 1fc00000), 1fc00000, cur c60400000, [0, 1fc00000, 0],\n    Device 0: type 0, var 200, addr 0xc01000000, size 0x7f000000, dma_ch 2, fw_ver 113, board rev 2, board type 1, ddr freq 4200, ddr type 1\n    Device 0: voltage [825, 800, 0, 0], clock [1000, 600, 0, 0], temperature [44, 46, 0, 0], dvfs [0, 0], boot_state 0\n</code></pre>"},{"location":"Getting-Started.html#3>_run>_simple>_benchmark>_with>_the>_model","title":"3. Run simple benchmark with the model","text":"<pre><code>sudo ./bin/run_model -m &lt;model directory&gt;\n</code></pre> If the application completes successfully, you can see detection result as following. <pre><code>Start benchmark.\nCompleted benchmark.\nData test: Sequential\n-----------------------------------\nmodel\n  Inference time : 4.41787ms\n  FPS : 226.354\n  Data Integrity Check : SKIP (0 pass, 0 fail )\n-----------------------------------\n...\n</code></pre>"},{"location":"Getting-Started.html#4>_run>_object>_detection>_demo>_with>_the>_model","title":"4. Run object detection demo with the model","text":"<pre><code>sudo ./bin/yolo -m &lt;model directory&gt; -p 2 -i sample/1.jpg\n</code></pre> If the demo application completes successfully, you can see detection result as following.(each result values will be different from this) <pre><code>  Detected 7 boxes.\n    BBOX:person(0) 0.838961, (382.578, 171.134, 498.847, 460.018)\n    BBOX:bowl(45) 0.761698, (31.2044, 450.033, 99.4646, 491.072)\n    BBOX:oven(69) 0.626437, (486.406, 309.157, 619.971, 450.749)\n    BBOX:bowl(45) 0.618453, (59.1502, 393.524, 135.132, 434.191)\n    BBOX:bowl(45) 0.456878, (155.886, 274.543, 184.228, 291.046)\n    BBOX:person(0) 0.441233, (0.316133, 369.499, 60.3764, 415.845)\n    BBOX:banana(46) 0.340904, (-0.602713, 106.704, 61.4954, 256.591)\nyolov5s_640 : latency 11548us, 5472us\n</code></pre>"},{"location":"High-Level-Design.html","title":"High Level Design","text":""},{"location":"High-Level-Design.html#overview","title":"Overview","text":"<p>Compiled NN model is imported as an <code>InferenceEngine</code> instance, just run it. <code>InferenceEngine</code> will do : - initialization of DEEPX devices - memory management for inference - real-time inference job scheduling based on optimized NPU/CPU interaction  </p> <p>Before running <code>InferenceEngine</code>, <code>InferenceOption</code> should be established, which describes what device to use, how much resources to use. Also, example applications provide typical pre/post-processing attached to OpenCV for NN inference. </p>"},{"location":"High-Level-Design.html#static>_architecture","title":"Static Architecture","text":"<p>DXRT Software stack is partitioned to 3 layers.</p> <ul> <li>NN API layer is designed to provide pulic inference APIs, tensor processing APIs to user's application SW.  </li> <li>Library layer is designed for actual implementation of runtime framework in userspace, and partitioned to various C++ classes that abstracts resources for inference. It also plays an important role in exchanging data related to inference with device drivers.  </li> <li>Device driver layer is responsible for communicating with firmware, managing inference, and HW-dependent implementation of NPU or PCIe functions (Device driver repository will be split from DXRT in future). </li> </ul>"},{"location":"High-Level-Design.html#inference>_concept","title":"Inference Concept","text":"<p>In point of high-level view, <code>InferenceEngine</code> abstracts all the steps of inference. When <code>InferenceEngine</code> is created, compiled model is loaded to one <code>Model</code>, which is partitioned and converted to graph of <code>Task</code>. <code>Task</code> represents NN operations information, and describes where(NPU or CPU) should it be done. Model parameters for each task will be written to proper device memory, and required memory allocation in both side(host, device) will be done automatically. Then, tensors are created based on the model parameters, and attached to each tasks. So, <code>Worker</code> instances will execute the task graph asynchronously internally. As a result, selected <code>Device</code> or <code>CpuHandle</code> by <code>Worker</code> will perform NN computations(if needed, memory write/read operations of tensors will be occured).  </p>"},{"location":"High-Level-Design.html#compiler>_interface","title":"Compiler Interface","text":"<p>Compiled model consists of NPU task, and CPU task parameters as followings. </p>"},{"location":"High-Level-Design.html#device>_allocation","title":"Device Allocation","text":"<p><code>Worker</code> will allocate real-time available device for each requested task.  </p>"},{"location":"High-Level-Design.html#memory>_allocation>_for>_deepx>_devices","title":"Memory Allocation for DEEPX devices","text":"<p>MemoryManager allocates required memory when InferenceEngine is created. In accelerator mode, non-contiguous userspace heap is used as NPU data interface. In standalone mode, contiguous reserved memory by linux CMA(Contiguous Memory Allocator) is used as NPU data interface. </p>"},{"location":"How-To-Create-CMake-Project.html","title":"How To Create CMake Project","text":""},{"location":"How-To-Create-CMake-Project.html#how>_to>_create>_a>_cmake>_project>_using>_dxrt","title":"How to Create a CMake Project Using DXRT","text":"<p>This guide provides step-by-step instructions on creating a new CMake project using the DXRT library. 1. Build DXRT library Refer to Installation, Build chapter for instructions on fetching and building the DXRT source code. 2. Create a new CMake project Create a new CMake project by making a project directory and creating a <code>CMakeLists.txt</code> file. </p><pre><code>mkdir MyProject\ncd MyProject\ntouch CMakeLists.txt\n</code></pre> 3. Helloworld with DXRT API Let's write a simple hello world app. that calls the DXRT API. main.cpp: <pre><code>#include \"dxrt/dxrt_api.h\"\n\nusing namespace std;\n\nint main(int argc, char *argv[])\n{\n    auto devices = dxrt::CheckDevices();\n    cout &lt;&lt; \"hello, world\" &lt;&lt; endl;\n    return 0;\n}\n</code></pre> 4. Modify CMakeLists.txt Open and modify <code>CMakeLists.txt</code> file as follows. <pre><code>cmake_minimum_required(VERSION 3.14)\nproject(app_template)\nset(CMAKE_CXX_STANDARD_REQUIRED \"ON\")\nset(CMAKE_CXX_STANDARD \"17\")\n\n# Specify the installation path where the DXRT library is installed.\nset(DXRT_LIB_PATH \"/usr/local/lib\") # Adjust this path to the actual installation path of DXRT library.\n\n# Find and retrieve the DXRT library.\nfind_library(DXRT_LIBRARY REQUIRED NAMES dxrt_${CMAKE_SYSTEM_PROCESSOR} PATHS ${DXRT_LIB_PATH})\n\n# add google protobuf library.\nadd_library(protobuf SHARED IMPORTED)\nset_target_properties(protobuf PROPERTIES\n  IMPORTED_LOCATION \"${DXRT_LIB_PATH}/libprotobuf.so.23\"  \n)\n\n# Add project source files and executable.\nadd_executable(HelloWorld main.cpp)\n\n# Link the DXRT library to the executable.\ntarget_link_libraries(HelloWorld PRIVATE ${DXRT_LIBRARY} protobuf)\n</code></pre> Set <code>DXRT_LIB_PATH</code> with the actual path where the DXRT library is installed. 5. Build the Project <pre><code>mkdir build\ncd build\ncmake ..\nmake\n</code></pre> 6. Run the Executable After a successful build, run the generated executable.   <p>Now, you have created a new CMake project using the DXRT library!  </p>"},{"location":"Inference-Guide.html","title":"Inference Guide","text":""},{"location":"Inference-Guide.html#model>_file>_format","title":"Model file format","text":"<p>Original ONNX model file is converted as following by DXCOM SDK. </p><pre><code>Model dir.\n    \u251c\u2500\u2500 rmap.bin\n    \u251c\u2500\u2500 rmap.info\n    \u2514\u2500\u2500 weight.bin\n</code></pre> <ul> <li>rmap.bin : NPU Command data  </li> <li>rmap.info : NPU Command metadata  </li> <li>weight.bin : Model weight/bias/bn parameters  </li> </ul>"},{"location":"Inference-Guide.html#basic>_flow","title":"Basic Flow","text":""},{"location":"Inference-Guide.html#0>_prepare>_a>_model","title":"0. Prepare a model","text":"<p>You can choose one by 2 ways. 1) Pick one prebuilt model from ModelZoo 2) Compile a model to DXRT model format (Details are in DXCOM SDK manual)  </p>"},{"location":"Inference-Guide.html#1>_set>_inference>_option","title":"1. Set inference option","text":"<p>Create an <code>dxrt::InferenceOption</code>. (Refer to API Reference) <code>dxrt::InferenceOption</code> represents detailed options for inference engine. - This implementation is deprecated temporarily in current version, and will be ported in next version.  </p>"},{"location":"Inference-Guide.html#2>_load>_model>_to>_inference>_engine","title":"2. Load model to inference engine","text":"<p>Create an <code>dxrt::InferenceEngine</code> from the model directory. Required initialization of HW resources will be done automatically by doing this. If <code>dxrt::InferenceOption</code> is not specified, default inference option is applied. </p><pre><code>auto ie = dxrt::InferenceEngine(\"yolov5s\");\nauto ie = dxrt::InferenceEngine(\"yolov5s\", &amp;option);\n</code></pre>"},{"location":"Inference-Guide.html#3>_connect>_input>_tensors","title":"3. Connect input tensors","text":"<p>Prepare input buffer(s) to perform infernce on. You can easily prepare an input buffer with minimal effort with the code below. </p><pre><code>vector&lt;uint8_t&gt; inputBuf(ie.input_size(), 0);\n</code></pre> Guides for connecting the inference engine to various image sources are provided in dx-app and dx-demo, along with preprocessing examples."},{"location":"Inference-Guide.html#4>_inference","title":"4. Inference","text":"<p>Run inference engine. <code>outputs</code> is a vector of shared_ptr of <code>dxrt::Tensor</code> returned from <code>dxrt::InferenceEngine::Run()</code>. </p><pre><code>auto outputs = ie.Run(inputBuf.data());\n</code></pre> If you want to perform inference in a non-blocking way, you can call <code>dxrt::InferenceEngine::RunAsync</code> as follows. <pre><code>auto req = ie.RunAsync(inputBuf.data());\n</code></pre> Non-blocking API issues a request ID, and you can wait for the request to complete. <pre><code>auto outputs = ie.Wait(req);\n</code></pre> If you don't want to wait to run your application efficiently, you can pipeline requests with callbacks. <pre><code>std::function&lt;int(vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt;, void*)&gt; postProcCallBack = \\\n    [&amp;](vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt; outputs, void *arg)\n    {\n        /* Process output tensors here */\n        ... ...\n        return 0;\n    };\nie.RegisterCallBack(postProcCallBack);\n</code></pre> For more detailed information, please check the API document."},{"location":"Inference-Guide.html#5>_process>_output>_tensors","title":"5. Process output tensors","text":"<p>Since inference is done, process output tensors using <code>Tensor</code> APIs and custom post-processing logic. You can find templates and example code in dx-app, dx-demo to help you post-process smoothly. As mentioned before, using callbacks allows for more efficient post-processing.  </p>"},{"location":"Inference-Guide.html#multiple>_device>_inference","title":"Multiple Device Inference","text":"<p>This is not applicable for single-NPU devices. Basically, the inference engine schedules and manages multiple devices in real time. If inference option is set explicitly, inference engine may only use specific devices during real-time inference for the model.  </p>"},{"location":"Inference-Guide.html#device>_tensor>_data>_format","title":"Device Tensor Data Format","text":"<p>Compiled models have tensor shape as NHWC format basically. Input tensor data format of current devices consists of 2 types. For formatter type, [1, 3, 224, 224] will be [1, 224, 224, 3] in device input tensor. For im2col type, [1, 3, 224, 224] will be [1, 224, 224*3+32] in device input tensor.  </p> name Compiled model format Device format Data size Formatter [N, H, W, C] [N, H, W, C] 8bit IM2COL [N, H, W, C] [N, H, align64(W*C)] 8bit <p>Output tensor data format of current devices is aligned NHWC format. For example, [1, 40, 52, 36] will be [1, 52, 36, 40+24] in device output tensor. By using <code>Tensor</code> APIs, post-processing can be done without converting formats.  </p> name Compiled model format Device format aligned NHWC [N, H, W, C] [N, H, W, align64(C)] <p>API to convert from device format to NCHW/NHWC format will be supported soon.  </p>"},{"location":"Inference-Guide.html#runtime>_linking>_error","title":"Runtime Linking Error","text":"<pre><code>yolo: error while loading shared libraries: libprotobuf.so.23:\n      cannot open shared object file: No such file or directory\nor\n\nyolo: error while loading shared libraries: libdxrt_x86_64 &lt;or riscv64/arm64&gt;.so: \n      cannot open shared object file: No such file or directory\n</code></pre> If you get any library(DXRT lib., openCV, googleprotobuf) linking error while executing applications, please set library path as following.   <ul> <li>dxrt lib. path is <code>build_&lt;CMAKE_SYSTEM_PROCESSOR&gt;/release/lib/</code> in the DXRT package.  </li> <li>openCV lib. path is <code>&lt;CMAKE_INSTALL_PREFIX&gt;/lib</code> in your openCV build repository.  </li> <li>googleprotobuf lib. path is <code>extern/lib/&lt;CMAKE_SYSTEM_PROCESSOR&gt;</code> in the DXRT package. <pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:&lt;lib. path&gt;\n</code></pre></li> </ul>"},{"location":"Inference-Guide.html#profile>_application","title":"Profile application","text":""},{"location":"Inference-Guide.html#1>_gather>_timing>_data>_per>_events","title":"1. Gather timing data per events","text":"<p>You can profile events using <code>Profiler</code> APIs. Please refer to API reference. After application is terminated, <code>profiler.json</code> will be created. </p><pre><code>auto&amp; profiler = dxrt::Profiler::GetInstance();\nprofiler.Start(\"1sec\");\nsleep(1);\nprofiler.End(\"1sec\");\n</code></pre>"},{"location":"Inference-Guide.html#2>_visualize>_profilers>_data","title":"2. Visualize profiler's data","text":"<p>You can visualize events from <code>profiler.json</code> using as following. </p><pre><code>python3 tool/profiler/plot.py --input profiler.json\n</code></pre> Then, generated image file <code>profiler.png</code> will show detailed profiling data.   Please refer to usage of <code>tool/profiler/plot.py</code>. <pre><code>usage: plot.py [-h] [-i INPUT] [-o OUTPUT] [-s START] [-e END] [-g]\n\nDraw timing chart from profiler data.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i INPUT, --input INPUT\n                        Input json file to plot\n  -o OUTPUT, --output OUTPUT\n                        Output image file to save the plot\n  -s START, --start START\n                        Starting point( &gt; 0.0) when the entire interval is 1\n  -e END, --end END     End point( &lt; 1.0) when the entire interval is 1\n  -g, --show_gap        Show time gap between starting points\n</code></pre>"},{"location":"Installation.html","title":"Installation","text":""},{"location":"Installation.html#setup>_build>_environments","title":"Setup build environments","text":"<ul> <li>OS : Ubuntu 18.04, Ubuntu 20.04  </li> <li>CMake : 3.14.0 or higher required. Refer to followings. <pre><code>sudo apt-get install zlib1g-dev libcurl4-openssl-dev\nwget https://cmake.org/files/v3.14/cmake-3.14.0.tar.gz --no-check-certificate\ntar xvf cmake-3.14.0.tar.gz\ncd cmake-3.14.0\n./bootstrap --system-curl\nmake -j8\nsudo make install\n</code></pre></li> <li>Ninja-build <pre><code>sudo apt install ninja-build\n</code></pre></li> <li>GCC : 8.0.0 or higher required.  </li> </ul> <p>arm64 gcc (for DX_M1) </p><pre><code>sudo apt-get install gcc-aarch64-linux-gnu g++-aarch64-linux-gnu\n</code></pre> riscv64 gcc (for DX_L2) <pre><code>sudo apt-get install gcc-riscv64-linux-gnu g++-riscv64-linux-gnu\n</code></pre> riscv64-eyenix gcc (for DX_L1) <pre><code>tar xvf riscv64_lp64d_enx122_linux_cross_2023-01-20.tgz\ncd cross_toolchain\n./cross_install\nsource /etc/profile\n</code></pre> * (optional) onnxruntime Required if you need CPU offloading for NN ops that NPU does not support. Refer to following installation example for onnxruntime v1.16.3. <pre><code>git clone --recursive https://github.com/Microsoft/onnxruntime.git\ncd onnxruntime\ngit checkout -b _v1.16.3 v1.16.3\n./build.sh --config RelWithDebInfo --build_shared_lib --parallel --compile_no_warning_as_error --skip_submodule_sync\nsudo make install\n</code></pre> Refer to onnxruntime Build Guide"},{"location":"Installation.html#get>_software>_repository","title":"Get software repository","text":"<p>Three types are provided. 1. Fetch git repository </p> <p></p><pre><code>git clone git@github.com:KOMOSYS/dxrt.git\n</code></pre> 2. Download release file package from release page 3. Download release file package from seperate corporate chain."},{"location":"Installation.html#file>_structure","title":"File structure","text":"<pre><code>\u251c\u2500\u2500 app\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 basic\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 classification\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 face_recognition\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hybrid\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 object_detection\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 segmentation\n\u251c\u2500\u2500 build.cfg\n\u251c\u2500\u2500 build.sh\n\u251c\u2500\u2500 cmake\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 driver\n\u251c\u2500\u2500 extern\n\u251c\u2500\u2500 lib\n\u251c\u2500\u2500 sample\n\u2514\u2500\u2500 test\n</code></pre> <ul> <li>app : demo applications  </li> <li>build.cfg : C/C++ preprocessor macros to use in application  </li> <li>build.sh : build shell script  </li> <li>cmake : required cmake scripts  </li> <li>docs : markdown files, and doxygen generator for API reference  </li> <li>driver : device driver files  </li> <li>extern : 3rd party library files  </li> <li>lib : prebuilt dxrt libarary files  </li> <li>sample : sample images for demo applications  </li> <li>test : dxrt unit test files  </li> </ul>"},{"location":"Installation.html#install>_linux>_device>_driver","title":"Install Linux Device Driver","text":"<ol> <li>For DX-M1 Execute install script in host device - sudo permission is mandatory. Linux kernel module file <code>dx_dma.ko</code>, <code>dxrt_driver.ko</code> will be built and installed. <pre><code>cd driver\nsudo ./install_m1.sh\n</code></pre></li> <li>For other devices Will be updated.  </li> </ol>"},{"location":"Introduction.html","title":"Introduction","text":"<p>DXRT is DEEPX Runtime SDK for AI inference based on DEEPX devices. It supports pre-built models from DEEPX model zoo, and compiled models by DXCOM(DEEPX Compiler SDK).  </p>"},{"location":"Introduction.html#supported>_devices","title":"Supported devices","text":"<p>DXRT provides common inference framework based on 2 kinds of inference mode.  </p> <ul> <li>Accelerator mode : inference by PCIe interface  </li> <li>Standalone mode : inference by direct AXI/APB interface  </li> </ul> Device Mode DX_L1 Standalone DX_L2 Standalone DX_M1 Accelerator DX_H1 Accelerator"},{"location":"Introduction.html#resources","title":"Resources","text":""},{"location":"Introduction.html#model>_zoo>_prebuilt>_models","title":"Model zoo (Prebuilt models)","text":"<p>Official modelzoo is in preparation.  </p>"},{"location":"Introduction.html#documents","title":"Documents","text":"<p>Official website documents are in preparation. Currently, documents are provided with limited rights. Please consult with our contact point person. In other way, you can generate documents from repository (using markdown files in <code>docs</code>).  </p> <ul> <li>python&gt;=3.9 is needed. <pre><code># install MkDocs\npip install mkdocs mkdocs-material mkdocs-video pymdown-extensions\n# generate html to directory \"html_docs\"\nmkdocs build\n</code></pre> You can also generate API reference using doxygen. <pre><code># install doxygen\nsudo apt install doxygen graphviz\n# generate API reference html\ncd docs/cpp_api\ndoxygen Doxyfile\n</code></pre></li> </ul>"},{"location":"Test.html","title":"Test","text":"<p>Based on googletest framework, test program is provided. ** Current version supports build of test program only in DEEPX-internal version. **  </p>"},{"location":"Test.html#dxrt>_unit>_test","title":"DXRT Unit Test","text":"<p>Perform DXRT's unit tests. (<code>USE_DXRT_TEST</code> should be <code>ON</code> in cmake/dxrt.cfg.cmake) Please check available test lists as following. </p><pre><code>sudo ./dxrt_test --gtest_list_tests\n</code></pre>"},{"location":"_Footer.html","title":"Footer","text":"<p>Copyright 2022. dxrt team, All rights reserved.</p>"}]}