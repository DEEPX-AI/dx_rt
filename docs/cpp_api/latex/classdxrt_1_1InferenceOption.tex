\hypertarget{classdxrt_1_1InferenceOption}{}\doxysection{dxrt\+::Inference\+Option Class Reference}
\label{classdxrt_1_1InferenceOption}\index{dxrt::InferenceOption@{dxrt::InferenceOption}}


This struct specifies inference options applied to \mbox{\hyperlink{classdxrt_1_1InferenceEngine}{dxrt\+::\+Inference\+Engine}}.  




{\ttfamily \#include \char`\"{}dxrt/dxrt\+\_\+api.\+h\char`\"{}}

\doxysubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
std\+::vector$<$ int $>$ \mbox{\hyperlink{classdxrt_1_1InferenceOption_ae98837ff566fe2b089a5b1bf07feb48c}{devices}} = \{\}
\begin{DoxyCompactList}\small\item\em device ID list to use \end{DoxyCompactList}\item 
uint32\+\_\+t \mbox{\hyperlink{classdxrt_1_1InferenceOption_a9fd270a5eeeb1a14ce5f133c16c30d55}{bound\+Option}} = B\+O\+U\+N\+D\+\_\+\+O\+P\+T\+I\+O\+N\+::\+N\+P\+U\+\_\+\+A\+LL
\begin{DoxyCompactList}\small\item\em Select the N\+PU core inside the device. \end{DoxyCompactList}\item 
bool \mbox{\hyperlink{classdxrt_1_1InferenceOption_aca38d2276725c63a2e9ac73046acc1a2}{use\+O\+RT}} = O\+R\+T\+\_\+\+O\+P\+T\+I\+O\+N\+\_\+\+D\+E\+F\+A\+U\+LT
\begin{DoxyCompactList}\small\item\em Select which uses O\+RT task or not. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
This struct specifies inference options applied to \mbox{\hyperlink{classdxrt_1_1InferenceEngine}{dxrt\+::\+Inference\+Engine}}. 

User can configure which npu device is used to inference. 

\doxysubsection{Member Data Documentation}
\mbox{\Hypertarget{classdxrt_1_1InferenceOption_a9fd270a5eeeb1a14ce5f133c16c30d55}\label{classdxrt_1_1InferenceOption_a9fd270a5eeeb1a14ce5f133c16c30d55}} 
\index{dxrt::InferenceOption@{dxrt::InferenceOption}!boundOption@{boundOption}}
\index{boundOption@{boundOption}!dxrt::InferenceOption@{dxrt::InferenceOption}}
\doxysubsubsection{\texorpdfstring{boundOption}{boundOption}}
{\footnotesize\ttfamily uint32\+\_\+t dxrt\+::\+Inference\+Option\+::bound\+Option = B\+O\+U\+N\+D\+\_\+\+O\+P\+T\+I\+O\+N\+::\+N\+P\+U\+\_\+\+A\+LL}



Select the N\+PU core inside the device. 

N\+P\+U\+\_\+\+A\+LL is an option that uses all N\+PU cores simultaneously. N\+P\+U\+\_\+0, N\+P\+U\+\_\+1, and N\+P\+U\+\_\+2 are options that allow using only a single N\+PU core. \mbox{\Hypertarget{classdxrt_1_1InferenceOption_ae98837ff566fe2b089a5b1bf07feb48c}\label{classdxrt_1_1InferenceOption_ae98837ff566fe2b089a5b1bf07feb48c}} 
\index{dxrt::InferenceOption@{dxrt::InferenceOption}!devices@{devices}}
\index{devices@{devices}!dxrt::InferenceOption@{dxrt::InferenceOption}}
\doxysubsubsection{\texorpdfstring{devices}{devices}}
{\footnotesize\ttfamily std\+::vector$<$int$>$ dxrt\+::\+Inference\+Option\+::devices = \{\}}



device ID list to use 

make a list which contains list of device ID to use. if it is empty(or use default value), then all devices are used. list of device ID to use (it is empty by default, then all devices are used.) \mbox{\Hypertarget{classdxrt_1_1InferenceOption_aca38d2276725c63a2e9ac73046acc1a2}\label{classdxrt_1_1InferenceOption_aca38d2276725c63a2e9ac73046acc1a2}} 
\index{dxrt::InferenceOption@{dxrt::InferenceOption}!useORT@{useORT}}
\index{useORT@{useORT}!dxrt::InferenceOption@{dxrt::InferenceOption}}
\doxysubsubsection{\texorpdfstring{useORT}{useORT}}
{\footnotesize\ttfamily bool dxrt\+::\+Inference\+Option\+::use\+O\+RT = O\+R\+T\+\_\+\+O\+P\+T\+I\+O\+N\+\_\+\+D\+E\+F\+A\+U\+LT}



Select which uses O\+RT task or not. 

if this is true, all task will works. if false, only npu task works. 