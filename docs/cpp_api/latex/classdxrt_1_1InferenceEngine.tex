\hypertarget{classdxrt_1_1InferenceEngine}{}\doxysection{dxrt\+::Inference\+Engine Class Reference}
\label{classdxrt_1_1InferenceEngine}\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}}


This class abstracts runtime inference executor for user\textquotesingle{}s compiled model.  




{\ttfamily \#include \char`\"{}dxrt/dxrt\+\_\+api.\+h\char`\"{}}

\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classdxrt_1_1InferenceEngine_aed622c0fa566620899c9e7e26011bacd}{Inference\+Engine}} (const std\+::string \&model\+Path, \mbox{\hyperlink{classdxrt_1_1InferenceOption}{Inference\+Option}} \&option=\mbox{\hyperlink{namespacedxrt_aab5060680ba2f567f47f8868cd8b1f00}{Default\+Inference\+Option}})
\begin{DoxyCompactList}\small\item\em Perform the task of loading the model and configuring the N\+PU to run. \end{DoxyCompactList}\item 
Tensor\+Ptrs \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a88cea5324cbeb436034adb60cf31050f}{Run}} (void $\ast$input\+Ptr, void $\ast$user\+Arg=nullptr, void $\ast$output\+Ptr=nullptr)
\begin{DoxyCompactList}\small\item\em Run inference engine using specific input pointer Synchronously. \end{DoxyCompactList}\item 
std\+::vector$<$ Tensor\+Ptrs $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ab54d666cc65cef4d6ae2fe4c300d7ba1}{Run}} (const std\+::vector$<$ void $\ast$ $>$ \&input\+Buffers, const std\+::vector$<$ void $\ast$ $>$ \&output\+Buffers, const std\+::vector$<$ void $\ast$ $>$ \&user\+Args=\{\})
\begin{DoxyCompactList}\small\item\em Runs the inference engine using a specific input pointer vector. \end{DoxyCompactList}\item 
int \mbox{\hyperlink{classdxrt_1_1InferenceEngine_af3006c9a7183f7e42d1528aa8a6fea24}{Run\+Async}} (void $\ast$input\+Ptr, void $\ast$user\+Arg=nullptr, void $\ast$output\+Ptr=nullptr)
\begin{DoxyCompactList}\small\item\em Non-\/blocking call to request asynchronous inference by input pointer, and get job ID from inference engine. \end{DoxyCompactList}\item 
int \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ac9c8166a8c588860ea4a5fc29d743eb5}{Run\+Async}} (const std\+::vector$<$ void $\ast$ $>$ \&input\+Ptrs, void $\ast$user\+Arg=nullptr, void $\ast$output\+Ptr=nullptr)
\begin{DoxyCompactList}\small\item\em Non-\/blocking call to request asynchronous inference with automatic multi-\/input detection. This function automatically detects whether the input should be interpreted as multi-\/input single inference or batch inference based on the model requirements and input count. \end{DoxyCompactList}\item 
int \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a6a5b11e470a9dfeca6c60f644726ea59}{Run\+Async\+Multi\+Input}} (const std\+::map$<$ std\+::string, void $\ast$ $>$ \&input\+Tensors, void $\ast$user\+Arg=nullptr, void $\ast$output\+Ptr=nullptr)
\begin{DoxyCompactList}\small\item\em Run async inference with multiple input tensors for multi-\/input models. \end{DoxyCompactList}\item 
int \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a8a61dde8d97418d77e5bfd348011517f}{Run\+Async\+Multi\+Input}} (const std\+::vector$<$ void $\ast$ $>$ \&input\+Ptrs, void $\ast$user\+Arg=nullptr, void $\ast$output\+Ptr=nullptr)
\begin{DoxyCompactList}\small\item\em Run async inference with multiple input tensors (vector format) for multi-\/input models. \end{DoxyCompactList}\item 
float \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ab0c6629f048341d62bfa3a400c90e827}{Run\+Bench\+Mark}} (int num, void $\ast$input\+Ptr=nullptr)
\begin{DoxyCompactList}\small\item\em run benchmark with loop n times (Legacy A\+PI) \end{DoxyCompactList}\item 
float \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a3846a2f47ea39933136e589aad036eb0}{Run\+Benchmark}} (int num, void $\ast$input\+Ptr=nullptr)
\begin{DoxyCompactList}\small\item\em run benchmark with loop n times \end{DoxyCompactList}\item 
Tensor\+Ptrs \mbox{\hyperlink{classdxrt_1_1InferenceEngine_aef4c9502a5d5a9d256d3899391376aba}{Validate\+Device}} (void $\ast$input\+Ptr, int device\+Id=0)
\begin{DoxyCompactList}\small\item\em Validate inference of a specific N\+PU device connected to the host. This function runs a validation process using the provided input data on the specified N\+PU device. It can be used to ensure that the N\+PU device is operational and can process inference tasks correctly. \end{DoxyCompactList}\item 
Tensor\+Ptrs \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a2cbf5e6220f5be2cd9132f2570129faf}{Validate\+Device}} (const std\+::vector$<$ void $\ast$ $>$ \&input\+Ptrs, int device\+Id=0)
\begin{DoxyCompactList}\small\item\em Validate inference of a specific N\+PU device with automatic multi-\/input detection. This function automatically detects whether the input should be interpreted as multi-\/input based on the model requirements and input count. \end{DoxyCompactList}\item 
Tensor\+Ptrs \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a5b6d78e715a91712548b905d2c730a0c}{Validate\+Device\+Multi\+Input}} (const std\+::map$<$ std\+::string, void $\ast$ $>$ \&input\+Tensors, int device\+Id=0)
\begin{DoxyCompactList}\small\item\em Validate N\+PU device with multiple input tensors for multi-\/input models. \end{DoxyCompactList}\item 
Tensor\+Ptrs \mbox{\hyperlink{classdxrt_1_1InferenceEngine_af252439b3ba07ea017792c4fe6d82c7a}{Validate\+Device\+Multi\+Input}} (const std\+::vector$<$ void $\ast$ $>$ \&input\+Ptrs, int device\+Id=0)
\begin{DoxyCompactList}\small\item\em Validate N\+PU device with multiple input tensors (vector format) for multi-\/input models. \end{DoxyCompactList}\item 
void \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a3d3fc75faa79470d9a3bdbdc61561a3a}{Register\+Call\+Back}} (std\+::function$<$ int(Tensor\+Ptrs \&\mbox{\hyperlink{classdxrt_1_1InferenceEngine_a0893b724d6eed939556f1315ce05829d}{outputs}}, void $\ast$user\+Arg)$>$ callback\+Func)
\begin{DoxyCompactList}\small\item\em Register user callback function to be called by inference completion. (Legacy A\+PI) \end{DoxyCompactList}\item 
void \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a1d8b33fa324a48b43433c401bd54280a}{Register\+Callback}} (std\+::function$<$ int(Tensor\+Ptrs \&\mbox{\hyperlink{classdxrt_1_1InferenceEngine_a0893b724d6eed939556f1315ce05829d}{outputs}}, void $\ast$user\+Arg)$>$ callback\+Func)
\begin{DoxyCompactList}\small\item\em Register user callback function to be called by inference completion. \end{DoxyCompactList}\item 
Tensor\+Ptrs \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a2c6f130d722950153287d4cf1efd5a5d}{Wait}} (int job\+Id)
\begin{DoxyCompactList}\small\item\em Wait until an request is complete and returns output. \end{DoxyCompactList}\item 
Tensors \mbox{\hyperlink{classdxrt_1_1InferenceEngine_afbee8b031fdd6c0a3916ed00d08c9cd8}{inputs}} (void $\ast$ptr=nullptr, uint64\+\_\+t phy\+Addr=0)
\begin{DoxyCompactList}\small\item\em Get input tensor (Legacy A\+PI) \end{DoxyCompactList}\item 
Tensors \mbox{\hyperlink{classdxrt_1_1InferenceEngine_abe536ab666125335e08764fa4916c157}{Get\+Inputs}} (void $\ast$ptr=nullptr, uint64\+\_\+t phy\+Addr=0)
\begin{DoxyCompactList}\small\item\em Get input tensor. \end{DoxyCompactList}\item 
std\+::vector$<$ Tensors $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_af81b48e8ccd95953dd9bc483e5ec678b}{inputs}} (int dev\+Id)
\begin{DoxyCompactList}\small\item\em Get input tensor (Legacy A\+PI) \end{DoxyCompactList}\item 
std\+::vector$<$ Tensors $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_adb0043cbbf3dfa50eba37d03b9e7d48e}{Get\+Inputs}} (int dev\+Id)
\begin{DoxyCompactList}\small\item\em Get input tensor. \end{DoxyCompactList}\item 
Tensors \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a0893b724d6eed939556f1315ce05829d}{outputs}} (void $\ast$ptr=nullptr, uint64\+\_\+t phy\+Addr=0)
\begin{DoxyCompactList}\small\item\em Get output tensor (Legacy A\+PI) \end{DoxyCompactList}\item 
Tensors \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a411b78e6c9560119f9a39c30ee1021af}{Get\+Outputs}} (void $\ast$ptr=nullptr, uint64\+\_\+t phy\+Addr=0)
\begin{DoxyCompactList}\small\item\em Get output tensor. \end{DoxyCompactList}\item 
uint64\+\_\+t \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a96c4e5953a0daf4a1c886002c052a6ba}{input\+\_\+size}} ()
\begin{DoxyCompactList}\small\item\em Get total size of input tensors (Legacy A\+PI) \end{DoxyCompactList}\item 
uint64\+\_\+t \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ad2c11689c4839b5dbb3571107a7b3a7c}{Get\+Input\+Size}} ()
\begin{DoxyCompactList}\small\item\em Get total size of input tensors. \end{DoxyCompactList}\item 
std\+::vector$<$ uint64\+\_\+t $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a62b2716dbe26bbca8dac24d4954f8c17}{Get\+Input\+Tensor\+Sizes}} ()
\begin{DoxyCompactList}\small\item\em Get individual input tensor sizes for multi-\/input models. \end{DoxyCompactList}\item 
std\+::vector$<$ uint64\+\_\+t $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_aee8bc8dd933efeed07de606fcf2fb161}{Get\+Output\+Tensor\+Sizes}} ()
\begin{DoxyCompactList}\small\item\em Get individual output tensor sizes for multi-\/output models. \end{DoxyCompactList}\item 
uint64\+\_\+t \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a93e1d7b460b7f8656e725ed708de507b}{output\+\_\+size}} ()
\begin{DoxyCompactList}\small\item\em Get total size of output tensors (Legacy A\+PI) \end{DoxyCompactList}\item 
uint64\+\_\+t \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a82af5685f6b43643c3ddf47bf0a4ade8}{Get\+Output\+Size}} ()
\begin{DoxyCompactList}\small\item\em Get total size of output tensors. \end{DoxyCompactList}\item 
std\+::string \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a3fb16094508bd367cc4971238f002765}{name}} ()
\begin{DoxyCompactList}\small\item\em Get model name (Legacy A\+PI) \end{DoxyCompactList}\item 
std\+::string \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a7ed90b0eaa55ce7c097cc7163009c66e}{Get\+Model\+Name}} ()
\begin{DoxyCompactList}\small\item\em Get model name. \end{DoxyCompactList}\item 
std\+::vector$<$ std\+::string $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a578a0098f9b49902fa4c81d95945baa4}{task\+\_\+order}} ()
\begin{DoxyCompactList}\small\item\em Get model task order (Legacy A\+PI) \end{DoxyCompactList}\item 
std\+::vector$<$ std\+::string $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a8b759a7bb098d710e1c3052170057143}{Get\+Task\+Order}} ()
\begin{DoxyCompactList}\small\item\em Get model task order. \end{DoxyCompactList}\item 
int \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a990ca720e7f8a28ac5394206eb6f9491}{latency}} ()
\begin{DoxyCompactList}\small\item\em Get latest latency (Legacy A\+PI) \end{DoxyCompactList}\item 
int \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a3758aeb28d7ed75216770a5fd5dd9619}{Get\+Latency}} ()
\begin{DoxyCompactList}\small\item\em Get latest latency. \end{DoxyCompactList}\item 
uint32\+\_\+t \mbox{\hyperlink{classdxrt_1_1InferenceEngine_aad2f3b616e0e996b66ef7df37dc16a85}{inference\+\_\+time}} ()
\begin{DoxyCompactList}\small\item\em Get latest inference time (Legacy A\+PI) \end{DoxyCompactList}\item 
uint32\+\_\+t \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a1ee64321b5d6c50cb95be282080f326d}{Get\+Npu\+Inference\+Time}} ()
\begin{DoxyCompactList}\small\item\em Get latest inference time. \end{DoxyCompactList}\item 
std\+::vector$<$ int $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a331862cfde13a009b3e4133678a1c597}{Get\+Latency\+Vector}} ()
\begin{DoxyCompactList}\small\item\em Get recent Latency. \end{DoxyCompactList}\item 
std\+::vector$<$ uint32\+\_\+t $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_aad79d35e5a285878517eb784439737a9}{Get\+Npu\+Inference\+Time\+Vector}} ()
\begin{DoxyCompactList}\small\item\em Get recent inference time. \end{DoxyCompactList}\item 
double \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a1e81cce5b5db047c17ba40b69bab9a60}{Get\+Latency\+Mean}} ()
\begin{DoxyCompactList}\small\item\em Get latency Mean. \end{DoxyCompactList}\item 
double \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a0d46e7a44cd5975c5a9c3a76be684dc3}{Get\+Npu\+Inference\+Time\+Mean}} ()
\begin{DoxyCompactList}\small\item\em Get inference time Mean. \end{DoxyCompactList}\item 
double \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ae8bec4ff8e4e0fb6fa680c04f84b642d}{Get\+Latency\+Std\+Dev}} ()
\begin{DoxyCompactList}\small\item\em Get latency Standard Deviation. \end{DoxyCompactList}\item 
double \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a4d552cb23986c189b1002a3e0b9915a9}{Get\+Npu\+Inference\+Time\+Std\+Dev}} ()
\begin{DoxyCompactList}\small\item\em Get inference time Standard Deviation. \end{DoxyCompactList}\item 
int \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a1259543ba3f8fc52efb503e40570f95e}{Get\+Latency\+Cnt}} ()
\begin{DoxyCompactList}\small\item\em Get latency Count. \end{DoxyCompactList}\item 
int \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ad7a32a700fa89a0213b815d57f8fc1c2}{Get\+Npu\+Inference\+Time\+Cnt}} ()
\begin{DoxyCompactList}\small\item\em Get inference time Count. \end{DoxyCompactList}\item 
std\+::vector$<$ Tensor\+Ptrs $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a34dfe51c8f5289bd54ff96be8d64bc12}{get\+\_\+outputs}} ()
\begin{DoxyCompactList}\small\item\em Get output tensors of all tasks (Legacy A\+PI) \end{DoxyCompactList}\item 
std\+::vector$<$ Tensor\+Ptrs $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ae936aff8999d075130a531f3de60125a}{Get\+All\+Task\+Outputs}} ()
\begin{DoxyCompactList}\small\item\em Get output tensors of all tasks. \end{DoxyCompactList}\item 
std\+::vector$<$ uint8\+\_\+t $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_aa8ab6719ae0efdf0f5c678e3401ab57d}{bitmatch\+\_\+mask}} (int index)
\item 
int \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a0b38cd5001ec327cbdc5b00a7143d0e2}{get\+\_\+num\+\_\+tails}} ()
\begin{DoxyCompactList}\small\item\em Returns the number of tail tasks in the model. (Legacy A\+PI) \end{DoxyCompactList}\item 
int \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a110445fe04396f2d2ea309c18c3bb930}{Get\+Num\+Tail\+Tasks}} ()
\begin{DoxyCompactList}\small\item\em Returns the number of tail tasks in the model. \end{DoxyCompactList}\item 
std\+::string \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a265ff0d27c6bac98c0c641c0e5f8f6ba}{get\+\_\+compile\+\_\+type}} ()
\begin{DoxyCompactList}\small\item\em Returns the compile type of the model. (Legacy A\+PI) \end{DoxyCompactList}\item 
std\+::string \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a9308cfc747e8d56119febfd2b48b876c}{Get\+Compile\+Type}} ()
\begin{DoxyCompactList}\small\item\em Returns the compile type of the model. \end{DoxyCompactList}\item 
std\+::string \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a1b3771241a7424353e97a0c81a28d875}{Get\+Model\+Version}} ()
\begin{DoxyCompactList}\small\item\em Returns the D\+X\+NN file format version of the model. \end{DoxyCompactList}\item 
bool \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a9aeb596b415843e7e318c47938444259}{is\+\_\+\+P\+PU}} ()
\begin{DoxyCompactList}\small\item\em Returns whether the model is using P\+PU. (Legacy A\+PI) \end{DoxyCompactList}\item 
bool \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ab6fd915af259467afe1681098b315dca}{Is\+P\+PU}} ()
\begin{DoxyCompactList}\small\item\em Returns whether the model is using P\+PU. \end{DoxyCompactList}\item 
bool \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a498ec2af6a8ebdd4e8edd26cba1faf24}{Is\+Ort\+Configured}} ()
\begin{DoxyCompactList}\small\item\em Checks whether O\+RT (O\+N\+NX Runtime) is configured. \end{DoxyCompactList}\item 
bool \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a896b05b9798d0ae33b3ba25e82e9df4c}{Is\+Multi\+Input\+Model}} () const
\begin{DoxyCompactList}\small\item\em Returns whether the model has multiple input tensors. \end{DoxyCompactList}\item 
int \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ab7e4e88fea3dfcc9c276531f7674c20b}{Get\+Input\+Tensor\+Count}} () const
\begin{DoxyCompactList}\small\item\em Returns the number of input tensors required by the model. \end{DoxyCompactList}\item 
std\+::vector$<$ std\+::string $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a41f090eff09ddb3e3963666eb507d7dc}{Get\+Input\+Tensor\+Names}} () const
\begin{DoxyCompactList}\small\item\em Returns the names of all input tensors in the model. \end{DoxyCompactList}\item 
std\+::vector$<$ std\+::string $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a1edebbe4f7b03f042a550e822ac16284}{Get\+Output\+Tensor\+Names}} () const
\begin{DoxyCompactList}\small\item\em Returns the names of all output tensors in the model. \end{DoxyCompactList}\item 
std\+::map$<$ std\+::string, std\+::string $>$ \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a937480e5d622b1ddcfdd430ed2ba0476}{Get\+Input\+Tensor\+To\+Task\+Mapping}} () const
\begin{DoxyCompactList}\small\item\em Returns the mapping from input tensor names to their target tasks. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_af3feefee6373ecfe7fe5b9b84501a723}\label{classdxrt_1_1InferenceEngine_af3feefee6373ecfe7fe5b9b84501a723}} 
void \mbox{\hyperlink{classdxrt_1_1InferenceEngine_af3feefee6373ecfe7fe5b9b84501a723}{Dispose}} ()
\begin{DoxyCompactList}\small\item\em Resource deallocation and cleanup. \end{DoxyCompactList}\item 
Tensor\+Ptrs \mbox{\hyperlink{classdxrt_1_1InferenceEngine_aff7eaeecfb9fbf7aa1ea8e27d0572b92}{Run\+Multi\+Input}} (const std\+::map$<$ std\+::string, void $\ast$ $>$ \&input\+Tensors, void $\ast$user\+Arg=nullptr, void $\ast$output\+Ptr=nullptr)
\begin{DoxyCompactList}\small\item\em Run inference engine with multiple input tensors for multi-\/input models. \end{DoxyCompactList}\item 
Tensor\+Ptrs \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a4b5c6a8aa5db938b8b50b7d9c844b367}{Run\+Multi\+Input}} (const std\+::vector$<$ void $\ast$ $>$ \&input\+Ptrs, void $\ast$user\+Arg=nullptr, void $\ast$output\+Ptr=nullptr)
\begin{DoxyCompactList}\small\item\em Run inference engine with multiple input tensors (vector format) for multi-\/input models. \end{DoxyCompactList}\item 
bool \mbox{\hyperlink{classdxrt_1_1InferenceEngine_aa227ad7501e794726b9af17e88418cf1}{supports\+Tensor\+Centric\+Offsets}} () const
\begin{DoxyCompactList}\small\item\em Check if tensor-\/centric offset calculation is supported. \end{DoxyCompactList}\item 
uint64\+\_\+t \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a2338c50d4d24b081179372bfea9d5f2c}{get\+Tensor\+Offset}} (const std\+::string \&tensor\+Name) const
\begin{DoxyCompactList}\small\item\em Get the offset of a tensor in the final output buffer. \end{DoxyCompactList}\item 
size\+\_\+t \mbox{\hyperlink{classdxrt_1_1InferenceEngine_aa861889fd3d562742950053868077086}{Get\+Output\+Tensor\+Offset}} (const std\+::string \&tensor\+Name) const
\begin{DoxyCompactList}\small\item\em Gets the offset for a specific output tensor in the final output buffer. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
This class abstracts runtime inference executor for user\textquotesingle{}s compiled model. 

After user loads compiled model to \mbox{\hyperlink{classdxrt_1_1InferenceEngine}{Inference\+Engine}}, real-\/time device tasks will be scheduled by internal runtime libraries. It supports both inference mode (synchronous/asynchronous) according to user\textquotesingle{}s request. 
\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{comment}{// Use default inference option}}
\DoxyCodeLine{\textcolor{keyword}{auto} modelPath = \textcolor{stringliteral}{"model.dxnn"}; \textcolor{comment}{// assume compiled model path name is "model.dxnn"}}
\DoxyCodeLine{\textcolor{keyword}{auto} ie = \mbox{\hyperlink{classdxrt_1_1InferenceEngine}{dxrt::InferenceEngine}}(modelPath, \textcolor{keyword}{nullptr});}
\end{DoxyCode}
 
\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{comment}{// Use a new inference option}}
\DoxyCodeLine{\textcolor{keyword}{auto} modelPath = \textcolor{stringliteral}{"model.dxnn"}; \textcolor{comment}{// assume compiled model path name is "model.dxnn"}}
\DoxyCodeLine{\mbox{\hyperlink{classdxrt_1_1InferenceOption}{dxrt::InferenceOption}} option;}
\DoxyCodeLine{option.\mbox{\hyperlink{classdxrt_1_1InferenceOption_ae98837ff566fe2b089a5b1bf07feb48c}{devices}} = \{0,1,3\};  \textcolor{comment}{//use only 0,1,3 device}}
\DoxyCodeLine{\textcolor{keyword}{auto} ie = \mbox{\hyperlink{classdxrt_1_1InferenceEngine}{dxrt::InferenceEngine}}(modelPath, option);}
\end{DoxyCode}
 

\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_aed622c0fa566620899c9e7e26011bacd}\label{classdxrt_1_1InferenceEngine_aed622c0fa566620899c9e7e26011bacd}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!InferenceEngine@{InferenceEngine}}
\index{InferenceEngine@{InferenceEngine}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{InferenceEngine()}{InferenceEngine()}}
{\footnotesize\ttfamily dxrt\+::\+Inference\+Engine\+::\+Inference\+Engine (\begin{DoxyParamCaption}\item[{const std\+::string \&}]{model\+Path,  }\item[{\mbox{\hyperlink{classdxrt_1_1InferenceOption}{Inference\+Option}} \&}]{option = {\ttfamily \mbox{\hyperlink{namespacedxrt_aab5060680ba2f567f47f8868cd8b1f00}{Default\+Inference\+Option}}} }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [explicit]}}



Perform the task of loading the model and configuring the N\+PU to run. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em model\+Path} & model path \\
\hline
\mbox{\texttt{ in}}  & {\em option} & device and npu core options 
\begin{DoxyCode}{1}
\DoxyCodeLine{\textcolor{keyword}{auto} modelPath = \textcolor{stringliteral}{"model.dxnn"}; \textcolor{comment}{// assume compiled model path name is "model.dxnn"}}
\DoxyCodeLine{\mbox{\hyperlink{classdxrt_1_1InferenceEngine}{dxrt::InferenceEngine}}(modelPath) ie(modelPath);}
\DoxyCodeLine{\textcolor{keyword}{auto} \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a0893b724d6eed939556f1315ce05829d}{outputs}} = ie.Run();}
\DoxyCodeLine{}
\DoxyCodeLine{\mbox{\hyperlink{classdxrt_1_1InferenceOption}{dxrt::InferenceOption}} op;}
\DoxyCodeLine{op.\mbox{\hyperlink{classdxrt_1_1InferenceOption_ae98837ff566fe2b089a5b1bf07feb48c}{devices}}.push\_back(0); }
\DoxyCodeLine{op.\mbox{\hyperlink{classdxrt_1_1InferenceOption_a9fd270a5eeeb1a14ce5f133c16c30d55}{boundOption}} = dxrt::InferenceOption::BOUND\_OPTION::NPU\_0; \textcolor{comment}{// NPU\_0 only}}
\DoxyCodeLine{\mbox{\hyperlink{classdxrt_1_1InferenceEngine}{dxrt::InferenceEngine}} ie(modelPath, op);}
\DoxyCodeLine{\textcolor{keyword}{auto} \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a0893b724d6eed939556f1315ce05829d}{outputs}} = ie.Run();}
\end{DoxyCode}
 \\
\hline
\end{DoxyParams}


\doxysubsection{Member Function Documentation}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_aa8ab6719ae0efdf0f5c678e3401ab57d}\label{classdxrt_1_1InferenceEngine_aa8ab6719ae0efdf0f5c678e3401ab57d}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!bitmatch\_mask@{bitmatch\_mask}}
\index{bitmatch\_mask@{bitmatch\_mask}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{bitmatch\_mask()}{bitmatch\_mask()}}
{\footnotesize\ttfamily std\+::vector$<$uint8\+\_\+t$>$ dxrt\+::\+Inference\+Engine\+::bitmatch\+\_\+mask (\begin{DoxyParamCaption}\item[{int}]{index }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000013}{Deprecated}}]Use Get\+Bitmatch\+Mask() instead. \end{DoxyRefDesc}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a265ff0d27c6bac98c0c641c0e5f8f6ba}\label{classdxrt_1_1InferenceEngine_a265ff0d27c6bac98c0c641c0e5f8f6ba}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!get\_compile\_type@{get\_compile\_type}}
\index{get\_compile\_type@{get\_compile\_type}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{get\_compile\_type()}{get\_compile\_type()}}
{\footnotesize\ttfamily std\+::string dxrt\+::\+Inference\+Engine\+::get\+\_\+compile\+\_\+type (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Returns the compile type of the model. (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000015}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a9308cfc747e8d56119febfd2b48b876c}{Get\+Compile\+Type()}} instead. \end{DoxyRefDesc}
\begin{DoxyReturn}{Returns}
The compile type of the model. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a0b38cd5001ec327cbdc5b00a7143d0e2}\label{classdxrt_1_1InferenceEngine_a0b38cd5001ec327cbdc5b00a7143d0e2}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!get\_num\_tails@{get\_num\_tails}}
\index{get\_num\_tails@{get\_num\_tails}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{get\_num\_tails()}{get\_num\_tails()}}
{\footnotesize\ttfamily int dxrt\+::\+Inference\+Engine\+::get\+\_\+num\+\_\+tails (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Returns the number of tail tasks in the model. (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000014}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a110445fe04396f2d2ea309c18c3bb930}{Get\+Num\+Tail\+Tasks()}} instead. \end{DoxyRefDesc}
\begin{DoxyReturn}{Returns}
The number of tasks that have no subsequent tasks.
\end{DoxyReturn}
Tail tasks are those which do not have any tasks following them in the model\textquotesingle{}s task chain. This function provides the count of such tail tasks. \mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a34dfe51c8f5289bd54ff96be8d64bc12}\label{classdxrt_1_1InferenceEngine_a34dfe51c8f5289bd54ff96be8d64bc12}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!get\_outputs@{get\_outputs}}
\index{get\_outputs@{get\_outputs}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{get\_outputs()}{get\_outputs()}}
{\footnotesize\ttfamily std\+::vector$<$Tensor\+Ptrs$>$ dxrt\+::\+Inference\+Engine\+::get\+\_\+outputs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get output tensors of all tasks (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000012}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ae936aff8999d075130a531f3de60125a}{Get\+All\+Task\+Outputs()}} instead. \end{DoxyRefDesc}
\begin{DoxyReturn}{Returns}
the output of all tasks as a vector of smart pointer instance vectors. ~\newline
 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_ae936aff8999d075130a531f3de60125a}\label{classdxrt_1_1InferenceEngine_ae936aff8999d075130a531f3de60125a}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetAllTaskOutputs@{GetAllTaskOutputs}}
\index{GetAllTaskOutputs@{GetAllTaskOutputs}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetAllTaskOutputs()}{GetAllTaskOutputs()}}
{\footnotesize\ttfamily std\+::vector$<$Tensor\+Ptrs$>$ dxrt\+::\+Inference\+Engine\+::\+Get\+All\+Task\+Outputs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get output tensors of all tasks. 

\begin{DoxyReturn}{Returns}
the output of all Tasks as a vector of smart pointer instance vectors. ~\newline
 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a9308cfc747e8d56119febfd2b48b876c}\label{classdxrt_1_1InferenceEngine_a9308cfc747e8d56119febfd2b48b876c}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetCompileType@{GetCompileType}}
\index{GetCompileType@{GetCompileType}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetCompileType()}{GetCompileType()}}
{\footnotesize\ttfamily std\+::string dxrt\+::\+Inference\+Engine\+::\+Get\+Compile\+Type (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Returns the compile type of the model. 

\begin{DoxyReturn}{Returns}
The compile type of the model. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_adb0043cbbf3dfa50eba37d03b9e7d48e}\label{classdxrt_1_1InferenceEngine_adb0043cbbf3dfa50eba37d03b9e7d48e}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetInputs@{GetInputs}}
\index{GetInputs@{GetInputs}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetInputs()}{GetInputs()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily std\+::vector$<$Tensors$>$ dxrt\+::\+Inference\+Engine\+::\+Get\+Inputs (\begin{DoxyParamCaption}\item[{int}]{dev\+Id }\end{DoxyParamCaption})}



Get input tensor. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em dev\+Id} & device id \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
vector of input tensors 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_abe536ab666125335e08764fa4916c157}\label{classdxrt_1_1InferenceEngine_abe536ab666125335e08764fa4916c157}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetInputs@{GetInputs}}
\index{GetInputs@{GetInputs}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetInputs()}{GetInputs()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Tensors dxrt\+::\+Inference\+Engine\+::\+Get\+Inputs (\begin{DoxyParamCaption}\item[{void $\ast$}]{ptr = {\ttfamily nullptr},  }\item[{uint64\+\_\+t}]{phy\+Addr = {\ttfamily 0} }\end{DoxyParamCaption})}



Get input tensor. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em ptr} & pointer to virtual address \\
\hline
\mbox{\texttt{ in}}  & {\em phy\+Addr} & pointer to physical address \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
if ptr is null, input memory area in engine is returned 

if ptr and phy\+Addr is given, inputs tensors that contains output addresses 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_ad2c11689c4839b5dbb3571107a7b3a7c}\label{classdxrt_1_1InferenceEngine_ad2c11689c4839b5dbb3571107a7b3a7c}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetInputSize@{GetInputSize}}
\index{GetInputSize@{GetInputSize}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetInputSize()}{GetInputSize()}}
{\footnotesize\ttfamily uint64\+\_\+t dxrt\+::\+Inference\+Engine\+::\+Get\+Input\+Size (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get total size of input tensors. 

\begin{DoxyReturn}{Returns}
Input size of one inference in bytes 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_ab7e4e88fea3dfcc9c276531f7674c20b}\label{classdxrt_1_1InferenceEngine_ab7e4e88fea3dfcc9c276531f7674c20b}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetInputTensorCount@{GetInputTensorCount}}
\index{GetInputTensorCount@{GetInputTensorCount}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetInputTensorCount()}{GetInputTensorCount()}}
{\footnotesize\ttfamily int dxrt\+::\+Inference\+Engine\+::\+Get\+Input\+Tensor\+Count (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}



Returns the number of input tensors required by the model. 

\begin{DoxyReturn}{Returns}
The number of input tensors. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a41f090eff09ddb3e3963666eb507d7dc}\label{classdxrt_1_1InferenceEngine_a41f090eff09ddb3e3963666eb507d7dc}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetInputTensorNames@{GetInputTensorNames}}
\index{GetInputTensorNames@{GetInputTensorNames}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetInputTensorNames()}{GetInputTensorNames()}}
{\footnotesize\ttfamily std\+::vector$<$std\+::string$>$ dxrt\+::\+Inference\+Engine\+::\+Get\+Input\+Tensor\+Names (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}



Returns the names of all input tensors in the model. 

\begin{DoxyReturn}{Returns}
Vector of input tensor names in the order they should be provided. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a62b2716dbe26bbca8dac24d4954f8c17}\label{classdxrt_1_1InferenceEngine_a62b2716dbe26bbca8dac24d4954f8c17}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetInputTensorSizes@{GetInputTensorSizes}}
\index{GetInputTensorSizes@{GetInputTensorSizes}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetInputTensorSizes()}{GetInputTensorSizes()}}
{\footnotesize\ttfamily std\+::vector$<$uint64\+\_\+t$>$ dxrt\+::\+Inference\+Engine\+::\+Get\+Input\+Tensor\+Sizes (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get individual input tensor sizes for multi-\/input models. 

\begin{DoxyReturn}{Returns}
Vector of input tensor sizes in bytes, in the order specified by \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a41f090eff09ddb3e3963666eb507d7dc}{Get\+Input\+Tensor\+Names()}} 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a937480e5d622b1ddcfdd430ed2ba0476}\label{classdxrt_1_1InferenceEngine_a937480e5d622b1ddcfdd430ed2ba0476}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetInputTensorToTaskMapping@{GetInputTensorToTaskMapping}}
\index{GetInputTensorToTaskMapping@{GetInputTensorToTaskMapping}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetInputTensorToTaskMapping()}{GetInputTensorToTaskMapping()}}
{\footnotesize\ttfamily std\+::map$<$std\+::string, std\+::string$>$ dxrt\+::\+Inference\+Engine\+::\+Get\+Input\+Tensor\+To\+Task\+Mapping (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}



Returns the mapping from input tensor names to their target tasks. 

\begin{DoxyReturn}{Returns}
Map where key is tensor name and value is task name. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a3758aeb28d7ed75216770a5fd5dd9619}\label{classdxrt_1_1InferenceEngine_a3758aeb28d7ed75216770a5fd5dd9619}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetLatency@{GetLatency}}
\index{GetLatency@{GetLatency}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetLatency()}{GetLatency()}}
{\footnotesize\ttfamily int dxrt\+::\+Inference\+Engine\+::\+Get\+Latency (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get latest latency. 

\begin{DoxyReturn}{Returns}
latency (microseconds) 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a1259543ba3f8fc52efb503e40570f95e}\label{classdxrt_1_1InferenceEngine_a1259543ba3f8fc52efb503e40570f95e}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetLatencyCnt@{GetLatencyCnt}}
\index{GetLatencyCnt@{GetLatencyCnt}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetLatencyCnt()}{GetLatencyCnt()}}
{\footnotesize\ttfamily int dxrt\+::\+Inference\+Engine\+::\+Get\+Latency\+Cnt (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get latency Count. 

\begin{DoxyReturn}{Returns}
latency Count 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a1e81cce5b5db047c17ba40b69bab9a60}\label{classdxrt_1_1InferenceEngine_a1e81cce5b5db047c17ba40b69bab9a60}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetLatencyMean@{GetLatencyMean}}
\index{GetLatencyMean@{GetLatencyMean}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetLatencyMean()}{GetLatencyMean()}}
{\footnotesize\ttfamily double dxrt\+::\+Inference\+Engine\+::\+Get\+Latency\+Mean (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get latency Mean. 

\begin{DoxyReturn}{Returns}
latency Mean (microseconds) 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_ae8bec4ff8e4e0fb6fa680c04f84b642d}\label{classdxrt_1_1InferenceEngine_ae8bec4ff8e4e0fb6fa680c04f84b642d}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetLatencyStdDev@{GetLatencyStdDev}}
\index{GetLatencyStdDev@{GetLatencyStdDev}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetLatencyStdDev()}{GetLatencyStdDev()}}
{\footnotesize\ttfamily double dxrt\+::\+Inference\+Engine\+::\+Get\+Latency\+Std\+Dev (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get latency Standard Deviation. 

\begin{DoxyReturn}{Returns}
latency Standard Deviation 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a331862cfde13a009b3e4133678a1c597}\label{classdxrt_1_1InferenceEngine_a331862cfde13a009b3e4133678a1c597}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetLatencyVector@{GetLatencyVector}}
\index{GetLatencyVector@{GetLatencyVector}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetLatencyVector()}{GetLatencyVector()}}
{\footnotesize\ttfamily std\+::vector$<$int$>$ dxrt\+::\+Inference\+Engine\+::\+Get\+Latency\+Vector (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get recent Latency. 

\begin{DoxyReturn}{Returns}
latency (microseconds) 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a7ed90b0eaa55ce7c097cc7163009c66e}\label{classdxrt_1_1InferenceEngine_a7ed90b0eaa55ce7c097cc7163009c66e}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetModelName@{GetModelName}}
\index{GetModelName@{GetModelName}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetModelName()}{GetModelName()}}
{\footnotesize\ttfamily std\+::string dxrt\+::\+Inference\+Engine\+::\+Get\+Model\+Name (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get model name. 

\begin{DoxyReturn}{Returns}
model name 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a1b3771241a7424353e97a0c81a28d875}\label{classdxrt_1_1InferenceEngine_a1b3771241a7424353e97a0c81a28d875}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetModelVersion@{GetModelVersion}}
\index{GetModelVersion@{GetModelVersion}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetModelVersion()}{GetModelVersion()}}
{\footnotesize\ttfamily std\+::string dxrt\+::\+Inference\+Engine\+::\+Get\+Model\+Version (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Returns the D\+X\+NN file format version of the model. 

\begin{DoxyReturn}{Returns}
The D\+X\+NN file format version as a string. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a1ee64321b5d6c50cb95be282080f326d}\label{classdxrt_1_1InferenceEngine_a1ee64321b5d6c50cb95be282080f326d}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetNpuInferenceTime@{GetNpuInferenceTime}}
\index{GetNpuInferenceTime@{GetNpuInferenceTime}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetNpuInferenceTime()}{GetNpuInferenceTime()}}
{\footnotesize\ttfamily uint32\+\_\+t dxrt\+::\+Inference\+Engine\+::\+Get\+Npu\+Inference\+Time (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get latest inference time. 

\begin{DoxyReturn}{Returns}
inference time (microseconds) 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_ad7a32a700fa89a0213b815d57f8fc1c2}\label{classdxrt_1_1InferenceEngine_ad7a32a700fa89a0213b815d57f8fc1c2}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetNpuInferenceTimeCnt@{GetNpuInferenceTimeCnt}}
\index{GetNpuInferenceTimeCnt@{GetNpuInferenceTimeCnt}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetNpuInferenceTimeCnt()}{GetNpuInferenceTimeCnt()}}
{\footnotesize\ttfamily int dxrt\+::\+Inference\+Engine\+::\+Get\+Npu\+Inference\+Time\+Cnt (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get inference time Count. 

\begin{DoxyReturn}{Returns}
inference time Count 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a0d46e7a44cd5975c5a9c3a76be684dc3}\label{classdxrt_1_1InferenceEngine_a0d46e7a44cd5975c5a9c3a76be684dc3}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetNpuInferenceTimeMean@{GetNpuInferenceTimeMean}}
\index{GetNpuInferenceTimeMean@{GetNpuInferenceTimeMean}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetNpuInferenceTimeMean()}{GetNpuInferenceTimeMean()}}
{\footnotesize\ttfamily double dxrt\+::\+Inference\+Engine\+::\+Get\+Npu\+Inference\+Time\+Mean (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get inference time Mean. 

\begin{DoxyReturn}{Returns}
inference time Mean (microseconds) 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a4d552cb23986c189b1002a3e0b9915a9}\label{classdxrt_1_1InferenceEngine_a4d552cb23986c189b1002a3e0b9915a9}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetNpuInferenceTimeStdDev@{GetNpuInferenceTimeStdDev}}
\index{GetNpuInferenceTimeStdDev@{GetNpuInferenceTimeStdDev}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetNpuInferenceTimeStdDev()}{GetNpuInferenceTimeStdDev()}}
{\footnotesize\ttfamily double dxrt\+::\+Inference\+Engine\+::\+Get\+Npu\+Inference\+Time\+Std\+Dev (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get inference time Standard Deviation. 

\begin{DoxyReturn}{Returns}
inference time Standard Deviation 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_aad79d35e5a285878517eb784439737a9}\label{classdxrt_1_1InferenceEngine_aad79d35e5a285878517eb784439737a9}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetNpuInferenceTimeVector@{GetNpuInferenceTimeVector}}
\index{GetNpuInferenceTimeVector@{GetNpuInferenceTimeVector}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetNpuInferenceTimeVector()}{GetNpuInferenceTimeVector()}}
{\footnotesize\ttfamily std\+::vector$<$uint32\+\_\+t$>$ dxrt\+::\+Inference\+Engine\+::\+Get\+Npu\+Inference\+Time\+Vector (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get recent inference time. 

\begin{DoxyReturn}{Returns}
inference time (microseconds) 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a110445fe04396f2d2ea309c18c3bb930}\label{classdxrt_1_1InferenceEngine_a110445fe04396f2d2ea309c18c3bb930}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetNumTailTasks@{GetNumTailTasks}}
\index{GetNumTailTasks@{GetNumTailTasks}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetNumTailTasks()}{GetNumTailTasks()}}
{\footnotesize\ttfamily int dxrt\+::\+Inference\+Engine\+::\+Get\+Num\+Tail\+Tasks (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Returns the number of tail tasks in the model. 

\begin{DoxyReturn}{Returns}
The number of tasks that have no subsequent tasks.
\end{DoxyReturn}
Tail tasks are those which do not have any tasks following them in the model\textquotesingle{}s task chain. This function provides the count of such tail tasks. \mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a411b78e6c9560119f9a39c30ee1021af}\label{classdxrt_1_1InferenceEngine_a411b78e6c9560119f9a39c30ee1021af}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetOutputs@{GetOutputs}}
\index{GetOutputs@{GetOutputs}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetOutputs()}{GetOutputs()}}
{\footnotesize\ttfamily Tensors dxrt\+::\+Inference\+Engine\+::\+Get\+Outputs (\begin{DoxyParamCaption}\item[{void $\ast$}]{ptr = {\ttfamily nullptr},  }\item[{uint64\+\_\+t}]{phy\+Addr = {\ttfamily 0} }\end{DoxyParamCaption})}



Get output tensor. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em ptr} & pointer to virtual address \\
\hline
\mbox{\texttt{ in}}  & {\em phy\+Addr} & pointer to physical address \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
if ptr is null, output memory area in engine is returned 

if ptr and phy\+Addr is given, outputs tensors that contains output addresses 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a82af5685f6b43643c3ddf47bf0a4ade8}\label{classdxrt_1_1InferenceEngine_a82af5685f6b43643c3ddf47bf0a4ade8}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetOutputSize@{GetOutputSize}}
\index{GetOutputSize@{GetOutputSize}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetOutputSize()}{GetOutputSize()}}
{\footnotesize\ttfamily uint64\+\_\+t dxrt\+::\+Inference\+Engine\+::\+Get\+Output\+Size (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get total size of output tensors. 

\begin{DoxyReturn}{Returns}
Output size of one inference in bytes 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a1edebbe4f7b03f042a550e822ac16284}\label{classdxrt_1_1InferenceEngine_a1edebbe4f7b03f042a550e822ac16284}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetOutputTensorNames@{GetOutputTensorNames}}
\index{GetOutputTensorNames@{GetOutputTensorNames}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetOutputTensorNames()}{GetOutputTensorNames()}}
{\footnotesize\ttfamily std\+::vector$<$std\+::string$>$ dxrt\+::\+Inference\+Engine\+::\+Get\+Output\+Tensor\+Names (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}



Returns the names of all output tensors in the model. 

\begin{DoxyReturn}{Returns}
Vector of output tensor names in the order they are produced. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_aa861889fd3d562742950053868077086}\label{classdxrt_1_1InferenceEngine_aa861889fd3d562742950053868077086}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetOutputTensorOffset@{GetOutputTensorOffset}}
\index{GetOutputTensorOffset@{GetOutputTensorOffset}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetOutputTensorOffset()}{GetOutputTensorOffset()}}
{\footnotesize\ttfamily size\+\_\+t dxrt\+::\+Inference\+Engine\+::\+Get\+Output\+Tensor\+Offset (\begin{DoxyParamCaption}\item[{const std\+::string \&}]{tensor\+Name }\end{DoxyParamCaption}) const}



Gets the offset for a specific output tensor in the final output buffer. 


\begin{DoxyParams}{Parameters}
{\em tensor\+Name} & Name of the output tensor \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Offset in bytes for the tensor in the output buffer 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_aee8bc8dd933efeed07de606fcf2fb161}\label{classdxrt_1_1InferenceEngine_aee8bc8dd933efeed07de606fcf2fb161}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetOutputTensorSizes@{GetOutputTensorSizes}}
\index{GetOutputTensorSizes@{GetOutputTensorSizes}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetOutputTensorSizes()}{GetOutputTensorSizes()}}
{\footnotesize\ttfamily std\+::vector$<$uint64\+\_\+t$>$ dxrt\+::\+Inference\+Engine\+::\+Get\+Output\+Tensor\+Sizes (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get individual output tensor sizes for multi-\/output models. 

\begin{DoxyReturn}{Returns}
Vector of output tensor sizes in bytes, in the order specified by \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a1edebbe4f7b03f042a550e822ac16284}{Get\+Output\+Tensor\+Names()}} 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a8b759a7bb098d710e1c3052170057143}\label{classdxrt_1_1InferenceEngine_a8b759a7bb098d710e1c3052170057143}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!GetTaskOrder@{GetTaskOrder}}
\index{GetTaskOrder@{GetTaskOrder}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{GetTaskOrder()}{GetTaskOrder()}}
{\footnotesize\ttfamily std\+::vector$<$std\+::string$>$ dxrt\+::\+Inference\+Engine\+::\+Get\+Task\+Order (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Get model task order. 

\begin{DoxyReturn}{Returns}
task order 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a2338c50d4d24b081179372bfea9d5f2c}\label{classdxrt_1_1InferenceEngine_a2338c50d4d24b081179372bfea9d5f2c}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!getTensorOffset@{getTensorOffset}}
\index{getTensorOffset@{getTensorOffset}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{getTensorOffset()}{getTensorOffset()}}
{\footnotesize\ttfamily uint64\+\_\+t dxrt\+::\+Inference\+Engine\+::get\+Tensor\+Offset (\begin{DoxyParamCaption}\item[{const std\+::string \&}]{tensor\+Name }\end{DoxyParamCaption}) const}



Get the offset of a tensor in the final output buffer. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em tensor\+Name} & Name of the tensor \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Offset in bytes, or 0 if tensor not found 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_aad2f3b616e0e996b66ef7df37dc16a85}\label{classdxrt_1_1InferenceEngine_aad2f3b616e0e996b66ef7df37dc16a85}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!inference\_time@{inference\_time}}
\index{inference\_time@{inference\_time}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{inference\_time()}{inference\_time()}}
{\footnotesize\ttfamily uint32\+\_\+t dxrt\+::\+Inference\+Engine\+::inference\+\_\+time (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get latest inference time (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000011}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a1ee64321b5d6c50cb95be282080f326d}{Get\+Npu\+Inference\+Time()}} instead. ~\newline
 \end{DoxyRefDesc}
\begin{DoxyReturn}{Returns}
inference time (microseconds) 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a96c4e5953a0daf4a1c886002c052a6ba}\label{classdxrt_1_1InferenceEngine_a96c4e5953a0daf4a1c886002c052a6ba}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!input\_size@{input\_size}}
\index{input\_size@{input\_size}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{input\_size()}{input\_size()}}
{\footnotesize\ttfamily uint64\+\_\+t dxrt\+::\+Inference\+Engine\+::input\+\_\+size (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get total size of input tensors (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000006}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ad2c11689c4839b5dbb3571107a7b3a7c}{Get\+Input\+Size()}} instead. \end{DoxyRefDesc}
\begin{DoxyReturn}{Returns}
Input size of one inference in bytes 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_af81b48e8ccd95953dd9bc483e5ec678b}\label{classdxrt_1_1InferenceEngine_af81b48e8ccd95953dd9bc483e5ec678b}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!inputs@{inputs}}
\index{inputs@{inputs}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{inputs()}{inputs()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily std\+::vector$<$Tensors$>$ dxrt\+::\+Inference\+Engine\+::inputs (\begin{DoxyParamCaption}\item[{int}]{dev\+Id }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get input tensor (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000004}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_abe536ab666125335e08764fa4916c157}{Get\+Inputs()}} instead. \end{DoxyRefDesc}

\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em dev\+Id} & device id \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
vector of input tensors 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_afbee8b031fdd6c0a3916ed00d08c9cd8}\label{classdxrt_1_1InferenceEngine_afbee8b031fdd6c0a3916ed00d08c9cd8}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!inputs@{inputs}}
\index{inputs@{inputs}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{inputs()}{inputs()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Tensors dxrt\+::\+Inference\+Engine\+::inputs (\begin{DoxyParamCaption}\item[{void $\ast$}]{ptr = {\ttfamily nullptr},  }\item[{uint64\+\_\+t}]{phy\+Addr = {\ttfamily 0} }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get input tensor (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000003}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_abe536ab666125335e08764fa4916c157}{Get\+Inputs()}} instead. \end{DoxyRefDesc}

\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em ptr} & pointer to virtual address \\
\hline
\mbox{\texttt{ in}}  & {\em phy\+Addr} & pointer to physical address \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
if ptr is null, input memory area in engine is returned 

if ptr and phy\+Addr is given, inputs tensors that contains output addresses 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a9aeb596b415843e7e318c47938444259}\label{classdxrt_1_1InferenceEngine_a9aeb596b415843e7e318c47938444259}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!is\_PPU@{is\_PPU}}
\index{is\_PPU@{is\_PPU}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{is\_PPU()}{is\_PPU()}}
{\footnotesize\ttfamily bool dxrt\+::\+Inference\+Engine\+::is\+\_\+\+P\+PU (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Returns whether the model is using P\+PU. (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000016}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_ab6fd915af259467afe1681098b315dca}{Is\+P\+P\+U()}} instead. \end{DoxyRefDesc}
\begin{DoxyReturn}{Returns}
whether the model is using P\+PU. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a896b05b9798d0ae33b3ba25e82e9df4c}\label{classdxrt_1_1InferenceEngine_a896b05b9798d0ae33b3ba25e82e9df4c}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!IsMultiInputModel@{IsMultiInputModel}}
\index{IsMultiInputModel@{IsMultiInputModel}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{IsMultiInputModel()}{IsMultiInputModel()}}
{\footnotesize\ttfamily bool dxrt\+::\+Inference\+Engine\+::\+Is\+Multi\+Input\+Model (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}



Returns whether the model has multiple input tensors. 

\begin{DoxyReturn}{Returns}
true if the model has multiple input tensors, false otherwise. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a498ec2af6a8ebdd4e8edd26cba1faf24}\label{classdxrt_1_1InferenceEngine_a498ec2af6a8ebdd4e8edd26cba1faf24}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!IsOrtConfigured@{IsOrtConfigured}}
\index{IsOrtConfigured@{IsOrtConfigured}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{IsOrtConfigured()}{IsOrtConfigured()}}
{\footnotesize\ttfamily bool dxrt\+::\+Inference\+Engine\+::\+Is\+Ort\+Configured (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Checks whether O\+RT (O\+N\+NX Runtime) is configured. 

\begin{DoxyReturn}{Returns}
true if O\+RT is configured, false otherwise. 
\end{DoxyReturn}

\begin{DoxyExceptions}{Exceptions}
{\em Invalid\+Argument\+Exception} & If U\+S\+E\+\_\+\+O\+RT is O\+FF but the use\+O\+RT option is set to true. \\
\hline
\end{DoxyExceptions}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_ab6fd915af259467afe1681098b315dca}\label{classdxrt_1_1InferenceEngine_ab6fd915af259467afe1681098b315dca}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!IsPPU@{IsPPU}}
\index{IsPPU@{IsPPU}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{IsPPU()}{IsPPU()}}
{\footnotesize\ttfamily bool dxrt\+::\+Inference\+Engine\+::\+Is\+P\+PU (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Returns whether the model is using P\+PU. 

\begin{DoxyReturn}{Returns}
whether the model is using P\+PU. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a990ca720e7f8a28ac5394206eb6f9491}\label{classdxrt_1_1InferenceEngine_a990ca720e7f8a28ac5394206eb6f9491}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!latency@{latency}}
\index{latency@{latency}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{latency()}{latency()}}
{\footnotesize\ttfamily int dxrt\+::\+Inference\+Engine\+::latency (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get latest latency (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000010}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a3758aeb28d7ed75216770a5fd5dd9619}{Get\+Latency()}} instead. \end{DoxyRefDesc}
\begin{DoxyReturn}{Returns}
latency (microseconds) 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a3fb16094508bd367cc4971238f002765}\label{classdxrt_1_1InferenceEngine_a3fb16094508bd367cc4971238f002765}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!name@{name}}
\index{name@{name}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{name()}{name()}}
{\footnotesize\ttfamily std\+::string dxrt\+::\+Inference\+Engine\+::name (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get model name (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000008}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a7ed90b0eaa55ce7c097cc7163009c66e}{Get\+Model\+Name()}} instead. \end{DoxyRefDesc}
\begin{DoxyReturn}{Returns}
model name 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a93e1d7b460b7f8656e725ed708de507b}\label{classdxrt_1_1InferenceEngine_a93e1d7b460b7f8656e725ed708de507b}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!output\_size@{output\_size}}
\index{output\_size@{output\_size}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{output\_size()}{output\_size()}}
{\footnotesize\ttfamily uint64\+\_\+t dxrt\+::\+Inference\+Engine\+::output\+\_\+size (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get total size of output tensors (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000007}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a82af5685f6b43643c3ddf47bf0a4ade8}{Get\+Output\+Size()}} instead. \end{DoxyRefDesc}
\begin{DoxyReturn}{Returns}
Output size of one inference in bytes 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a0893b724d6eed939556f1315ce05829d}\label{classdxrt_1_1InferenceEngine_a0893b724d6eed939556f1315ce05829d}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!outputs@{outputs}}
\index{outputs@{outputs}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{outputs()}{outputs()}}
{\footnotesize\ttfamily Tensors dxrt\+::\+Inference\+Engine\+::outputs (\begin{DoxyParamCaption}\item[{void $\ast$}]{ptr = {\ttfamily nullptr},  }\item[{uint64\+\_\+t}]{phy\+Addr = {\ttfamily 0} }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get output tensor (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000005}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a411b78e6c9560119f9a39c30ee1021af}{Get\+Outputs()}} instead. \end{DoxyRefDesc}

\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em ptr} & pointer to virtual address \\
\hline
\mbox{\texttt{ in}}  & {\em phy\+Addr} & pointer to physical address \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
if ptr is null, output memory area in engine is returned 

if ptr and phy\+Addr is given, outputs tensors that contains output addresses 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a3d3fc75faa79470d9a3bdbdc61561a3a}\label{classdxrt_1_1InferenceEngine_a3d3fc75faa79470d9a3bdbdc61561a3a}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!RegisterCallBack@{RegisterCallBack}}
\index{RegisterCallBack@{RegisterCallBack}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{RegisterCallBack()}{RegisterCallBack()}}
{\footnotesize\ttfamily void dxrt\+::\+Inference\+Engine\+::\+Register\+Call\+Back (\begin{DoxyParamCaption}\item[{std\+::function$<$ int(Tensor\+Ptrs \&\mbox{\hyperlink{classdxrt_1_1InferenceEngine_a0893b724d6eed939556f1315ce05829d}{outputs}}, void $\ast$user\+Arg)$>$}]{callback\+Func }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Register user callback function to be called by inference completion. (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000002}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a1d8b33fa324a48b43433c401bd54280a}{Register\+Callback()}} instead. \end{DoxyRefDesc}

\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em callback\+Func} & Function which is called when inference is complete, it gets outputs and user\+\_\+arg ptr \\
\hline
 & {\em outputs} & output tensors data \\
\hline
 & {\em user\+Arg} & user\+Arg given by \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a88cea5324cbeb436034adb60cf31050f}{Run()}}; \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a1d8b33fa324a48b43433c401bd54280a}\label{classdxrt_1_1InferenceEngine_a1d8b33fa324a48b43433c401bd54280a}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!RegisterCallback@{RegisterCallback}}
\index{RegisterCallback@{RegisterCallback}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{RegisterCallback()}{RegisterCallback()}}
{\footnotesize\ttfamily void dxrt\+::\+Inference\+Engine\+::\+Register\+Callback (\begin{DoxyParamCaption}\item[{std\+::function$<$ int(Tensor\+Ptrs \&\mbox{\hyperlink{classdxrt_1_1InferenceEngine_a0893b724d6eed939556f1315ce05829d}{outputs}}, void $\ast$user\+Arg)$>$}]{callback\+Func }\end{DoxyParamCaption})}



Register user callback function to be called by inference completion. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em callback\+Func} & Function which is called when inference is complete, it gets outputs and user\+\_\+arg ptr \\
\hline
 & {\em outputs} & output tensors data \\
\hline
 & {\em user\+Arg} & user\+Arg given by \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a88cea5324cbeb436034adb60cf31050f}{Run()}}; \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_ab54d666cc65cef4d6ae2fe4c300d7ba1}\label{classdxrt_1_1InferenceEngine_ab54d666cc65cef4d6ae2fe4c300d7ba1}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!Run@{Run}}
\index{Run@{Run}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{Run()}{Run()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily std\+::vector$<$Tensor\+Ptrs$>$ dxrt\+::\+Inference\+Engine\+::\+Run (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ void $\ast$ $>$ \&}]{input\+Buffers,  }\item[{const std\+::vector$<$ void $\ast$ $>$ \&}]{output\+Buffers,  }\item[{const std\+::vector$<$ void $\ast$ $>$ \&}]{user\+Args = {\ttfamily \{\}} }\end{DoxyParamCaption})}



Runs the inference engine using a specific input pointer vector. 

This function executes inference based on the provided input data pointer vector and returns a vector of output tensors. Users can optionally provide additional user-\/defined arguments and output pointer vectors. If user\+Args is used, the number of elements in input\+Ptrs must be the same as the number of elements in user\+Args. An Invalid\+Argument\+Exception is thrown if the size of input\+Ptrs and user\+Args are different. An Invalid\+Argument\+Exception is thrown if the size of input\+Ptrs and Output\+Ptrs are different. An Invalid\+Argument\+Exception is thrown if the size of input\+Ptrs is 0.


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Buffers} & Vector of input data pointers used for inference. \\
\hline
\mbox{\texttt{ out}}  & {\em output\+Buffers} & Vector of output data pointers. \\
\hline
\mbox{\texttt{ in}}  & {\em user\+Args} & Vector of user-\/defined arguments (e.\+g., original frame data, input metadata, etc.). (Optional)\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Vector of output tensors as smart pointer instances. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a88cea5324cbeb436034adb60cf31050f}\label{classdxrt_1_1InferenceEngine_a88cea5324cbeb436034adb60cf31050f}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!Run@{Run}}
\index{Run@{Run}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{Run()}{Run()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Tensor\+Ptrs dxrt\+::\+Inference\+Engine\+::\+Run (\begin{DoxyParamCaption}\item[{void $\ast$}]{input\+Ptr,  }\item[{void $\ast$}]{user\+Arg = {\ttfamily nullptr},  }\item[{void $\ast$}]{output\+Ptr = {\ttfamily nullptr} }\end{DoxyParamCaption})}



Run inference engine using specific input pointer Synchronously. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Ptr} & input data pointer to run inference \\
\hline
\mbox{\texttt{ in}}  & {\em user\+Arg} & user-\/defined arguments as a void pointer(e.\+g. original frame data, metadata about input, ... ) \\
\hline
\mbox{\texttt{ out}}  & {\em output\+Ptr} & pointer to output data, if it is nullptr, output data is stored in buffer inside D\+X\+RT. 
\begin{DoxyCode}{1}
\DoxyCodeLine{\textcolor{keyword}{auto} modelPath = \textcolor{stringliteral}{"model.dxnn"}; \textcolor{comment}{// assume compiled model path name is "model.dxnn"}}
\DoxyCodeLine{\mbox{\hyperlink{classdxrt_1_1InferenceEngine}{dxrt::InferenceEngine}} ie(modelPath);}
\DoxyCodeLine{\textcolor{keyword}{auto} \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a0893b724d6eed939556f1315ce05829d}{outputs}} = ie.Run();}
\end{DoxyCode}
 \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
output tensors as vector of smart pointer instances 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_ac9c8166a8c588860ea4a5fc29d743eb5}\label{classdxrt_1_1InferenceEngine_ac9c8166a8c588860ea4a5fc29d743eb5}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!RunAsync@{RunAsync}}
\index{RunAsync@{RunAsync}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{RunAsync()}{RunAsync()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily int dxrt\+::\+Inference\+Engine\+::\+Run\+Async (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ void $\ast$ $>$ \&}]{input\+Ptrs,  }\item[{void $\ast$}]{user\+Arg = {\ttfamily nullptr},  }\item[{void $\ast$}]{output\+Ptr = {\ttfamily nullptr} }\end{DoxyParamCaption})}



Non-\/blocking call to request asynchronous inference with automatic multi-\/input detection. This function automatically detects whether the input should be interpreted as multi-\/input single inference or batch inference based on the model requirements and input count. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Ptrs} & Vector of input data pointers \\
\hline
\mbox{\texttt{ in}}  & {\em user\+Arg} & user-\/defined arguments as a void pointer \\
\hline
\mbox{\texttt{ out}}  & {\em output\+Ptr} & pointer to output data, if it is nullptr, output data area is allocated by D\+X\+RT. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
job id that can be used to wait() function 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_af3006c9a7183f7e42d1528aa8a6fea24}\label{classdxrt_1_1InferenceEngine_af3006c9a7183f7e42d1528aa8a6fea24}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!RunAsync@{RunAsync}}
\index{RunAsync@{RunAsync}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{RunAsync()}{RunAsync()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily int dxrt\+::\+Inference\+Engine\+::\+Run\+Async (\begin{DoxyParamCaption}\item[{void $\ast$}]{input\+Ptr,  }\item[{void $\ast$}]{user\+Arg = {\ttfamily nullptr},  }\item[{void $\ast$}]{output\+Ptr = {\ttfamily nullptr} }\end{DoxyParamCaption})}



Non-\/blocking call to request asynchronous inference by input pointer, and get job ID from inference engine. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Ptr} & input data pointer to run inference \\
\hline
\mbox{\texttt{ in}}  & {\em user\+Arg} & user-\/defined arguments as a void pointer(e.\+g. original frame data, metadata about input, ... ) \\
\hline
\mbox{\texttt{ out}}  & {\em output\+Ptr} & pointer to output data, if it is nullptr, output data area is allocated by D\+X\+RT. 
\begin{DoxyCode}{1}
\DoxyCodeLine{\textcolor{keyword}{auto} modelPath = \textcolor{stringliteral}{"model"}; \textcolor{comment}{// assume compiled model path name is "model"}}
\DoxyCodeLine{\mbox{\hyperlink{classdxrt_1_1InferenceOption}{dxrt::InferenceOption}} option;}
\DoxyCodeLine{option.\mbox{\hyperlink{classdxrt_1_1InferenceOption_ae98837ff566fe2b089a5b1bf07feb48c}{devices}} = \{0,1,3\};  \textcolor{comment}{//use only 0,1,3 device}}
\DoxyCodeLine{\mbox{\hyperlink{classdxrt_1_1InferenceEngine}{dxrt::InferenceEngine}} ie(modelPath, option);}
\DoxyCodeLine{\textcolor{keyword}{auto} \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a0893b724d6eed939556f1315ce05829d}{outputs}} = ie.Run();}
\end{DoxyCode}
 \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
job id that can be used to wait() function 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a6a5b11e470a9dfeca6c60f644726ea59}\label{classdxrt_1_1InferenceEngine_a6a5b11e470a9dfeca6c60f644726ea59}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!RunAsyncMultiInput@{RunAsyncMultiInput}}
\index{RunAsyncMultiInput@{RunAsyncMultiInput}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{RunAsyncMultiInput()}{RunAsyncMultiInput()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily int dxrt\+::\+Inference\+Engine\+::\+Run\+Async\+Multi\+Input (\begin{DoxyParamCaption}\item[{const std\+::map$<$ std\+::string, void $\ast$ $>$ \&}]{input\+Tensors,  }\item[{void $\ast$}]{user\+Arg = {\ttfamily nullptr},  }\item[{void $\ast$}]{output\+Ptr = {\ttfamily nullptr} }\end{DoxyParamCaption})}



Run async inference with multiple input tensors for multi-\/input models. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Tensors} & Map of tensor name to input data pointer \\
\hline
\mbox{\texttt{ in}}  & {\em user\+Arg} & user-\/defined arguments as a void pointer \\
\hline
\mbox{\texttt{ out}}  & {\em output\+Ptr} & pointer to output data, if it is nullptr, output data is stored in buffer inside D\+X\+RT. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
job id that can be used to wait() function 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a8a61dde8d97418d77e5bfd348011517f}\label{classdxrt_1_1InferenceEngine_a8a61dde8d97418d77e5bfd348011517f}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!RunAsyncMultiInput@{RunAsyncMultiInput}}
\index{RunAsyncMultiInput@{RunAsyncMultiInput}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{RunAsyncMultiInput()}{RunAsyncMultiInput()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily int dxrt\+::\+Inference\+Engine\+::\+Run\+Async\+Multi\+Input (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ void $\ast$ $>$ \&}]{input\+Ptrs,  }\item[{void $\ast$}]{user\+Arg = {\ttfamily nullptr},  }\item[{void $\ast$}]{output\+Ptr = {\ttfamily nullptr} }\end{DoxyParamCaption})}



Run async inference with multiple input tensors (vector format) for multi-\/input models. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Ptrs} & Vector of input data pointers in the order specified by \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a41f090eff09ddb3e3963666eb507d7dc}{Get\+Input\+Tensor\+Names()}} \\
\hline
\mbox{\texttt{ in}}  & {\em user\+Arg} & user-\/defined arguments as a void pointer \\
\hline
\mbox{\texttt{ out}}  & {\em output\+Ptr} & pointer to output data, if it is nullptr, output data is stored in buffer inside D\+X\+RT. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
job id that can be used to wait() function 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_ab0c6629f048341d62bfa3a400c90e827}\label{classdxrt_1_1InferenceEngine_ab0c6629f048341d62bfa3a400c90e827}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!RunBenchMark@{RunBenchMark}}
\index{RunBenchMark@{RunBenchMark}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{RunBenchMark()}{RunBenchMark()}}
{\footnotesize\ttfamily float dxrt\+::\+Inference\+Engine\+::\+Run\+Bench\+Mark (\begin{DoxyParamCaption}\item[{int}]{num,  }\item[{void $\ast$}]{input\+Ptr = {\ttfamily nullptr} }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



run benchmark with loop n times (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000001}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a3846a2f47ea39933136e589aad036eb0}{Run\+Benchmark()}} instead. \end{DoxyRefDesc}

\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em num} & number of inferences \\
\hline
\mbox{\texttt{ in}}  & {\em input\+Ptr} & input data pointer to run inference \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
average fps 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a3846a2f47ea39933136e589aad036eb0}\label{classdxrt_1_1InferenceEngine_a3846a2f47ea39933136e589aad036eb0}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!RunBenchmark@{RunBenchmark}}
\index{RunBenchmark@{RunBenchmark}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{RunBenchmark()}{RunBenchmark()}}
{\footnotesize\ttfamily float dxrt\+::\+Inference\+Engine\+::\+Run\+Benchmark (\begin{DoxyParamCaption}\item[{int}]{num,  }\item[{void $\ast$}]{input\+Ptr = {\ttfamily nullptr} }\end{DoxyParamCaption})}



run benchmark with loop n times 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em num} & number of inferences \\
\hline
\mbox{\texttt{ in}}  & {\em input\+Ptr} & input data pointer to run inference \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
average fps 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_aff7eaeecfb9fbf7aa1ea8e27d0572b92}\label{classdxrt_1_1InferenceEngine_aff7eaeecfb9fbf7aa1ea8e27d0572b92}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!RunMultiInput@{RunMultiInput}}
\index{RunMultiInput@{RunMultiInput}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{RunMultiInput()}{RunMultiInput()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Tensor\+Ptrs dxrt\+::\+Inference\+Engine\+::\+Run\+Multi\+Input (\begin{DoxyParamCaption}\item[{const std\+::map$<$ std\+::string, void $\ast$ $>$ \&}]{input\+Tensors,  }\item[{void $\ast$}]{user\+Arg = {\ttfamily nullptr},  }\item[{void $\ast$}]{output\+Ptr = {\ttfamily nullptr} }\end{DoxyParamCaption})}



Run inference engine with multiple input tensors for multi-\/input models. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Tensors} & Map of tensor name to input data pointer \\
\hline
\mbox{\texttt{ in}}  & {\em user\+Arg} & user-\/defined arguments as a void pointer \\
\hline
\mbox{\texttt{ out}}  & {\em output\+Ptr} & pointer to output data, if it is nullptr, output data is stored in buffer inside D\+X\+RT. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
output tensors as vector of smart pointer instances 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a4b5c6a8aa5db938b8b50b7d9c844b367}\label{classdxrt_1_1InferenceEngine_a4b5c6a8aa5db938b8b50b7d9c844b367}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!RunMultiInput@{RunMultiInput}}
\index{RunMultiInput@{RunMultiInput}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{RunMultiInput()}{RunMultiInput()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Tensor\+Ptrs dxrt\+::\+Inference\+Engine\+::\+Run\+Multi\+Input (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ void $\ast$ $>$ \&}]{input\+Ptrs,  }\item[{void $\ast$}]{user\+Arg = {\ttfamily nullptr},  }\item[{void $\ast$}]{output\+Ptr = {\ttfamily nullptr} }\end{DoxyParamCaption})}



Run inference engine with multiple input tensors (vector format) for multi-\/input models. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Ptrs} & Vector of input data pointers in the order specified by \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a41f090eff09ddb3e3963666eb507d7dc}{Get\+Input\+Tensor\+Names()}} \\
\hline
\mbox{\texttt{ in}}  & {\em user\+Arg} & user-\/defined arguments as a void pointer \\
\hline
\mbox{\texttt{ out}}  & {\em output\+Ptr} & pointer to output data, if it is nullptr, output data is stored in buffer inside D\+X\+RT. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
output tensors as vector of smart pointer instances 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_aa227ad7501e794726b9af17e88418cf1}\label{classdxrt_1_1InferenceEngine_aa227ad7501e794726b9af17e88418cf1}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!supportsTensorCentricOffsets@{supportsTensorCentricOffsets}}
\index{supportsTensorCentricOffsets@{supportsTensorCentricOffsets}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{supportsTensorCentricOffsets()}{supportsTensorCentricOffsets()}}
{\footnotesize\ttfamily bool dxrt\+::\+Inference\+Engine\+::supports\+Tensor\+Centric\+Offsets (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}



Check if tensor-\/centric offset calculation is supported. 

\begin{DoxyReturn}{Returns}
true if supported, false otherwise 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a578a0098f9b49902fa4c81d95945baa4}\label{classdxrt_1_1InferenceEngine_a578a0098f9b49902fa4c81d95945baa4}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!task\_order@{task\_order}}
\index{task\_order@{task\_order}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{task\_order()}{task\_order()}}
{\footnotesize\ttfamily std\+::vector$<$std\+::string$>$ dxrt\+::\+Inference\+Engine\+::task\+\_\+order (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get model task order (Legacy A\+PI) 

\begin{DoxyRefDesc}{Deprecated}
\item[\mbox{\hyperlink{deprecated__deprecated000009}{Deprecated}}]Use \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a8b759a7bb098d710e1c3052170057143}{Get\+Task\+Order()}} instead. \end{DoxyRefDesc}
\begin{DoxyReturn}{Returns}
task order 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a2cbf5e6220f5be2cd9132f2570129faf}\label{classdxrt_1_1InferenceEngine_a2cbf5e6220f5be2cd9132f2570129faf}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!ValidateDevice@{ValidateDevice}}
\index{ValidateDevice@{ValidateDevice}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{ValidateDevice()}{ValidateDevice()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Tensor\+Ptrs dxrt\+::\+Inference\+Engine\+::\+Validate\+Device (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ void $\ast$ $>$ \&}]{input\+Ptrs,  }\item[{int}]{device\+Id = {\ttfamily 0} }\end{DoxyParamCaption})}



Validate inference of a specific N\+PU device with automatic multi-\/input detection. This function automatically detects whether the input should be interpreted as multi-\/input based on the model requirements and input count. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Ptrs} & Vector of input data pointers for validation. \\
\hline
\mbox{\texttt{ in}}  & {\em device\+Id} & ID of the N\+PU device to validate. Default is 0 (first device). \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Output tensors as a vector of smart pointer instances, representing the validation results. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_aef4c9502a5d5a9d256d3899391376aba}\label{classdxrt_1_1InferenceEngine_aef4c9502a5d5a9d256d3899391376aba}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!ValidateDevice@{ValidateDevice}}
\index{ValidateDevice@{ValidateDevice}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{ValidateDevice()}{ValidateDevice()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Tensor\+Ptrs dxrt\+::\+Inference\+Engine\+::\+Validate\+Device (\begin{DoxyParamCaption}\item[{void $\ast$}]{input\+Ptr,  }\item[{int}]{device\+Id = {\ttfamily 0} }\end{DoxyParamCaption})}



Validate inference of a specific N\+PU device connected to the host. This function runs a validation process using the provided input data on the specified N\+PU device. It can be used to ensure that the N\+PU device is operational and can process inference tasks correctly. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Ptr} & Pointer to the input data used for validation. \\
\hline
\mbox{\texttt{ in}}  & {\em device\+Id} & ID of the N\+PU device to validate. Default is 0 (first device). \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Output tensors as a vector of smart pointer instances, representing the validation results. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a5b6d78e715a91712548b905d2c730a0c}\label{classdxrt_1_1InferenceEngine_a5b6d78e715a91712548b905d2c730a0c}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!ValidateDeviceMultiInput@{ValidateDeviceMultiInput}}
\index{ValidateDeviceMultiInput@{ValidateDeviceMultiInput}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{ValidateDeviceMultiInput()}{ValidateDeviceMultiInput()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Tensor\+Ptrs dxrt\+::\+Inference\+Engine\+::\+Validate\+Device\+Multi\+Input (\begin{DoxyParamCaption}\item[{const std\+::map$<$ std\+::string, void $\ast$ $>$ \&}]{input\+Tensors,  }\item[{int}]{device\+Id = {\ttfamily 0} }\end{DoxyParamCaption})}



Validate N\+PU device with multiple input tensors for multi-\/input models. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Tensors} & Map of tensor name to input data pointer \\
\hline
\mbox{\texttt{ in}}  & {\em device\+Id} & ID of the N\+PU device to validate. Default is 0 (first device). \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Output tensors as a vector of smart pointer instances, representing the validation results. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_af252439b3ba07ea017792c4fe6d82c7a}\label{classdxrt_1_1InferenceEngine_af252439b3ba07ea017792c4fe6d82c7a}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!ValidateDeviceMultiInput@{ValidateDeviceMultiInput}}
\index{ValidateDeviceMultiInput@{ValidateDeviceMultiInput}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{ValidateDeviceMultiInput()}{ValidateDeviceMultiInput()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Tensor\+Ptrs dxrt\+::\+Inference\+Engine\+::\+Validate\+Device\+Multi\+Input (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ void $\ast$ $>$ \&}]{input\+Ptrs,  }\item[{int}]{device\+Id = {\ttfamily 0} }\end{DoxyParamCaption})}



Validate N\+PU device with multiple input tensors (vector format) for multi-\/input models. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em input\+Ptrs} & Vector of input data pointers in the order specified by \mbox{\hyperlink{classdxrt_1_1InferenceEngine_a41f090eff09ddb3e3963666eb507d7dc}{Get\+Input\+Tensor\+Names()}} \\
\hline
\mbox{\texttt{ in}}  & {\em device\+Id} & ID of the N\+PU device to validate. Default is 0 (first device). \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Output tensors as a vector of smart pointer instances, representing the validation results. 
\end{DoxyReturn}
\mbox{\Hypertarget{classdxrt_1_1InferenceEngine_a2c6f130d722950153287d4cf1efd5a5d}\label{classdxrt_1_1InferenceEngine_a2c6f130d722950153287d4cf1efd5a5d}} 
\index{dxrt::InferenceEngine@{dxrt::InferenceEngine}!Wait@{Wait}}
\index{Wait@{Wait}!dxrt::InferenceEngine@{dxrt::InferenceEngine}}
\doxysubsubsection{\texorpdfstring{Wait()}{Wait()}}
{\footnotesize\ttfamily Tensor\+Ptrs dxrt\+::\+Inference\+Engine\+::\+Wait (\begin{DoxyParamCaption}\item[{int}]{job\+Id }\end{DoxyParamCaption})}



Wait until an request is complete and returns output. 


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em job\+Id} & job Id returned by \mbox{\hyperlink{classdxrt_1_1InferenceEngine_af3006c9a7183f7e42d1528aa8a6fea24}{Run\+Async()}} \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
output tensors as vector of smart pointer instances 
\end{DoxyReturn}
