<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>DXRT: dxrt::InferenceEngine Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">DXRT
   </div>
   <div id="projectbrief">DEEPX Runtime SDK for AI inference on DEEPX devices.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacedxrt.html">dxrt</a></li><li class="navelem"><a class="el" href="classdxrt_1_1InferenceEngine.html">InferenceEngine</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classdxrt_1_1InferenceEngine-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">dxrt::InferenceEngine Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>This class abstracts runtime inference executor for user's compiled model.  
 <a href="classdxrt_1_1InferenceEngine.html#details">More...</a></p>

<p><code>#include &quot;<a class="el" href="dxrt__api_8h_source.html">dxrt/dxrt_api.h</a>&quot;</code></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:aed622c0fa566620899c9e7e26011bacd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aed622c0fa566620899c9e7e26011bacd">InferenceEngine</a> (const std::string &amp;modelPath, <a class="el" href="classdxrt_1_1InferenceOption.html">InferenceOption</a> &amp;option=<a class="el" href="namespacedxrt.html#aab5060680ba2f567f47f8868cd8b1f00">DefaultInferenceOption</a>)</td></tr>
<tr class="memdesc:aed622c0fa566620899c9e7e26011bacd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Perform the task of loading the model and configuring the NPU to run.  <a href="classdxrt_1_1InferenceEngine.html#aed622c0fa566620899c9e7e26011bacd">More...</a><br /></td></tr>
<tr class="separator:aed622c0fa566620899c9e7e26011bacd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a88cea5324cbeb436034adb60cf31050f"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a88cea5324cbeb436034adb60cf31050f">Run</a> (void *inputPtr, void *userArg=nullptr, void *outputPtr=nullptr)</td></tr>
<tr class="memdesc:a88cea5324cbeb436034adb60cf31050f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Run inference engine using specific input pointer Synchronously.  <a href="classdxrt_1_1InferenceEngine.html#a88cea5324cbeb436034adb60cf31050f">More...</a><br /></td></tr>
<tr class="separator:a88cea5324cbeb436034adb60cf31050f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab54d666cc65cef4d6ae2fe4c300d7ba1"><td class="memItemLeft" align="right" valign="top">std::vector&lt; TensorPtrs &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ab54d666cc65cef4d6ae2fe4c300d7ba1">Run</a> (const std::vector&lt; void * &gt; &amp;inputBuffers, const std::vector&lt; void * &gt; &amp;outputBuffers, const std::vector&lt; void * &gt; &amp;userArgs={})</td></tr>
<tr class="memdesc:ab54d666cc65cef4d6ae2fe4c300d7ba1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Runs the inference engine using a specific input pointer vector.  <a href="classdxrt_1_1InferenceEngine.html#ab54d666cc65cef4d6ae2fe4c300d7ba1">More...</a><br /></td></tr>
<tr class="separator:ab54d666cc65cef4d6ae2fe4c300d7ba1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af3006c9a7183f7e42d1528aa8a6fea24"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#af3006c9a7183f7e42d1528aa8a6fea24">RunAsync</a> (void *inputPtr, void *userArg=nullptr, void *outputPtr=nullptr)</td></tr>
<tr class="memdesc:af3006c9a7183f7e42d1528aa8a6fea24"><td class="mdescLeft">&#160;</td><td class="mdescRight">Non-blocking call to request asynchronous inference by input pointer, and get job ID from inference engine.  <a href="classdxrt_1_1InferenceEngine.html#af3006c9a7183f7e42d1528aa8a6fea24">More...</a><br /></td></tr>
<tr class="separator:af3006c9a7183f7e42d1528aa8a6fea24"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab0c6629f048341d62bfa3a400c90e827"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ab0c6629f048341d62bfa3a400c90e827">RunBenchMark</a> (int num, void *inputPtr=nullptr)</td></tr>
<tr class="memdesc:ab0c6629f048341d62bfa3a400c90e827"><td class="mdescLeft">&#160;</td><td class="mdescRight">run benchmark with loop n times (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#ab0c6629f048341d62bfa3a400c90e827">More...</a><br /></td></tr>
<tr class="separator:ab0c6629f048341d62bfa3a400c90e827"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3846a2f47ea39933136e589aad036eb0"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a3846a2f47ea39933136e589aad036eb0">RunBenchmark</a> (int num, void *inputPtr=nullptr)</td></tr>
<tr class="memdesc:a3846a2f47ea39933136e589aad036eb0"><td class="mdescLeft">&#160;</td><td class="mdescRight">run benchmark with loop n times  <a href="classdxrt_1_1InferenceEngine.html#a3846a2f47ea39933136e589aad036eb0">More...</a><br /></td></tr>
<tr class="separator:a3846a2f47ea39933136e589aad036eb0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aef4c9502a5d5a9d256d3899391376aba"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aef4c9502a5d5a9d256d3899391376aba">ValidateDevice</a> (void *inputPtr, int deviceId=0)</td></tr>
<tr class="memdesc:aef4c9502a5d5a9d256d3899391376aba"><td class="mdescLeft">&#160;</td><td class="mdescRight">Validate inference of a specific NPU device connected to the host. This function runs a validation process using the provided input data on the specified NPU device. It can be used to ensure that the NPU device is operational and can process inference tasks correctly.  <a href="classdxrt_1_1InferenceEngine.html#aef4c9502a5d5a9d256d3899391376aba">More...</a><br /></td></tr>
<tr class="separator:aef4c9502a5d5a9d256d3899391376aba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3d3fc75faa79470d9a3bdbdc61561a3a"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a3d3fc75faa79470d9a3bdbdc61561a3a">RegisterCallBack</a> (std::function&lt; int(TensorPtrs &amp;<a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a>, void *userArg)&gt; callbackFunc)</td></tr>
<tr class="memdesc:a3d3fc75faa79470d9a3bdbdc61561a3a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Register user callback function to be called by inference completion. (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a3d3fc75faa79470d9a3bdbdc61561a3a">More...</a><br /></td></tr>
<tr class="separator:a3d3fc75faa79470d9a3bdbdc61561a3a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1d8b33fa324a48b43433c401bd54280a"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a1d8b33fa324a48b43433c401bd54280a">RegisterCallback</a> (std::function&lt; int(TensorPtrs &amp;<a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a>, void *userArg)&gt; callbackFunc)</td></tr>
<tr class="memdesc:a1d8b33fa324a48b43433c401bd54280a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Register user callback function to be called by inference completion.  <a href="classdxrt_1_1InferenceEngine.html#a1d8b33fa324a48b43433c401bd54280a">More...</a><br /></td></tr>
<tr class="separator:a1d8b33fa324a48b43433c401bd54280a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2c6f130d722950153287d4cf1efd5a5d"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a2c6f130d722950153287d4cf1efd5a5d">Wait</a> (int jobId)</td></tr>
<tr class="memdesc:a2c6f130d722950153287d4cf1efd5a5d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Wait until an request is complete and returns output.  <a href="classdxrt_1_1InferenceEngine.html#a2c6f130d722950153287d4cf1efd5a5d">More...</a><br /></td></tr>
<tr class="separator:a2c6f130d722950153287d4cf1efd5a5d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afbee8b031fdd6c0a3916ed00d08c9cd8"><td class="memItemLeft" align="right" valign="top">Tensors&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#afbee8b031fdd6c0a3916ed00d08c9cd8">inputs</a> (void *ptr=nullptr, uint64_t phyAddr=0)</td></tr>
<tr class="memdesc:afbee8b031fdd6c0a3916ed00d08c9cd8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get input tensor (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#afbee8b031fdd6c0a3916ed00d08c9cd8">More...</a><br /></td></tr>
<tr class="separator:afbee8b031fdd6c0a3916ed00d08c9cd8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abe536ab666125335e08764fa4916c157"><td class="memItemLeft" align="right" valign="top">Tensors&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#abe536ab666125335e08764fa4916c157">GetInputs</a> (void *ptr=nullptr, uint64_t phyAddr=0)</td></tr>
<tr class="memdesc:abe536ab666125335e08764fa4916c157"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get input tensor.  <a href="classdxrt_1_1InferenceEngine.html#abe536ab666125335e08764fa4916c157">More...</a><br /></td></tr>
<tr class="separator:abe536ab666125335e08764fa4916c157"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af81b48e8ccd95953dd9bc483e5ec678b"><td class="memItemLeft" align="right" valign="top">std::vector&lt; Tensors &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#af81b48e8ccd95953dd9bc483e5ec678b">inputs</a> (int devId)</td></tr>
<tr class="memdesc:af81b48e8ccd95953dd9bc483e5ec678b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get input tensor (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#af81b48e8ccd95953dd9bc483e5ec678b">More...</a><br /></td></tr>
<tr class="separator:af81b48e8ccd95953dd9bc483e5ec678b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adb0043cbbf3dfa50eba37d03b9e7d48e"><td class="memItemLeft" align="right" valign="top">std::vector&lt; Tensors &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#adb0043cbbf3dfa50eba37d03b9e7d48e">GetInputs</a> (int devId)</td></tr>
<tr class="memdesc:adb0043cbbf3dfa50eba37d03b9e7d48e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get input tensor.  <a href="classdxrt_1_1InferenceEngine.html#adb0043cbbf3dfa50eba37d03b9e7d48e">More...</a><br /></td></tr>
<tr class="separator:adb0043cbbf3dfa50eba37d03b9e7d48e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0893b724d6eed939556f1315ce05829d"><td class="memItemLeft" align="right" valign="top">Tensors&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> (void *ptr=nullptr, uint64_t phyAddr=0)</td></tr>
<tr class="memdesc:a0893b724d6eed939556f1315ce05829d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get output tensor (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">More...</a><br /></td></tr>
<tr class="separator:a0893b724d6eed939556f1315ce05829d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a411b78e6c9560119f9a39c30ee1021af"><td class="memItemLeft" align="right" valign="top">Tensors&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a411b78e6c9560119f9a39c30ee1021af">GetOutputs</a> (void *ptr=nullptr, uint64_t phyAddr=0)</td></tr>
<tr class="memdesc:a411b78e6c9560119f9a39c30ee1021af"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get output tensor.  <a href="classdxrt_1_1InferenceEngine.html#a411b78e6c9560119f9a39c30ee1021af">More...</a><br /></td></tr>
<tr class="separator:a411b78e6c9560119f9a39c30ee1021af"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a96c4e5953a0daf4a1c886002c052a6ba"><td class="memItemLeft" align="right" valign="top">uint64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a96c4e5953a0daf4a1c886002c052a6ba">input_size</a> ()</td></tr>
<tr class="memdesc:a96c4e5953a0daf4a1c886002c052a6ba"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get total size of input tensors (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a96c4e5953a0daf4a1c886002c052a6ba">More...</a><br /></td></tr>
<tr class="separator:a96c4e5953a0daf4a1c886002c052a6ba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad2c11689c4839b5dbb3571107a7b3a7c"><td class="memItemLeft" align="right" valign="top">uint64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ad2c11689c4839b5dbb3571107a7b3a7c">GetInputSize</a> ()</td></tr>
<tr class="memdesc:ad2c11689c4839b5dbb3571107a7b3a7c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get total size of input tensors.  <a href="classdxrt_1_1InferenceEngine.html#ad2c11689c4839b5dbb3571107a7b3a7c">More...</a><br /></td></tr>
<tr class="separator:ad2c11689c4839b5dbb3571107a7b3a7c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a93e1d7b460b7f8656e725ed708de507b"><td class="memItemLeft" align="right" valign="top">uint64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a93e1d7b460b7f8656e725ed708de507b">output_size</a> ()</td></tr>
<tr class="memdesc:a93e1d7b460b7f8656e725ed708de507b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get total size of output tensors (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a93e1d7b460b7f8656e725ed708de507b">More...</a><br /></td></tr>
<tr class="separator:a93e1d7b460b7f8656e725ed708de507b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a82af5685f6b43643c3ddf47bf0a4ade8"><td class="memItemLeft" align="right" valign="top">uint64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a82af5685f6b43643c3ddf47bf0a4ade8">GetOutputSize</a> ()</td></tr>
<tr class="memdesc:a82af5685f6b43643c3ddf47bf0a4ade8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get total size of output tensors.  <a href="classdxrt_1_1InferenceEngine.html#a82af5685f6b43643c3ddf47bf0a4ade8">More...</a><br /></td></tr>
<tr class="separator:a82af5685f6b43643c3ddf47bf0a4ade8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3fb16094508bd367cc4971238f002765"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a3fb16094508bd367cc4971238f002765">name</a> ()</td></tr>
<tr class="memdesc:a3fb16094508bd367cc4971238f002765"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get model name (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a3fb16094508bd367cc4971238f002765">More...</a><br /></td></tr>
<tr class="separator:a3fb16094508bd367cc4971238f002765"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7ed90b0eaa55ce7c097cc7163009c66e"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a7ed90b0eaa55ce7c097cc7163009c66e">GetModelName</a> ()</td></tr>
<tr class="memdesc:a7ed90b0eaa55ce7c097cc7163009c66e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get model name.  <a href="classdxrt_1_1InferenceEngine.html#a7ed90b0eaa55ce7c097cc7163009c66e">More...</a><br /></td></tr>
<tr class="separator:a7ed90b0eaa55ce7c097cc7163009c66e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a22087ed8a6839bfc1c4a6b9c21629236"><td class="memItemLeft" align="right" valign="top">std::vector&lt; string &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a22087ed8a6839bfc1c4a6b9c21629236">task_order</a> ()</td></tr>
<tr class="memdesc:a22087ed8a6839bfc1c4a6b9c21629236"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get model task order (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a22087ed8a6839bfc1c4a6b9c21629236">More...</a><br /></td></tr>
<tr class="separator:a22087ed8a6839bfc1c4a6b9c21629236"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0fabc1c0f736be0d297b2ce14929847b"><td class="memItemLeft" align="right" valign="top">std::vector&lt; string &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a0fabc1c0f736be0d297b2ce14929847b">GetTaskOrder</a> ()</td></tr>
<tr class="memdesc:a0fabc1c0f736be0d297b2ce14929847b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get model task order.  <a href="classdxrt_1_1InferenceEngine.html#a0fabc1c0f736be0d297b2ce14929847b">More...</a><br /></td></tr>
<tr class="separator:a0fabc1c0f736be0d297b2ce14929847b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a990ca720e7f8a28ac5394206eb6f9491"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a990ca720e7f8a28ac5394206eb6f9491">latency</a> ()</td></tr>
<tr class="memdesc:a990ca720e7f8a28ac5394206eb6f9491"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latest latency (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a990ca720e7f8a28ac5394206eb6f9491">More...</a><br /></td></tr>
<tr class="separator:a990ca720e7f8a28ac5394206eb6f9491"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3758aeb28d7ed75216770a5fd5dd9619"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a3758aeb28d7ed75216770a5fd5dd9619">GetLatency</a> ()</td></tr>
<tr class="memdesc:a3758aeb28d7ed75216770a5fd5dd9619"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latest latency.  <a href="classdxrt_1_1InferenceEngine.html#a3758aeb28d7ed75216770a5fd5dd9619">More...</a><br /></td></tr>
<tr class="separator:a3758aeb28d7ed75216770a5fd5dd9619"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad2f3b616e0e996b66ef7df37dc16a85"><td class="memItemLeft" align="right" valign="top">uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aad2f3b616e0e996b66ef7df37dc16a85">inference_time</a> ()</td></tr>
<tr class="memdesc:aad2f3b616e0e996b66ef7df37dc16a85"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latest inference time (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#aad2f3b616e0e996b66ef7df37dc16a85">More...</a><br /></td></tr>
<tr class="separator:aad2f3b616e0e996b66ef7df37dc16a85"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1ee64321b5d6c50cb95be282080f326d"><td class="memItemLeft" align="right" valign="top">uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a1ee64321b5d6c50cb95be282080f326d">GetNpuInferenceTime</a> ()</td></tr>
<tr class="memdesc:a1ee64321b5d6c50cb95be282080f326d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latest inference time.  <a href="classdxrt_1_1InferenceEngine.html#a1ee64321b5d6c50cb95be282080f326d">More...</a><br /></td></tr>
<tr class="separator:a1ee64321b5d6c50cb95be282080f326d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a331862cfde13a009b3e4133678a1c597"><td class="memItemLeft" align="right" valign="top">std::vector&lt; int &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a331862cfde13a009b3e4133678a1c597">GetLatencyVector</a> ()</td></tr>
<tr class="memdesc:a331862cfde13a009b3e4133678a1c597"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get recent Latency.  <a href="classdxrt_1_1InferenceEngine.html#a331862cfde13a009b3e4133678a1c597">More...</a><br /></td></tr>
<tr class="separator:a331862cfde13a009b3e4133678a1c597"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad79d35e5a285878517eb784439737a9"><td class="memItemLeft" align="right" valign="top">std::vector&lt; uint32_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aad79d35e5a285878517eb784439737a9">GetNpuInferenceTimeVector</a> ()</td></tr>
<tr class="memdesc:aad79d35e5a285878517eb784439737a9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get recent inference time.  <a href="classdxrt_1_1InferenceEngine.html#aad79d35e5a285878517eb784439737a9">More...</a><br /></td></tr>
<tr class="separator:aad79d35e5a285878517eb784439737a9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1e81cce5b5db047c17ba40b69bab9a60"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a1e81cce5b5db047c17ba40b69bab9a60">GetLatencyMean</a> ()</td></tr>
<tr class="memdesc:a1e81cce5b5db047c17ba40b69bab9a60"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latency Mean.  <a href="classdxrt_1_1InferenceEngine.html#a1e81cce5b5db047c17ba40b69bab9a60">More...</a><br /></td></tr>
<tr class="separator:a1e81cce5b5db047c17ba40b69bab9a60"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0d46e7a44cd5975c5a9c3a76be684dc3"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a0d46e7a44cd5975c5a9c3a76be684dc3">GetNpuInferenceTimeMean</a> ()</td></tr>
<tr class="memdesc:a0d46e7a44cd5975c5a9c3a76be684dc3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get inference time Mean.  <a href="classdxrt_1_1InferenceEngine.html#a0d46e7a44cd5975c5a9c3a76be684dc3">More...</a><br /></td></tr>
<tr class="separator:a0d46e7a44cd5975c5a9c3a76be684dc3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae8bec4ff8e4e0fb6fa680c04f84b642d"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ae8bec4ff8e4e0fb6fa680c04f84b642d">GetLatencyStdDev</a> ()</td></tr>
<tr class="memdesc:ae8bec4ff8e4e0fb6fa680c04f84b642d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latency Standard Deviation.  <a href="classdxrt_1_1InferenceEngine.html#ae8bec4ff8e4e0fb6fa680c04f84b642d">More...</a><br /></td></tr>
<tr class="separator:ae8bec4ff8e4e0fb6fa680c04f84b642d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4d552cb23986c189b1002a3e0b9915a9"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a4d552cb23986c189b1002a3e0b9915a9">GetNpuInferenceTimeStdDev</a> ()</td></tr>
<tr class="memdesc:a4d552cb23986c189b1002a3e0b9915a9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get inference time Standard Deviation.  <a href="classdxrt_1_1InferenceEngine.html#a4d552cb23986c189b1002a3e0b9915a9">More...</a><br /></td></tr>
<tr class="separator:a4d552cb23986c189b1002a3e0b9915a9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1259543ba3f8fc52efb503e40570f95e"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a1259543ba3f8fc52efb503e40570f95e">GetLatencyCnt</a> ()</td></tr>
<tr class="memdesc:a1259543ba3f8fc52efb503e40570f95e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latency Count.  <a href="classdxrt_1_1InferenceEngine.html#a1259543ba3f8fc52efb503e40570f95e">More...</a><br /></td></tr>
<tr class="separator:a1259543ba3f8fc52efb503e40570f95e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad7a32a700fa89a0213b815d57f8fc1c2"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ad7a32a700fa89a0213b815d57f8fc1c2">GetNpuInferenceTimeCnt</a> ()</td></tr>
<tr class="memdesc:ad7a32a700fa89a0213b815d57f8fc1c2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get inference time Count.  <a href="classdxrt_1_1InferenceEngine.html#ad7a32a700fa89a0213b815d57f8fc1c2">More...</a><br /></td></tr>
<tr class="separator:ad7a32a700fa89a0213b815d57f8fc1c2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a077977d15551f525f546e2a3f6ab010c"><td class="memItemLeft" align="right" valign="top">vector&lt; TensorPtrs &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a077977d15551f525f546e2a3f6ab010c">get_outputs</a> ()</td></tr>
<tr class="memdesc:a077977d15551f525f546e2a3f6ab010c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get output tensors of all tasks (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a077977d15551f525f546e2a3f6ab010c">More...</a><br /></td></tr>
<tr class="separator:a077977d15551f525f546e2a3f6ab010c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a12ebd04c3d8780e2d6c08d0fc2913534"><td class="memItemLeft" align="right" valign="top">vector&lt; TensorPtrs &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a12ebd04c3d8780e2d6c08d0fc2913534">GetAllTaskOutputs</a> ()</td></tr>
<tr class="memdesc:a12ebd04c3d8780e2d6c08d0fc2913534"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get output tensors of all tasks.  <a href="classdxrt_1_1InferenceEngine.html#a12ebd04c3d8780e2d6c08d0fc2913534">More...</a><br /></td></tr>
<tr class="separator:a12ebd04c3d8780e2d6c08d0fc2913534"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0e23ce411e6d6d45e6939fe1b3a7e700"><td class="memItemLeft" align="right" valign="top">vector&lt; uint8_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a0e23ce411e6d6d45e6939fe1b3a7e700">bitmatch_mask</a> (int index)</td></tr>
<tr class="separator:a0e23ce411e6d6d45e6939fe1b3a7e700"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0b38cd5001ec327cbdc5b00a7143d0e2"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a0b38cd5001ec327cbdc5b00a7143d0e2">get_num_tails</a> ()</td></tr>
<tr class="memdesc:a0b38cd5001ec327cbdc5b00a7143d0e2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the number of tail tasks in the model. (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a0b38cd5001ec327cbdc5b00a7143d0e2">More...</a><br /></td></tr>
<tr class="separator:a0b38cd5001ec327cbdc5b00a7143d0e2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a110445fe04396f2d2ea309c18c3bb930"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a110445fe04396f2d2ea309c18c3bb930">GetNumTailTasks</a> ()</td></tr>
<tr class="memdesc:a110445fe04396f2d2ea309c18c3bb930"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the number of tail tasks in the model.  <a href="classdxrt_1_1InferenceEngine.html#a110445fe04396f2d2ea309c18c3bb930">More...</a><br /></td></tr>
<tr class="separator:a110445fe04396f2d2ea309c18c3bb930"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adb7b0c6f4a8411823d7eaed57cedd36d"><td class="memItemLeft" align="right" valign="top">string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#adb7b0c6f4a8411823d7eaed57cedd36d">get_compile_type</a> ()</td></tr>
<tr class="memdesc:adb7b0c6f4a8411823d7eaed57cedd36d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the compile type of the model. (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#adb7b0c6f4a8411823d7eaed57cedd36d">More...</a><br /></td></tr>
<tr class="separator:adb7b0c6f4a8411823d7eaed57cedd36d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae6f83c617959dc76fc993b9da3c51769"><td class="memItemLeft" align="right" valign="top">string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ae6f83c617959dc76fc993b9da3c51769">GetCompileType</a> ()</td></tr>
<tr class="memdesc:ae6f83c617959dc76fc993b9da3c51769"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the compile type of the model.  <a href="classdxrt_1_1InferenceEngine.html#ae6f83c617959dc76fc993b9da3c51769">More...</a><br /></td></tr>
<tr class="separator:ae6f83c617959dc76fc993b9da3c51769"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9aeb596b415843e7e318c47938444259"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a9aeb596b415843e7e318c47938444259">is_PPU</a> ()</td></tr>
<tr class="memdesc:a9aeb596b415843e7e318c47938444259"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns whether the model is using PPU. (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a9aeb596b415843e7e318c47938444259">More...</a><br /></td></tr>
<tr class="separator:a9aeb596b415843e7e318c47938444259"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab6fd915af259467afe1681098b315dca"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ab6fd915af259467afe1681098b315dca">IsPPU</a> ()</td></tr>
<tr class="memdesc:ab6fd915af259467afe1681098b315dca"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns whether the model is using PPU.  <a href="classdxrt_1_1InferenceEngine.html#ab6fd915af259467afe1681098b315dca">More...</a><br /></td></tr>
<tr class="separator:ab6fd915af259467afe1681098b315dca"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af3feefee6373ecfe7fe5b9b84501a723"><td class="memItemLeft" align="right" valign="top"><a id="af3feefee6373ecfe7fe5b9b84501a723"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#af3feefee6373ecfe7fe5b9b84501a723">Dispose</a> ()</td></tr>
<tr class="memdesc:af3feefee6373ecfe7fe5b9b84501a723"><td class="mdescLeft">&#160;</td><td class="mdescRight">Resource deallocation and cleanup. <br /></td></tr>
<tr class="separator:af3feefee6373ecfe7fe5b9b84501a723"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>This class abstracts runtime inference executor for user's compiled model. </p>
<p>After user loads compiled model to <a class="el" href="classdxrt_1_1InferenceEngine.html" title="This class abstracts runtime inference executor for user&#39;s compiled model.">InferenceEngine</a>, real-time device tasks will be scheduled by internal runtime libraries. It supports both inference mode (synchronous/asynchronous) according to user's request. </p><div class="fragment"><div class="line"><span class="comment">// Use default inference option</span></div>
<div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><span class="keyword">auto</span> ie = <a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a>(modelPath, <span class="keyword">nullptr</span>);</div>
</div><!-- fragment --> <div class="fragment"><div class="line"><span class="comment">// Use a new inference option</span></div>
<div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a> option;</div>
<div class="line">option.<a class="code" href="classdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">devices</a> = {0,1,3};  <span class="comment">//use only 0,1,3 device</span></div>
<div class="line"><span class="keyword">auto</span> ie = <a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a>(modelPath, option);</div>
</div><!-- fragment --> </div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="aed622c0fa566620899c9e7e26011bacd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aed622c0fa566620899c9e7e26011bacd">&#9670;&nbsp;</a></span>InferenceEngine()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">dxrt::InferenceEngine::InferenceEngine </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>modelPath</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classdxrt_1_1InferenceOption.html">InferenceOption</a> &amp;&#160;</td>
          <td class="paramname"><em>option</em> = <code><a class="el" href="namespacedxrt.html#aab5060680ba2f567f47f8868cd8b1f00">DefaultInferenceOption</a></code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Perform the task of loading the model and configuring the NPU to run. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">modelPath</td><td>model path </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">option</td><td>device and npu core options <div class="fragment"><div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a>(modelPath) ie(modelPath);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
<div class="line"> </div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a> op;</div>
<div class="line">op.<a class="code" href="classdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">devices</a>.push_back(0); </div>
<div class="line">op.<a class="code" href="classdxrt_1_1InferenceOption.html#a9fd270a5eeeb1a14ce5f133c16c30d55">boundOption</a> = dxrt::InferenceOption::BOUND_OPTION::NPU_0; <span class="comment">// NPU_0 only</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a> ie(modelPath, op);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
</div><!-- fragment --> </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a0e23ce411e6d6d45e6939fe1b3a7e700"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0e23ce411e6d6d45e6939fe1b3a7e700">&#9670;&nbsp;</a></span>bitmatch_mask()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">vector&lt;uint8_t&gt; dxrt::InferenceEngine::bitmatch_mask </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>index</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000013">Deprecated:</a></b></dt><dd>Use GetBitmatchMask() instead. </dd></dl>

</div>
</div>
<a id="adb7b0c6f4a8411823d7eaed57cedd36d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adb7b0c6f4a8411823d7eaed57cedd36d">&#9670;&nbsp;</a></span>get_compile_type()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">string dxrt::InferenceEngine::get_compile_type </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the compile type of the model. (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000015">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#ae6f83c617959dc76fc993b9da3c51769" title="Returns the compile type of the model.">GetCompileType()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>The compile type of the model. </dd></dl>

</div>
</div>
<a id="a0b38cd5001ec327cbdc5b00a7143d0e2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0b38cd5001ec327cbdc5b00a7143d0e2">&#9670;&nbsp;</a></span>get_num_tails()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::get_num_tails </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the number of tail tasks in the model. (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000014">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a110445fe04396f2d2ea309c18c3bb930" title="Returns the number of tail tasks in the model.">GetNumTailTasks()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>The number of tasks that have no subsequent tasks.</dd></dl>
<p>Tail tasks are those which do not have any tasks following them in the model's task chain. This function provides the count of such tail tasks. </p>

</div>
</div>
<a id="a077977d15551f525f546e2a3f6ab010c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a077977d15551f525f546e2a3f6ab010c">&#9670;&nbsp;</a></span>get_outputs()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">vector&lt;TensorPtrs&gt; dxrt::InferenceEngine::get_outputs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get output tensors of all tasks (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000012">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a12ebd04c3d8780e2d6c08d0fc2913534" title="Get output tensors of all tasks.">GetAllTaskOutputs()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>the output of all tasks as a vector of smart pointer instance vectors. <br  />
 </dd></dl>

</div>
</div>
<a id="a12ebd04c3d8780e2d6c08d0fc2913534"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a12ebd04c3d8780e2d6c08d0fc2913534">&#9670;&nbsp;</a></span>GetAllTaskOutputs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">vector&lt;TensorPtrs&gt; dxrt::InferenceEngine::GetAllTaskOutputs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get output tensors of all tasks. </p>
<dl class="section return"><dt>Returns</dt><dd>the output of all Tasks as a vector of smart pointer instance vectors. <br  />
 </dd></dl>

</div>
</div>
<a id="ae6f83c617959dc76fc993b9da3c51769"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae6f83c617959dc76fc993b9da3c51769">&#9670;&nbsp;</a></span>GetCompileType()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">string dxrt::InferenceEngine::GetCompileType </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the compile type of the model. </p>
<dl class="section return"><dt>Returns</dt><dd>The compile type of the model. </dd></dl>

</div>
</div>
<a id="adb0043cbbf3dfa50eba37d03b9e7d48e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adb0043cbbf3dfa50eba37d03b9e7d48e">&#9670;&nbsp;</a></span>GetInputs() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;Tensors&gt; dxrt::InferenceEngine::GetInputs </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>devId</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">devId</td><td>device id </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>vector of input tensors </dd></dl>

</div>
</div>
<a id="abe536ab666125335e08764fa4916c157"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abe536ab666125335e08764fa4916c157">&#9670;&nbsp;</a></span>GetInputs() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensors dxrt::InferenceEngine::GetInputs </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>ptr</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint64_t&#160;</td>
          <td class="paramname"><em>phyAddr</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">ptr</td><td>pointer to virtual address </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">phyAddr</td><td>pointer to physical address </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>if ptr is null, input memory area in engine is returned </dd>
<dd>
if ptr and phyAddr is given, inputs tensors that contains output addresses </dd></dl>

</div>
</div>
<a id="ad2c11689c4839b5dbb3571107a7b3a7c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad2c11689c4839b5dbb3571107a7b3a7c">&#9670;&nbsp;</a></span>GetInputSize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">uint64_t dxrt::InferenceEngine::GetInputSize </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get total size of input tensors. </p>
<dl class="section return"><dt>Returns</dt><dd>Input size of one inference in bytes </dd></dl>

</div>
</div>
<a id="a3758aeb28d7ed75216770a5fd5dd9619"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3758aeb28d7ed75216770a5fd5dd9619">&#9670;&nbsp;</a></span>GetLatency()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::GetLatency </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get latest latency. </p>
<dl class="section return"><dt>Returns</dt><dd>latency (microseconds) </dd></dl>

</div>
</div>
<a id="a1259543ba3f8fc52efb503e40570f95e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1259543ba3f8fc52efb503e40570f95e">&#9670;&nbsp;</a></span>GetLatencyCnt()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::GetLatencyCnt </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get latency Count. </p>
<dl class="section return"><dt>Returns</dt><dd>latency Count </dd></dl>

</div>
</div>
<a id="a1e81cce5b5db047c17ba40b69bab9a60"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1e81cce5b5db047c17ba40b69bab9a60">&#9670;&nbsp;</a></span>GetLatencyMean()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double dxrt::InferenceEngine::GetLatencyMean </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get latency Mean. </p>
<dl class="section return"><dt>Returns</dt><dd>latency Mean (microseconds) </dd></dl>

</div>
</div>
<a id="ae8bec4ff8e4e0fb6fa680c04f84b642d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae8bec4ff8e4e0fb6fa680c04f84b642d">&#9670;&nbsp;</a></span>GetLatencyStdDev()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double dxrt::InferenceEngine::GetLatencyStdDev </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get latency Standard Deviation. </p>
<dl class="section return"><dt>Returns</dt><dd>latency Standard Deviation </dd></dl>

</div>
</div>
<a id="a331862cfde13a009b3e4133678a1c597"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a331862cfde13a009b3e4133678a1c597">&#9670;&nbsp;</a></span>GetLatencyVector()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;int&gt; dxrt::InferenceEngine::GetLatencyVector </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get recent Latency. </p>
<dl class="section return"><dt>Returns</dt><dd>latency (microseconds) </dd></dl>

</div>
</div>
<a id="a7ed90b0eaa55ce7c097cc7163009c66e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7ed90b0eaa55ce7c097cc7163009c66e">&#9670;&nbsp;</a></span>GetModelName()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::string dxrt::InferenceEngine::GetModelName </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get model name. </p>
<dl class="section return"><dt>Returns</dt><dd>model name </dd></dl>

</div>
</div>
<a id="a1ee64321b5d6c50cb95be282080f326d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1ee64321b5d6c50cb95be282080f326d">&#9670;&nbsp;</a></span>GetNpuInferenceTime()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">uint32_t dxrt::InferenceEngine::GetNpuInferenceTime </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get latest inference time. </p>
<dl class="section return"><dt>Returns</dt><dd>inference time (microseconds) </dd></dl>

</div>
</div>
<a id="ad7a32a700fa89a0213b815d57f8fc1c2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad7a32a700fa89a0213b815d57f8fc1c2">&#9670;&nbsp;</a></span>GetNpuInferenceTimeCnt()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::GetNpuInferenceTimeCnt </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get inference time Count. </p>
<dl class="section return"><dt>Returns</dt><dd>inference time Count </dd></dl>

</div>
</div>
<a id="a0d46e7a44cd5975c5a9c3a76be684dc3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0d46e7a44cd5975c5a9c3a76be684dc3">&#9670;&nbsp;</a></span>GetNpuInferenceTimeMean()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double dxrt::InferenceEngine::GetNpuInferenceTimeMean </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get inference time Mean. </p>
<dl class="section return"><dt>Returns</dt><dd>inference time Mean (microseconds) </dd></dl>

</div>
</div>
<a id="a4d552cb23986c189b1002a3e0b9915a9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4d552cb23986c189b1002a3e0b9915a9">&#9670;&nbsp;</a></span>GetNpuInferenceTimeStdDev()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double dxrt::InferenceEngine::GetNpuInferenceTimeStdDev </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get inference time Standard Deviation. </p>
<dl class="section return"><dt>Returns</dt><dd>inference time Standard Deviation </dd></dl>

</div>
</div>
<a id="aad79d35e5a285878517eb784439737a9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aad79d35e5a285878517eb784439737a9">&#9670;&nbsp;</a></span>GetNpuInferenceTimeVector()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;uint32_t&gt; dxrt::InferenceEngine::GetNpuInferenceTimeVector </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get recent inference time. </p>
<dl class="section return"><dt>Returns</dt><dd>inference time (microseconds) </dd></dl>

</div>
</div>
<a id="a110445fe04396f2d2ea309c18c3bb930"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a110445fe04396f2d2ea309c18c3bb930">&#9670;&nbsp;</a></span>GetNumTailTasks()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::GetNumTailTasks </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the number of tail tasks in the model. </p>
<dl class="section return"><dt>Returns</dt><dd>The number of tasks that have no subsequent tasks.</dd></dl>
<p>Tail tasks are those which do not have any tasks following them in the model's task chain. This function provides the count of such tail tasks. </p>

</div>
</div>
<a id="a411b78e6c9560119f9a39c30ee1021af"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a411b78e6c9560119f9a39c30ee1021af">&#9670;&nbsp;</a></span>GetOutputs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensors dxrt::InferenceEngine::GetOutputs </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>ptr</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint64_t&#160;</td>
          <td class="paramname"><em>phyAddr</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get output tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">ptr</td><td>pointer to virtual address </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">phyAddr</td><td>pointer to physical address </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>if ptr is null, output memory area in engine is returned </dd>
<dd>
if ptr and phyAddr is given, outputs tensors that contains output addresses </dd></dl>

</div>
</div>
<a id="a82af5685f6b43643c3ddf47bf0a4ade8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a82af5685f6b43643c3ddf47bf0a4ade8">&#9670;&nbsp;</a></span>GetOutputSize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">uint64_t dxrt::InferenceEngine::GetOutputSize </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get total size of output tensors. </p>
<dl class="section return"><dt>Returns</dt><dd>Output size of one inference in bytes </dd></dl>

</div>
</div>
<a id="a0fabc1c0f736be0d297b2ce14929847b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0fabc1c0f736be0d297b2ce14929847b">&#9670;&nbsp;</a></span>GetTaskOrder()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;string&gt; dxrt::InferenceEngine::GetTaskOrder </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get model task order. </p>
<dl class="section return"><dt>Returns</dt><dd>task order </dd></dl>

</div>
</div>
<a id="aad2f3b616e0e996b66ef7df37dc16a85"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aad2f3b616e0e996b66ef7df37dc16a85">&#9670;&nbsp;</a></span>inference_time()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">uint32_t dxrt::InferenceEngine::inference_time </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get latest inference time (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000011">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a1ee64321b5d6c50cb95be282080f326d" title="Get latest inference time.">GetNpuInferenceTime()</a> instead. <br  />
 </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>inference time (microseconds) </dd></dl>

</div>
</div>
<a id="a96c4e5953a0daf4a1c886002c052a6ba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a96c4e5953a0daf4a1c886002c052a6ba">&#9670;&nbsp;</a></span>input_size()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">uint64_t dxrt::InferenceEngine::input_size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get total size of input tensors (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000006">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#ad2c11689c4839b5dbb3571107a7b3a7c" title="Get total size of input tensors.">GetInputSize()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>Input size of one inference in bytes </dd></dl>

</div>
</div>
<a id="af81b48e8ccd95953dd9bc483e5ec678b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af81b48e8ccd95953dd9bc483e5ec678b">&#9670;&nbsp;</a></span>inputs() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;Tensors&gt; dxrt::InferenceEngine::inputs </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>devId</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get input tensor (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000004">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#abe536ab666125335e08764fa4916c157" title="Get input tensor.">GetInputs()</a> instead. </dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">devId</td><td>device id </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>vector of input tensors </dd></dl>

</div>
</div>
<a id="afbee8b031fdd6c0a3916ed00d08c9cd8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afbee8b031fdd6c0a3916ed00d08c9cd8">&#9670;&nbsp;</a></span>inputs() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">Tensors dxrt::InferenceEngine::inputs </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>ptr</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint64_t&#160;</td>
          <td class="paramname"><em>phyAddr</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get input tensor (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000003">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#abe536ab666125335e08764fa4916c157" title="Get input tensor.">GetInputs()</a> instead. </dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">ptr</td><td>pointer to virtual address </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">phyAddr</td><td>pointer to physical address </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>if ptr is null, input memory area in engine is returned </dd>
<dd>
if ptr and phyAddr is given, inputs tensors that contains output addresses </dd></dl>

</div>
</div>
<a id="a9aeb596b415843e7e318c47938444259"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9aeb596b415843e7e318c47938444259">&#9670;&nbsp;</a></span>is_PPU()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool dxrt::InferenceEngine::is_PPU </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns whether the model is using PPU. (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000016">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#ab6fd915af259467afe1681098b315dca" title="Returns whether the model is using PPU.">IsPPU()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>whether the model is using PPU. </dd></dl>

</div>
</div>
<a id="ab6fd915af259467afe1681098b315dca"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab6fd915af259467afe1681098b315dca">&#9670;&nbsp;</a></span>IsPPU()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool dxrt::InferenceEngine::IsPPU </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns whether the model is using PPU. </p>
<dl class="section return"><dt>Returns</dt><dd>whether the model is using PPU. </dd></dl>

</div>
</div>
<a id="a990ca720e7f8a28ac5394206eb6f9491"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a990ca720e7f8a28ac5394206eb6f9491">&#9670;&nbsp;</a></span>latency()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::latency </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get latest latency (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000010">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a3758aeb28d7ed75216770a5fd5dd9619" title="Get latest latency.">GetLatency()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>latency (microseconds) </dd></dl>

</div>
</div>
<a id="a3fb16094508bd367cc4971238f002765"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3fb16094508bd367cc4971238f002765">&#9670;&nbsp;</a></span>name()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::string dxrt::InferenceEngine::name </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get model name (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000008">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a7ed90b0eaa55ce7c097cc7163009c66e" title="Get model name.">GetModelName()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>model name </dd></dl>

</div>
</div>
<a id="a93e1d7b460b7f8656e725ed708de507b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a93e1d7b460b7f8656e725ed708de507b">&#9670;&nbsp;</a></span>output_size()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">uint64_t dxrt::InferenceEngine::output_size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get total size of output tensors (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000007">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a82af5685f6b43643c3ddf47bf0a4ade8" title="Get total size of output tensors.">GetOutputSize()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>Output size of one inference in bytes </dd></dl>

</div>
</div>
<a id="a0893b724d6eed939556f1315ce05829d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0893b724d6eed939556f1315ce05829d">&#9670;&nbsp;</a></span>outputs()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">Tensors dxrt::InferenceEngine::outputs </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>ptr</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint64_t&#160;</td>
          <td class="paramname"><em>phyAddr</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get output tensor (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000005">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a411b78e6c9560119f9a39c30ee1021af" title="Get output tensor.">GetOutputs()</a> instead. </dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">ptr</td><td>pointer to virtual address </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">phyAddr</td><td>pointer to physical address </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>if ptr is null, output memory area in engine is returned </dd>
<dd>
if ptr and phyAddr is given, outputs tensors that contains output addresses </dd></dl>

</div>
</div>
<a id="a3d3fc75faa79470d9a3bdbdc61561a3a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3d3fc75faa79470d9a3bdbdc61561a3a">&#9670;&nbsp;</a></span>RegisterCallBack()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void dxrt::InferenceEngine::RegisterCallBack </td>
          <td>(</td>
          <td class="paramtype">std::function&lt; int(TensorPtrs &amp;<a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a>, void *userArg)&gt;&#160;</td>
          <td class="paramname"><em>callbackFunc</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Register user callback function to be called by inference completion. (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000002">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a1d8b33fa324a48b43433c401bd54280a" title="Register user callback function to be called by inference completion.">RegisterCallback()</a> instead. </dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">callbackFunc</td><td>Function which is called when inference is complete, it gets outputs and user_arg ptr </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">outputs</td><td>output tensors data </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">userArg</td><td>userArg given by <a class="el" href="classdxrt_1_1InferenceEngine.html#a88cea5324cbeb436034adb60cf31050f" title="Run inference engine using specific input pointer Synchronously.">Run()</a>; </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a1d8b33fa324a48b43433c401bd54280a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1d8b33fa324a48b43433c401bd54280a">&#9670;&nbsp;</a></span>RegisterCallback()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void dxrt::InferenceEngine::RegisterCallback </td>
          <td>(</td>
          <td class="paramtype">std::function&lt; int(TensorPtrs &amp;<a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a>, void *userArg)&gt;&#160;</td>
          <td class="paramname"><em>callbackFunc</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Register user callback function to be called by inference completion. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">callbackFunc</td><td>Function which is called when inference is complete, it gets outputs and user_arg ptr </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">outputs</td><td>output tensors data </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">userArg</td><td>userArg given by <a class="el" href="classdxrt_1_1InferenceEngine.html#a88cea5324cbeb436034adb60cf31050f" title="Run inference engine using specific input pointer Synchronously.">Run()</a>; </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ab54d666cc65cef4d6ae2fe4c300d7ba1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab54d666cc65cef4d6ae2fe4c300d7ba1">&#9670;&nbsp;</a></span>Run() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;TensorPtrs&gt; dxrt::InferenceEngine::Run </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>inputBuffers</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>outputBuffers</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>userArgs</em> = <code>{}</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Runs the inference engine using a specific input pointer vector. </p>
<p>This function executes inference based on the provided input data pointer vector and returns a vector of output tensors. Users can optionally provide additional user-defined arguments and output pointer vectors. If userArgs is used, the number of elements in inputPtrs must be the same as the number of elements in userArgs. An InvalidArgumentException is thrown if the size of inputPtrs and userArgs are different. An InvalidArgumentException is thrown if the size of inputPtrs and OutputPtrs are different. An InvalidArgumentException is thrown if the size of inputPtrs is 0.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputBuffers</td><td>Vector of input data pointers used for inference. </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputBuffers</td><td>Vector of output data pointers. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArgs</td><td>Vector of user-defined arguments (e.g., original frame data, input metadata, etc.). (Optional)</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Vector of output tensors as smart pointer instances. </dd></dl>

</div>
</div>
<a id="a88cea5324cbeb436034adb60cf31050f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a88cea5324cbeb436034adb60cf31050f">&#9670;&nbsp;</a></span>Run() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::Run </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userArg</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>outputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Run inference engine using specific input pointer Synchronously. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>input data pointer to run inference </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArg</td><td>user-defined arguments as a void pointer(e.g. original frame data, metadata about input, ... ) </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputPtr</td><td>pointer to output data, if it is nullptr, output data is stored in buffer inside DXRT. <div class="fragment"><div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a> ie(modelPath);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
</div><!-- fragment --> </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>output tensors as vector of smart pointer instances </dd></dl>

</div>
</div>
<a id="af3006c9a7183f7e42d1528aa8a6fea24"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af3006c9a7183f7e42d1528aa8a6fea24">&#9670;&nbsp;</a></span>RunAsync()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::RunAsync </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userArg</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>outputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Non-blocking call to request asynchronous inference by input pointer, and get job ID from inference engine. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>input data pointer to run inference </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArg</td><td>user-defined arguments as a void pointer(e.g. original frame data, metadata about input, ... ) </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputPtr</td><td>pointer to output data, if it is nullptr, output data area is allocated by DXRT. <div class="fragment"><div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model&quot;</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a> option;</div>
<div class="line">option.<a class="code" href="classdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">devices</a> = {0,1,3};  <span class="comment">//use only 0,1,3 device</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a> ie(modelPath, option);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
</div><!-- fragment --> </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>job id that can be used to wait() function </dd></dl>

</div>
</div>
<a id="ab0c6629f048341d62bfa3a400c90e827"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab0c6629f048341d62bfa3a400c90e827">&#9670;&nbsp;</a></span>RunBenchMark()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">float dxrt::InferenceEngine::RunBenchMark </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>run benchmark with loop n times (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000001">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a3846a2f47ea39933136e589aad036eb0" title="run benchmark with loop n times">RunBenchmark()</a> instead. </dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">num</td><td>number of inferences </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>input data pointer to run inference </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>average fps </dd></dl>

</div>
</div>
<a id="a3846a2f47ea39933136e589aad036eb0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3846a2f47ea39933136e589aad036eb0">&#9670;&nbsp;</a></span>RunBenchmark()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">float dxrt::InferenceEngine::RunBenchmark </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>run benchmark with loop n times </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">num</td><td>number of inferences </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>input data pointer to run inference </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>average fps </dd></dl>

</div>
</div>
<a id="a22087ed8a6839bfc1c4a6b9c21629236"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a22087ed8a6839bfc1c4a6b9c21629236">&#9670;&nbsp;</a></span>task_order()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;string&gt; dxrt::InferenceEngine::task_order </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get model task order (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000009">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a0fabc1c0f736be0d297b2ce14929847b" title="Get model task order.">GetTaskOrder()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>task order </dd></dl>

</div>
</div>
<a id="aef4c9502a5d5a9d256d3899391376aba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aef4c9502a5d5a9d256d3899391376aba">&#9670;&nbsp;</a></span>ValidateDevice()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::ValidateDevice </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>deviceId</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Validate inference of a specific NPU device connected to the host. This function runs a validation process using the provided input data on the specified NPU device. It can be used to ensure that the NPU device is operational and can process inference tasks correctly. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>Pointer to the input data used for validation. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">deviceId</td><td>ID of the NPU device to validate. Default is 0 (first device). </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Output tensors as a vector of smart pointer instances, representing the validation results. </dd></dl>

</div>
</div>
<a id="a2c6f130d722950153287d4cf1efd5a5d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2c6f130d722950153287d4cf1efd5a5d">&#9670;&nbsp;</a></span>Wait()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::Wait </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>jobId</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Wait until an request is complete and returns output. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">jobId</td><td>job Id returned by <a class="el" href="classdxrt_1_1InferenceEngine.html#af3006c9a7183f7e42d1528aa8a6fea24" title="Non-blocking call to request asynchronous inference by input pointer, and get job ID from inference e...">RunAsync()</a> </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>output tensors as vector of smart pointer instances </dd></dl>

</div>
</div>
</div><!-- contents -->
<div class="ttc" id="aclassdxrt_1_1InferenceOption_html_ae98837ff566fe2b089a5b1bf07feb48c"><div class="ttname"><a href="classdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">dxrt::InferenceOption::devices</a></div><div class="ttdeci">std::vector&lt; int &gt; devices</div><div class="ttdoc">device ID list to use</div><div class="ttdef"><b>Definition:</b> inference_option.h:34</div></div>
<div class="ttc" id="aclassdxrt_1_1InferenceOption_html"><div class="ttname"><a href="classdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a></div><div class="ttdoc">This struct specifies inference options applied to dxrt::InferenceEngine.</div><div class="ttdef"><b>Definition:</b> inference_option.h:21</div></div>
<div class="ttc" id="aclassdxrt_1_1InferenceEngine_html_a0893b724d6eed939556f1315ce05829d"><div class="ttname"><a href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">dxrt::InferenceEngine::outputs</a></div><div class="ttdeci">Tensors outputs(void *ptr=nullptr, uint64_t phyAddr=0)</div><div class="ttdoc">Get output tensor (Legacy API)</div><div class="ttdef"><b>Definition:</b> inference_engine.h:227</div></div>
<div class="ttc" id="aclassdxrt_1_1InferenceOption_html_a9fd270a5eeeb1a14ce5f133c16c30d55"><div class="ttname"><a href="classdxrt_1_1InferenceOption.html#a9fd270a5eeeb1a14ce5f133c16c30d55">dxrt::InferenceOption::boundOption</a></div><div class="ttdeci">uint32_t boundOption</div><div class="ttdoc">Select the NPU core inside the device.</div><div class="ttdef"><b>Definition:</b> inference_option.h:39</div></div>
<div class="ttc" id="aclassdxrt_1_1InferenceEngine_html"><div class="ttname"><a href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a></div><div class="ttdoc">This class abstracts runtime inference executor for user's compiled model.</div><div class="ttdef"><b>Definition:</b> inference_engine.h:60</div></div>
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
