{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"docs/01_DXNN_Runtime_Overview.html","title":"DXNN Runtime Overview","text":"<p>This chapter provides an overview of the DEEPX SDK architecture and explains each core component and its role in the AI development workflow.  </p>"},{"location":"docs/01_DXNN_Runtime_Overview.html#deepx>_sdk>_architecture","title":"DEEPX SDK Architecture","text":"<p>  Figure. DEEPX SDK Architecture    </p> <p>DEEPX SDK is an all-in-one software development platform that streamlines the process of compiling, optimizing, simulating, and deploying AI inference applications on DEEPX NPUs (Neural Processing Units). It provides a complete toolchain, from AI model creation to runtime deployment, optimized for edge and embedded systems, enabling developers to build high-performance AI applications with minimal effort.  </p> <p>DX-COM is the compiler in the DEEPX SDK that converts a pre-trained ONNX model and its associated configuration JSON file into a hardware-optimized .dxnn binary for DEEPX NPUs. The ONNX file contains the model structure and weights, while the JSON file defines pre/post-processing settings and compilation parameters. DX-COM provides a fully compiled .dxnn file, optimized for low-latency and high-efficient inference on DEEPX NPU.  </p> <p>DX-RT is the runtime software responsible for executing ,dxnn models on DEEPX NPU hardware. DX-RT directly interacts with the DEEPX NPU through firmware and device drivers, using PCIe interface for high-speed data transfer between the host and the NPU, and provides C/C++ and Python APIs for application-level inference control. DX-RT offers a complete runtime environment, including model loading, I/O buffer management, inference execution, and real-time hardware monitoring.  </p> <p>DX ModelZoo is a curated collection of pre-trained neural network models optimized for DEEPX NPU, designed to simplify AI development for DEEPX users. It includes pre-trained ONNX models, configuration JSON files, and pre-compiled DXNN binaries, allowing developers to rapidly test and deploy applications. DX ModelZoo also provides benchmark tools for comparing the performance of quantized INT8 models on DEEPX NPU with full-precision FP32 models on CPU or GPU.  </p> <p>DX-STREAM is a custom GStreamer plugin that enables real-time streaming data integration into AI inference applications on DEEPX NPU. It provides a modular pipeline framework with configurable elements for preprocessing, inference, and postprocessing, tailored to vision AI work. DX-Stream allows developers to build flexible, high-performance applications for use cases such as video analytics, smart cameras, and edge AI systems.  </p> <p>DX-APP is a sample application that demonstrates how to run compiled models on actual DEEPX NPU using DX-RT. It includes ready-to-use code for common vision tasks such as object detection, face recognition, and image classification. DX-APP helps developers quickly set up the runtime environment and serves as a template for building and customizing their own AI applications.  </p>"},{"location":"docs/01_DXNN_Runtime_Overview.html#inference>_flow>_of>_dx-rt","title":"Inference Flow of DX-RT","text":"<p>Here is the inference flow of DX-RT.</p> <p>  Figure. Inference Flow of DXNN Runtime    </p> <p>This figure illustrates the inference workflow of the DXNN Runtime SDK, which integrates OpenCV-based input/output handling with efficient NPU-accelerated model execution.</p> <p>Input &amp; Pre-Processing Input data\u2014such as images, camera feeds, or video\u2014is captured using OpenCV. The data is then passed through a Pre-Processing module, which transforms it into \\input tensors suitable for the model.  </p> <p>Feeding Input to the Inference Engine The pre-processed input tensors are fed into the InferenceEngine along with the compiled model (.dxnn). Before execution, you must configure the InferenceOption, which specifies the target device and available resources.  </p> <p>Model Execution The InferenceEngine is the core component of the DXNN Runtime SDK. It: - Initializes and controls the NPU device - Manages memory for input/output tensors - Schedules inference tasks across NPU and CPU, optimizing their interaction for real-time performance</p> <p>Post-Processing &amp; Display The output tensors are processed to a Post-Processing stage, typically involving OpenCV for decoding, formatting, or visualization. Finally, the results are displayed or forwarded to the next processing step. </p>"},{"location":"docs/02_Installation_on_Linux.html","title":"Installation on Linux","text":"<p>This chapter describes the system requirements, source file structure, and the installation instructions for setting up DX-RT on a Linux-based host system.  </p> <p>After you check the system requirements, follow these instructions.  </p> <ul> <li>System Requirement Check  </li> <li>Build Environment Setup  </li> <li>Source File Structure Check  </li> <li>Framework Build  </li> <li>Linux Device Driver Installation  </li> <li>Python Package Installation  </li> <li>Service Registration  </li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#system>_requirements","title":"System Requirements","text":"<p>This section describes the hardware and software requirements for running DX-RT on Linux.</p> <p>Hardware and Software Requirements </p> <ul> <li>CPU: x86_64, aarch64</li> <li>RAM: 8GB RAM (16GB RAM or higher is recommended)</li> <li>Storage: 4GB or higher available disk space</li> <li>OS: Ubuntu 20.04 / 22.04 / 24.04 (x86_64 / aarch64)</li> <li>Hardware: The system must support connection to an M1 M.2 module with the M.2 interface on the host PC. </li> </ul> <p>  Figure. DX-M1 M.2 Module    </p>"},{"location":"docs/02_Installation_on_Linux.html#build>_environment>_setup","title":"Build Environment Setup","text":"<p>DEEPX provides an installation shell script to set up the DX-RT build environment. You can install the entire toolchain installation or perform a partial installation as necessary.</p> <p>DX-RT supports the Target OS of Ubuntu 18.04, Ubuntu 20.04, Ubuntu 22.04, and Ubuntu 24.04. </p> <p>Installation of DX-RT  To install the full DX-RT toolchain, use the following commands.  </p> <pre><code>$ cd dx_rt\n$ ./install.sh --all\n</code></pre> <p>Here are the available <code>install.sh</code> options.  <pre><code>  ./install.sh [ options ]\n    --help            Shows help message\n    --arch [x86_64, aarch64]\n                      Sets target CPU architecture\n    --dep             Installs build dependencies : cmake, gcc, ninja, etc..\n    --onnxruntime     (Optional) Installs onnxruntime library\n    --all             Installs architecture + dependency + onnxruntime library\n</code></pre></p> <p>Installation with ONNX Runtime Use the ONNX Runtime option if you need to offload certain neural network (NN) operations to the CPU that are not supported by the NPU.  </p> <p>We recommend using ONNX Runtime linux x64 version more than v1.20.1. <pre><code>https://github.com/microsoft/onnxruntime/releases/download/v1.20.1/onnxruntime-linux-x64-1.20.1.tgz\n$ sudo tar -xvzf onnxruntime-linux-x64-1.20.1.tgz -C /usr/local --strip-components=1\n$ sudo ldconfig\n</code></pre></p> <p>To install the ONNX Runtime library, run the following command. <pre><code>./install.sh --onnxruntime\n</code></pre></p> <p>Installation for a Specific CPU Architecture The DX-RT targets the x86_64 architecture. If you\u2019re compiling for another architecture (e.g., aarch64), specify it using the <code>--arch</code> option.  <pre><code>./install.sh --arch aarch64 --onnxruntime\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#source>_file>_structure","title":"Source File Structure","text":"<p>The DX-RT source directory is organized as follows.  You can install the full toolchain using the <code>install.sh</code>, and the build and library using <code>build.sh</code>.  </p> <pre><code>\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 cli\n\u251c\u2500\u2500 build.sh\n\u251c\u2500\u2500 build_x86_64\n\u251c\u2500\u2500 build_aarch64\n\u251c\u2500\u2500 cmake\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 examples\n\u251c\u2500\u2500 extern\n\u251c\u2500\u2500 install.sh\n\u251c\u2500\u2500 lib\n\u251c\u2500\u2500 python_package\n\u251c\u2500\u2500 sample\n\u251c\u2500\u2500 service\n\u2514\u2500\u2500 tool\n</code></pre> <ul> <li><code>assets</code>: Images for documentation</li> <li><code>bin</code>: Compiled binary executables</li> <li><code>cli</code>: Command-line application source code</li> <li><code>build.sh</code>: Shell script for building the framework</li> <li><code>build_arch</code>: Build outputs for aarch64 architecture</li> <li><code>cmake</code>: CMake scripts for build configuration</li> <li><code>docs</code>: Markdown documents </li> <li><code>examples</code>: Inference example files</li> <li><code>extern</code>: Third-party libraries</li> <li><code>install.sh</code>: Shell script for toolchain installation</li> <li><code>lib</code>: DX-RT library sources</li> <li><code>python_package</code>: Python modules for DX-RT</li> <li><code>sample</code>: Sample input files for demo apps</li> <li><code>service</code>: Service unit files for runtime management</li> <li><code>tool</code>: Profiler result visualization tools</li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#framework>_build>_on>_linux","title":"Framework Build on Linux","text":"<p>After compiling the DX-RT environment setup, you can build the framework using the provided <code>build.sh</code> shell script.</p>"},{"location":"docs/02_Installation_on_Linux.html#framework>_source>_build","title":"Framework Source Build","text":"<p>DEEPX supports the default target CPU architecture as x86_64, aarch64.  </p> <p>The build script also supports options for build cleaning, specifying build type, and installing libraries to the system paths.  </p> <p>Build Instructions To build the DX-RT framework, run the following command. <pre><code>$ cd dx_rt\n$ ./build.sh\n</code></pre></p> <p>Here are the <code>build.sh</code> options and their descriptions. <pre><code>./build.sh [ options ]\n    --help       Shows help message\n    --clean      Cleans previous build artifacts\n    --verbose    Shows full build commands during execution\n    --type [Release, Debug, RelWithDebInfo]\n                 Specifies the cmake build type\n    --arch [x86_64, aarch64]\n                 Sets target CPU architecture\n    --install &lt;path&gt;\n                 Sets the installation path for built libraries\n    --uninstall  Removes installed DX-RT files\n    --clang      Compiles using clang\n</code></pre></p> <p>Example. Build with <code>clean</code> Option To clean existing build files before rebuilding. <pre><code>$ ./build.sh --clean\n</code></pre></p> <p>Example. Build with <code>library</code> Option To install build library files to <code>/usr/local</code>.  <pre><code># default path is /usr/local\n$ ./build.sh --install /usr/local\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#options>_for>_build>_target","title":"Options for Build Target","text":"<p>DX-RT supports configuration  build targets, allowing you to enable or disable option features such as  ONNX Runtime, Python API, multi-process service support, and shared library builds.  </p> <p>You can configure these options by editing the following file: <code>cmake/dxrt.cfg.cmake</code> </p> <p>Here are the available options for building targets. <pre><code>option(USE_ORT \"Use ONNX Runtime\" OFF)\noption(USE_PYTHON \"Use Python\" OFF)\noption(USE_SERVICE \"Use Service\" OFF)\noption(USE_SHARED_DXRT_LIB \"Build for DX-RT Shared Library\" ON)\n</code></pre></p> <ul> <li><code>USE_ORT</code>: Enables ONNX Runtime for NN (neural network) operations that NPU does not support</li> <li><code>USE_PYTHON</code>: Enables Python API support</li> <li><code>USE_SERVICE</code>: Enables service for multi-process support </li> <li><code>USE_SHARED_DXRT_LIB</code>: Builds DX-RT as shared library (default: ON)</li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#build>_guide>_for>_cross-compile","title":"Build Guide for Cross-compile","text":"<p>Setup Files for Cross-compile  DEEPX supports cross-compilation for the following default target CPU Architecture: x86_64, aarch64. DEEPX supports  the default target CPU architecture as x86_64.  </p> <p>Toolchain Configuration To cross-compile for a specific target, configure the toolchain file. <code>cmake/toolchain.&lt;CMAKE_SYSTEM_PROCESSOR&gt;.cmake</code></p> <p>Example To cross-compile files for aarch64. <pre><code>SET(CMAKE_C_COMPILER /usr/bin/aarch64-linux-gnu-gcc )\nSET(CMAKE_CXX_COMPILER /usr/bin/aarch64-linux-gnu-g++ )\nSET(CMAKE_LINKER /usr/bin/aarch64-linux-gnu-ld )\nSET(CMAKE_NM /usr/bin/aarch64-linux-gnu-nm )\nSET(CMAKE_OBJCOPY /usr/bin/aarch64-linux-gnu-objcopy )\nSET(CMAKE_OBJDUMP /usr/bin/aarch64-linux-gnu-objdump )\nSET(CMAKE_RANLIB /usr/bin/aarch64-linux-gnu-ranlib )\n</code></pre></p> <p>Non Cross-compile Case (Build on Host) To build and install DX-RT on the host system, run the following command. <pre><code>./build.sh --install /usr/local\n</code></pre></p> <p>Recommended install path: <code>/usr/local</code> (commonly included in OS search paths)  </p> <p>Cross-compile Case (Build for Target Architecture) Cross-compile for a specific architecture, run the following command. <pre><code>./build.sh --arch &lt;target_cpu&gt;\n</code></pre></p> <p>Here are the <code>build.sh</code> options and their descriptions. <pre><code>./build.sh [ options ]\n   --help   Shows help message\n   --clean  Cleans previous build artifacts\n   --verbose    Shows full build commands during execution\n   --type   Specifies the cmake build type : [ Release, Debug, RelWithDebInfo ]\n   --arch   Sets target CPU architecture : [ x86_64, aarch64 ]\n   --install    Installs build libraries \n   --uninstall  Removes installed DX-RT files\n</code></pre></p> <p>Here are the examples of cross-compile cases. <pre><code>./build.sh --arch aarch64\n./build.sh --arch x86_64\n</code></pre></p> <p>Output Directory After a successful build, output binaries is located under <code>&lt;build directory&gt; /bin/</code> <pre><code>&lt;build directory&gt;/bin/\n \u251c\u2500\u2500 dxrtd\n \u251c\u2500\u2500 dxrt-cli\n \u251c\u2500\u2500 parse_model\n \u251c\u2500\u2500 run_model\n \u2514\u2500\u2500 examples\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#linux>_device>_driver>_installation","title":"Linux Device Driver Installation","text":"<p>After building the DX-RT framework, you can install the Linux device driver for M1 AI Accelerator (NPU).  </p>"},{"location":"docs/02_Installation_on_Linux.html#prerequisites","title":"Prerequisites","text":"<p>Before installing the Linux device driver, you should check that the accelerator device is properly recognized by the system.  </p> <p>To check PCIe device recognition, run the following command. <pre><code>$ lspci -vn | grep 1ff4\n0b:00.0 1200: 1ff4:0000\n</code></pre></p> <p>Note. If there is no output, the PCIe link is not properly connected. Please check the physical connection and system BIOS settings.  </p> <p>Optional. Display the DEEPX name in <code>lspci</code>. If you want to display the DEEPX name in <code>lspci</code>, you can modify the PCI DB. (Only for Ubuntu)  </p> <p>To display the DeepX device name, run the following command. <pre><code>$ sudo update-pciids\n$ lspci\n...\n0b:00.0 Processing accelerators: DEEPX Co., Ltd. DX_M1\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#linux>_device>_driver>_structure","title":"Linux Device Driver Structure","text":"<p>The DX-RT Linux device driver source is structured to support flexible builds across devices, architectures, and modules. The directory layout is as follows. <pre><code>- .gitmodules\n\n- [modules]\n    |\n    - device.mk     \n    - kbuild\n    - Makefile\n    - build.sh\n    - [rt]\n        - Kbuild\n    - [pci_deepx] : submodule\n        - Kbuild\n\n- [utils] : submodule\n</code></pre></p> <ul> <li><code>device.mk</code>: Device configuration file</li> <li><code>kbuild</code>: Top-level build rules</li> <li><code>Makefile</code>: Build entry point</li> <li><code>build.sh</code>: Build automation script</li> <li><code>rt</code>: Runtime driver source (<code>dxrt_driver.ko</code>)</li> <li><code>pci_deepx</code>: PCIe DMA driver (<code>submodule, dx_dma.ko</code>)</li> <li><code>utils</code>: Supporting utilities (<code>submodule</code>)</li> </ul> <p>Here are the descriptions of the key components.  </p> <p><code>device.mk</code> Defines supported device configuration.  </p> <p>To build for a specific device, run the following command. <pre><code>$ make DEVICE=[device]\n</code></pre></p> <p>For example, in the case of a device like M1, you should select a submodule, such as PCIe, that has a dependency on M1. <pre><code>$ make DEVICE=m1 PCIE=[deepx]\n</code></pre></p> <p><code>kbuild</code> Linux kernel build configuration file for each module directory. It instructs the kernel build system on how to compile driver modules.  </p> <p><code>build.sh</code> Shell script to streamline the build process. It runs the Makefile with common options. </p> <p>Here are the options for <code>build.sh</code>. <pre><code>Usage:\nUsage:\n   build.sh &lt;options&gt;\n\noptions:\n   -d, --device   [device]      select target device: m1\n   -m, --module   [module]      select PCIe module: deepx\n   -k, --kernel   [kernel dir]  'KERNEL_DIR=[kernel dir]', The directory where the \n                                kernel source is located \n                                default: /lib/modules/6.5.0-18-generic/build)\n   -a, --arch     [arch]        set 'ARCH=[arch]' Target CPU architecture for \n                                cross-compilation, default: x86_64\n   -t, --compiler [cross tool]  'CROSS_COMPILE=[cross tool]' cross compiler binary, \n                                e.g aarch64-linux-gnu-\n   -i, --install  [install dir] 'INSTALL_MOD_PATH=[install dir]', module install \n                                directory install to: \n                                [install dir]/lib/modules/[KERNELRELEASE]/extra/\n   -c, --command  [command]     clean | install | uninstall\n                                - uninstall: Remove the module files installed \n                                on the host PC.\n   -j, --jops     [jobs]        set build jobs\n   -f, --debug    [debug]       set debug feature [debugfs | log | all]\n   -v, --verbose                build verbose (V=1)\n   -h, --help                   show this help\n</code></pre></p> <p>The build process generates the following kernel modules.  </p> <ul> <li> <p><code>modules/rt</code> -&gt; <code>dxrt_driver.ko</code>     : a core runtime driver for M1 NPU devices. This is responsible for system-level communication, memory control, and device command execution. </p> </li> <li> <p><code>modules/pci_deepx</code> -&gt; <code>dx_dma.ko</code>     : PCIe DMA (Direct Memory Access) kernel module for high-speed data transfer between host and the M1 device. This enables efficient data movement with minimal CPU overhead, ideal for real-time and data intensive AI workloads. </p> </li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#linux>_device>_driver>_build","title":"Linux Device Driver Build","text":"<p>After completing the environment setup of the DXNN Linux Device Driver, you can build the kernel modules using either the make(Makefile) or <code>build.sh</code> script. Both methods are supported by DEEPX.  </p> <p>Option 1. Build Using <code>Makefile</code> </p> <p><code>build</code> <pre><code>e.g $ cd modules\ne.g $ make DEVICE=m1 PCIE=deepx\n</code></pre></p> <p><code>clean</code> <pre><code>e.g $ cd modules\ne.g $ make DEVICE=m1 PCIE=deepx clean\n</code></pre></p> <p><code>install</code> Installs the driver to: <code>/lib/modules/$(KERNELRELEASE)/extra/</code> <pre><code>e.g $ cd modules\ne.g $ make DEVICE=m1 PCIE=deepx install\n</code></pre></p> <p>Option 2. Build Using <code>build.sh</code> </p> <p>Use this method if your system supports self-compiling kernel modules (<code>.ko</code> files).</p> <p><code>build</code> <pre><code>e.g $ ./build.sh -d m1 -m deepx\n(Default device: m1, PCI3 module: deepx)\n</code></pre></p> <p><code>clean</code> <pre><code>e.g $ ./build.sh -c clean\n</code></pre></p> <p><code>install</code> Installs the driver to: <code>/lib/modules/$(KERNELRELEASE)/extra/</code> <pre><code>e.g $ sudo ./build.sh -c install\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#auto-loading>_modules>_at>_boot>_time","title":"Auto-Loading Modules at Boot Time","text":"<p>DEEPX allows kernel modules to be automatically loaded at system boot, either through manual setup or using the <code>build.sh</code> script.  </p> <p>Manual Installation Method </p> <p>Step 1. Install Kernel Modules Installs modules to: <code>/lib/modules/$(KERNELRELEASE)/extra/</code> <pre><code>make DEVICE=m1 PCIE=deepx install\n</code></pre></p> <p>Step 2. Update Module Dependencies Updates: <code>/lib/modules/$(KERNELRELEASE)/modules.dep</code> <pre><code>$ sudo depmod -A\n</code></pre></p> <p>Step 3. Add Module Confiduration Copy the preconfigured module config file. <pre><code>$ sudo cp modules/dx_dma.conf /etc/modprobe.d/\n</code></pre></p> <p>This ensures the modules (<code>dx_dma</code>) are auto-loaded on boot.  </p> <p>Step 4. Test with modprobe To verify the correct installation. <pre><code>$ sudo modprobe dx_dma\n$ lsmod\n  dxrt_driver            40960  0\n  dx_dma                176128  1 dxrt_driver\n</code></pre></p> <p>Automated Installation Using <code>build.sh</code> The <code>build.sh</code> script automates installation and setup, including dependency updates and modprobe configuration.  </p> <p>Run the following command <pre><code>$ sudo ./build.sh -d m1 -m deepx -c install\n- DEVICE        : m1\n- PCIE          : deepx\n- MODULE CONF   : /.../rt_npu_linux_driver/modules/dx_dma.conf\n- ARCH (HOST)   : x86_64\n- KERNEL        : /lib/modules/5.15.0-102-generic/build\n- INSTALL       : /lib/modules/5.15.0-102-generic/extra/\n\n *** Build : install ***\n$ make DEVICE=m1 PCIE=deepx install\n\nmake -C /lib/modules/5.15.0-102-generic/build M=/home/jhk/deepx/dxrt/module/rt_npu_linux_driver/modules  modules_install\n ....\n - SUCCESS\n\n *** Update : /lib/modules/5.15.0-102-generic/modules.dep ***\n $ depmod -A\n $ cp /home/jhk/deepx/dxrt/module/rt_npu_linux_driver/modules/dx_dma.conf /etc/modprobe.d/\n</code></pre></p> <p>Uninstalling Modules To completely remove the installed modules and configs. <pre><code>$ ./build.sh -d m1 -m deepx -c uninstall\n- DEVICE        : m1\n- PCIE          : deepx\n- MODULE CONF   : /.../rt_npu_linux_driver/modules/dx_dma.conf\n- ARCH (HOST)   : x86_64\n- KERNEL        : /lib/modules/5.15.0-102-generic/build\n- INSTALL       : /lib/modules/5.15.0-102-generic/extra/\n\n *** Remove : /lib/modules/5.15.0-102-generic/extra ***\n $ rm -rf /lib/modules/5.15.0-102-generic/extra/pci_deepx\n $ rm -rf /lib/modules/5.15.0-102-generic/extra/rt\n\n *** Remove : /etc/modprobe.d ***\n $ rm /etc/modprobe.d/dx_dma.conf\n\n *** Update : /lib/modules/5.15.0-102-generic/modules.dep ***\n $ depmod\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#python>_package>_installation","title":"Python Package Installation","text":"<p>DEEPX provides a Python package for DX-RT, available under the module name dx-engine. It supports Python 3.8 or later and allows you to interface with DX-RT in Python-based applications. </p> <p>Installation Steps 1. Navigate to the python_package directory.  <pre><code>$ cd python_package\n</code></pre></p> <p>2. Install the package <pre><code>$ pip install .\n</code></pre></p> <p>3. Verify the installation <pre><code>$ pip list | grep dx\ndx-engine          1.0.0\n</code></pre></p> <p>For details on using DX-RT with Python, refer to Section 6.2 Python in 6. Programming Guide.</p>"},{"location":"docs/02_Installation_on_Linux.html#service>_registration","title":"Service Registration","text":"<p>DX-RT supports multi-process operation through the background service (<code>dxrtd daemon</code>). To enable the multi-process feature, you must build the Runtime with Service support and the service must be registered in the system below.  </p> <p>Note.   - DX-RT must be built with <code>USE_SERVICE=ON</code>. (default setting)   - DX-RT must be registered and managed as a system service using <code>systemd</code>.  </p> <p>Registering and Running the DX-RT Service 1. Modify the service unit file.   Ensure the ExecStart path is correctly configured. <pre><code>$ vi ./service/dxrt.service\n</code></pre></p> <p>2. Copy the service file to the system folder. <pre><code>$ sudo cp ./service/dxrt.service /etc/systemd/system\n</code></pre></p> <p>3. Start the service. <pre><code>$ sudo systemctl start dxrt.service\n</code></pre></p> <p>Service Management Commands <pre><code>$ sudo systemctl stop dxrt.service          # Stop the service\n$ sudo systemctl status dxrt.service        # Check service status\n$ sudo systemctl restart dxrt.service       # Restart the service\n$ sudo systemctl enable dxrt.service        # Enable on boot\n$ sudo systemctl disable dxrt.service       # Disable on boot\n$ sudo journalctl -u dxrt.service           # View service logs\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#sanity>_check","title":"Sanity Check","text":"<p>The Sanity Check is a script used to quickly verify that a driver has been installed correctly and that the device is recognized properly.</p> <pre><code>$ sudo ./SanityCheck.sh\n============================================================================\n==== Sanity Check Date : DATE ====\nLog file location : .../dx_rt/dx_report/sanity/result/sanity_check_result_[date]_[hh/mm/ss].log\n\n==== PCI Link-up Check ====\n[OK] Vendor ID 1ff4 is present in the PCI devices.(num=2)\n==== Device File Check ====\n[OK] /dev/dxrt0 exists.\n[OK] /dev/dxrt0 is a character device.\n[OK] /dev/dxrt0 has correct permissions (0666).\n[OK] /dev/dxrt1 exists.\n[OK] /dev/dxrt1 is a character device.\n[OK] /dev/dxrt1 has correct permissions (0666).\n==== Kernel Module Check ====\n[OK] dxrt_driver module is loaded.\n[OK] dx_dma module is loaded.\n[OK] PCIe 02:00.0 driver probe is success.\n[OK] PCIe 07:00.0 driver probe is success.\n\n============================================================================\n** Sanity check PASSED!\n============================================================================\n</code></pre>"},{"location":"docs/03_Installation_on_Windows.html","title":"Installation on Windows","text":"<p>This chapter describes the instructions for installing and using DX-RT on a Windows system.  </p>"},{"location":"docs/03_Installation_on_Windows.html#system>_requirements","title":"System Requirements","text":"<p>This section describes the hardware and software requirements for running DX-RT on Windows.  </p> <ul> <li>RAM: 8GB RAM (16GB RAM or higher is recommended)</li> <li>Storage: 4GB or higher available disk space</li> <li>OS: Windows 10 / 11</li> <li>Python: Version 3.11 (for Python module support)</li> <li>Compiler: Visual Studio 2022 (required for building C++ examples)</li> <li>Hardware: The system must support connection to an M1 M.2 module with the M.2 interface on the host PC.  </li> </ul> <p>The current version only supports Single-process and does not support Multi-process.  </p> <p>  Figure. DX-M1 M.2 Module    </p>"},{"location":"docs/03_Installation_on_Windows.html#execute>_installer","title":"Execute Installer","text":"<p>DEEPX provides the Windows installer executable file for DX-RT.  </p> <ul> <li><code>DXNN_Runtime_v[version]_windows_[architecture].exe</code> </li> </ul> <p>Here is an example of the execution file.  </p> <ul> <li><code>DXNN_Runtime_vX.X.X_windows_amd64.exe</code> </li> </ul> <p>Default Directory Path </p> <ul> <li>'C:/DevTools/DXNN/dxrt_v[version]'  </li> </ul> <p>Once you install the exe file, the driver will be installed automatically. So you can verify the installation via Device Manager under DEEPX_DEVICE.  </p> <p>Note. If Visual Studio 2022 is not installed, you may be prompted to install the Microsoft Visual C++ Redistributable (<code>VC_redist.x64.exe</code>) using administrator permissions.  </p>"},{"location":"docs/03_Installation_on_Windows.html#file>_structure","title":"File Structure","text":"<pre><code>\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 drivers\n\u251c\u2500\u2500 examples\n\u251c\u2500\u2500 firmware\n\u251c\u2500\u2500 include\n\u251c\u2500\u2500 lib\n\u2514\u2500\u2500 python_package\n</code></pre> <ul> <li><code>bin</code>: Compiled binary executables</li> <li><code>docs</code>: Markdown documents </li> <li><code>examples</code>: Inference example files</li> <li><code>include</code>: Header files for DX-RT libraries</li> <li><code>lib</code>: Pre-built DX-RT libraries</li> <li><code>python_package</code>: Python modules for DX-RT</li> <li><code>sample</code>: Sample input files for demo apps</li> <li><code>service</code>: Service unit files for runtime management</li> <li><code>tool</code>: Profiler result visualization tools</li> </ul>"},{"location":"docs/03_Installation_on_Windows.html#running>_examples","title":"Running Examples","text":"<p>DX-RT includes sample programs in both C++ and Python.  </p>"},{"location":"docs/03_Installation_on_Windows.html#building>_c>_examples","title":"Building C++ Examples","text":"<p>Visual Studio 2022 should be installed on your PC.  </p> <p>1. Open the solution file in the following location. <code>examples\\&lt;example-name&gt;\\msvc\\&lt;example-name&gt;.sln</code></p> <p>2. In Visual Studio, Click Rebuild Solution.  </p> <p>Once the build is complete, an <code>x64</code> directory is generated in the same location as the solution file. The executable file of the sample includes the Debug or Release sub-folder.  </p>"},{"location":"docs/03_Installation_on_Windows.html#running>_c>_examples","title":"Running C++ Examples","text":"<p>1. Run the executable file of the sample at the following location. <code>examples\\&lt;example-name&gt;\\msvc\\x64\\[Debug|Release]\\&lt;example-name&gt;.exe</code></p> <p>Example <pre><code>C:\\...&gt; cd .\\examples\\run_async_model\\msvc\\x64\\Release\nC:\\...\\examples\\run_async_model\\msvc\\x64\\Release&gt; .\\run_async_model.exe model.dxnn 100\n</code></pre></p>"},{"location":"docs/03_Installation_on_Windows.html#python>_package>_installation","title":"Python Package Installation","text":"<p>DEEPX provides a Python module named <code>dx_engine</code> for Python 3.11.  </p> <p>1. Build and Install the Package Navigate to the Python package directory and install the module.  </p> <pre><code>C:\\...\\dxrt_vX.X.X&gt; cd python_package/\nC:\\...\\dxrt_vX.X.X\\python_package&gt; pip install .\n</code></pre> <p>2. Verify the Installation Open a Python shell and check the installed version.  </p> <pre><code>C:\\...&gt; python\n... \n&gt;&gt;&gt; from dx_engine import version\n&gt;&gt;&gt; print(version.__version__)\n1.0.1\n</code></pre> <p>Examples <pre><code>cd examples/python\nC:\\...\\examples\\python&gt; python run_async_model.py ...model.dxnn 10\n</code></pre></p>"},{"location":"docs/04_Model_Inference.html","title":"Model Inference","text":""},{"location":"docs/04_Model_Inference.html#model>_file>_format","title":"Model File Format","text":"<p>The original ONNX model is converted by DX-COM into the following structure.</p> <pre><code>Model dir.\n    \u2514\u2500\u2500 graph.dxnn\n</code></pre> <ul> <li> <code>graph.dxnn</code> A unified DEEPX artifact that contains  NPU command data, model metadata, model parameters.   </li> </ul> <p>This file is used directly for inference on DEEPX hardware</p>"},{"location":"docs/04_Model_Inference.html#inference>_workflow","title":"Inference Workflow","text":"<p>Here the inference workflow using the DXNN Runtime as follows.  </p> <p>  Figure. Inference Workflow    </p> <ul> <li>1. Compiled Model and optional InferenceOption are provided to initialize the InferenceEngine.  </li> <li>2. Pre-processed Input Tensors are passed to the InferenceEngine for inference.  </li> <li>3. The InferenceEngine produces Output Tensors as a result of the inference.  </li> <li>4. These outputs are then passed to the Post-Processing stage for interpretation or further action.  </li> </ul>"},{"location":"docs/04_Model_Inference.html#prepare>_the>_model","title":"Prepare the Model","text":"<p>Choose one of the following options.  </p> <ul> <li>Use a pre-built model from DX ModelZoo </li> <li>Compile an ONNX model into the DX-RT format using DX-COM (Refer to the DX-COM User Guide for details.)  </li> </ul>"},{"location":"docs/04_Model_Inference.html#configure>_inference>_options","title":"Configure Inference Options","text":"<p>Create a <code>dxrt::InferenceOption</code> object to configure runtime settings for the inference engine.  </p> <p>Note. This option is temporarily unsupported in the current version, and will be available in the next release.</p>"},{"location":"docs/04_Model_Inference.html#load>_the>_model>_into>_the>_inference>_engine","title":"Load the Model into the Inference Engine","text":"<p>Create a <code>dxrt::InferenceEngine</code> instance using the path to the compiled model directory. Hardware resources are automatically initialized during this step.  </p> <p>If <code>dxrt::InferenceEngine</code> is not provided, a default option is applied.  </p> <pre><code>auto ie = dxrt::InferenceEngine(\"yolov5s.dxnn\");\nauto ie = dxrt::InferenceEngine(\"yolov5s.dxnn\", &amp;option);\n</code></pre>"},{"location":"docs/04_Model_Inference.html#connect>_input>_tensors","title":"Connect Input Tensors","text":"<p>Prepare input buffers for inference.  </p> <p>The following example shows how to initialize the buffer with the appropriate size.  </p> <pre><code>std::vector&lt;uint8_t&gt; inputBuf(ie.GetInputSize(), 0);  \n</code></pre> <p>Refer to DX-APP User Guide for practical examples on connecting inference engines to image sources such as cameras or video, along with the preprocessing routines. </p>"},{"location":"docs/04_Model_Inference.html#inference","title":"Inference","text":"<p>DX-RT provides both synchronous and asynchronous execution modes for flexible inference handling.  </p> <p>1. Run - Synchronous Execution Use the <code>dxrt::InferenceEngine::Run()</code> method for blocking, single-core inference.  </p> <pre><code>auto outputs = ie.Run(inputBuf.data());\n</code></pre> <ul> <li>This method processes input and output on the same thread.  </li> <li>This method is suitable for simple and sequential workloads.  </li> </ul> <p>2. Run - Asynchronous Execution </p> <p>a. With <code>Wait()</code> </p> <p>Use <code>RunAsync()</code> to perform the inference in non-blocking mode, and retrieve results later with <code>Wait()</code>.  </p> <pre><code>auto jobId = ie.RunAsync(inputBuf.data());\nauto outputs = ie.Wait(jobId);\n</code></pre> <ul> <li>This method is ideal for parallel workloads where inference can run in the background.  </li> <li>This method is continuously executed while waiting for the result.  </li> </ul> <p>b. With Callback  </p> <p>Use a callback function to handle output as soon as inference completes. </p> <pre><code>std::function&lt;int(vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt;, void*)&gt; postProcCallBack = \\\n    [&amp;](vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt; outputs, void *arg)\n    {\n        /* Process output tensors here */\n        ... ...\n        return 0;\n    };\nie.RegisterCallback(postProcCallBack)\n</code></pre> <ul> <li>The callback is triggered by a background thread after inference.  </li> <li>You can pass a custom argument to track input/output pairs.</li> </ul> <p>Note. Output data is only valid within the callback scope.  </p>"},{"location":"docs/04_Model_Inference.html#process>_output>_tensors","title":"Process Output Tensors","text":"<p>Once inference is complete, the output tensors are processed using Tensor APIs and custom post-processing logic. You can find the templates and example code in DX-APP to help you implement post-process smoothly. As noted earlier, using callbacks allows for more efficient and real-time post-processing.  </p>"},{"location":"docs/04_Model_Inference.html#multiple>_device>_inference","title":"Multiple Device Inference","text":"<p>This feature is not applicable to single-NPU devices. Basically, the inference engine schedules and manages multiple devices in real time. If the inference option is explicitly set, the inference engine may only use specific devices during real-time inference for the model.  </p>"},{"location":"docs/04_Model_Inference.html#data>_format>_of>_device>_tensor","title":"Data Format of Device Tensor","text":"<p>Compiled models use the NHWC format by default.  </p> <p>However, the input tensor formats on the device side may vary depending on the hardware\u2019s processing type.  </p> <p>Input Tensor Formats </p> Type Compiled Model Format Device Format Data Size <code>Formatter</code> <code>[N, H, W, C]</code> <code>[N, H, W, C]</code> 8-bit <code>IM2COL</code> <code>[N, H, W, C]</code> <code>[N, H, align64(W*C)]</code> 8-bit <ul> <li>Formatter Type Example: <code>[1, 3, 224, 224] (NCHW) -&gt; [1, 224, 224, 3] (NHWC)</code> </li> <li>IM2COL Type Example: <code>[1, 3, 224, 224] (NCHW) -&gt; [1, 224, 224*3+32] (NH, aligned width x channel)</code> </li> </ul> <p>Output Tensor Formats </p> <p>The output tensor format is also aligned with the NHWC format, but with padding applied for alignment.</p> Type Compiled Model Format Device Format <code>Aligned NHWC</code> <code>[N, H, W, C]</code> <code>[N, H, W, align64(C)]</code> <ul> <li>Output Example: <code>[1, 40, 52, 36] (NCHW) -&gt; [1, 52, 36, 40+24]</code>    (Channel size is aligned for optimal memory access.)  </li> </ul> <p>Post-processing can be performed directly without converting formats. API to convert from device format to NCHW/NHWC format will be supported in the next release.  </p>"},{"location":"docs/04_Model_Inference.html#profile>_application","title":"Profile Application","text":""},{"location":"docs/04_Model_Inference.html#gather>_timing>_data>_per>_event","title":"Gather Timing Data per Event","text":"<p>You can profile events within your application using the Profiler APIs. Please refer to Section 8. API reference.  </p> <p>Here is a basic usage example. </p> <pre><code>auto&amp; profiler = dxrt::Profiler::GetInstance();\nprofiler.Start(\"1sec\");\nsleep(1);\nprofiler.End(\"1sec\");\n</code></pre> <p>After the application is finished, <code>profiler.json</code> is created in the working directory.</p>"},{"location":"docs/04_Model_Inference.html#visualize>_profiler>_data","title":"Visualize Profiler Data","text":"<p>You can visualize the profiling results using the following Python script.  </p> <pre><code>python3 tool/profiler/plot.py --input profiler.json\n</code></pre> <p>This generates an image file named <code>profiler.png</code>, providing a detailed view of runtime event timing for performance analysis. </p> <p>  Figure. DX-RT Profiling Report    </p> <p>Script Usage: <code>tool/profiler/plot.py</code> </p> <p>Use this script to draw a timing chart from profiling data generated by DX-RT.</p> <pre><code>usage: plot.py [-h] [-i INPUT] [-o OUTPUT] [-s START] [-e END] [-g]\n</code></pre> <p>Optional Arguments  </p> <ul> <li><code>-h, --help</code>: Show help message and exit  </li> <li><code>-i INPUT, --input INPUT</code>: Input <code>.json</code> file to visualize (e.g., <code>profiler.json</code>)  </li> <li><code>-o OUTPUT, --output OUTPUT</code>: Output image file name to save (e.g., profiler.png)  </li> <li><code>-s START, --start START</code>: Starting position (normalized, &gt; 0.0) within the time interval [0.0-1.0]  </li> <li><code>-e END, --end END</code>: End position (normalized, &lt; 1.0) within the time interval [0.0-1.0]  </li> <li><code>-g, --show_gap</code>: Show time gaps between the start point of each event  </li> </ul> <p>Please refer to usage of <code>tool/profiler/plot.py</code>.  </p> <pre><code>usage: plot.py [-h] [-i INPUT] [-o OUTPUT] [-s START] [-e END] [-g]\n</code></pre>"},{"location":"docs/04_Model_Inference.html#how>_to>_create>_an>_application>_using>_dx-rt","title":"How To Create an Application Using DX-RT","text":"<p>This guide provides step-by-step instructions for creating a new CMake project using the DX-RT library.</p> <p>1. Build the DX-RT Library  Before starting, make sure the DX-RT library is already built.  </p> <p>Refer to Chapter 2. Installation on Linus and Chapter 3. Installation on Windows for detailed build instructions. </p> <p>2. Create a New CMake Project  Create a project directory and an initial <code>CMakeLists.txt</code> file. <pre><code>mkdir MyProject\ncd MyProject\ntouch CMakeLists.txt\n</code></pre></p> <p>3. \u201cHello World\u201d with DX-RT API Create a simple source file (<code>main.cpp</code>) that uses a DX-RT API.  </p> <pre><code>#include \"dxrt/dxrt_api.h\"\nusing namespace std;\n\nint main(int argc, char *argv[])\n{\n auto&amp; devices = dxrt::CheckDevices();\n cout &lt;&lt; \"hello, world\" &lt;&lt; endl;\n return 0;\n}\n</code></pre> <p>4. Modify CMakeLists.txt Edit the <code>CMakeLists.txt</code> file as follows.  </p> <pre><code>cmake_minimum_required(VERSION 3.14)\nproject(app_template)\n\nset(CMAKE_CXX_STANDARD_REQUIRED \"ON\")\nset(CMAKE_CXX_STANDARD \"14\")\n\n# Set the DX-RT library installation path (adjust as needed)\nset(DXRT_LIB_PATH \"/usr/local/lib\") \n\n# Locate the DX-RT library\nfind_library(DXRT_LIBRARY REQUIRED NAMES dxrt_${CMAKE_SYSTEM_PROCESSOR} PATHS $\n{DXRT_LIB_PATH})\n\n# Add executable and link libraries\nadd_executable(HelloWorld main.cpp)\ntarget_link_libraries(HelloWorld PRIVATE ${DXRT_LIBRARY} protobuf)\n</code></pre> <p>Replace <code>/usr/local/lib</code> with the actual path where the DX-RT library is installed.</p> <p>5. Build the Project Compile your project using the following commands. <pre><code>mkdir build\ncd build\ncmake ..\nmake\n</code></pre></p> <p>6. Run the Executable After a successful build, run the generated executable.   </p> <pre><code>./HelloWorld\n</code></pre> <p>You now successfully create and build a CMake project using the DX-RT library. </p>"},{"location":"docs/04_Model_Inference.html#optional>_improving>_cpu>_task>_throughput>_with>_dxrt>_dynamic>_cpu>_thread","title":"(Optional) Improving CPU Task Throughput with DXRT_DYNAMIC_CPU_THREAD","text":"<p>The USE_ORT option allows for enabling ONNX Runtime to handle operations that are not supported by the NPU.  When this option is active, the model's CPU tasks are executed via ONNX Runtime. </p> <p>To mitigate potential bottlenecks in these CPU tasks, especially under varying Host CPU conditions, an optional dynamic multi-threading feature is provided. This feature monitors the input queue load to identify CPU task congestion. If a high load is detected, it dynamically increases the number of threads allocated to CPU tasks, thereby improving their throughput. This dynamic CPU threading can be enabled by setting the DXRT_DYNAMIC_CPU_THREAD=ON environment variable (e.g., export DXRT_DYNAMIC_CPU_THREAD=ON). </p> <p>Additionally, if the system observes that CPU tasks are experiencing significant load, it will display a message: \"To improve FPS, set: 'export DXRT_DYNAMIC_CPU_THREAD=ON'\", recommending the activation of this feature for better performance.</p> <p>Warning: Enabling the DXRT_DYNAMIC_CPU_THREAD=ON option does not always guarantee an FPS increase; its effectiveness can vary depending on the specific workload and system conditions.</p>"},{"location":"docs/05_Command_Line_Interface.html","title":"Command Line Interface","text":"<p>This chapter introduces DX-RT command-line tools for model inspection, execution, and device management.</p>"},{"location":"docs/05_Command_Line_Interface.html#parse>_model","title":"Parse Model","text":"<p>This tool is used to parse and inspect a compiled model file (<code>.dxnn</code>), printing model structure and metadata.  </p> <p>Source: <code>bin/parse_model.cpp</code></p> <p>Usage <pre><code>parse_model -m &lt;model_dir&gt;\n</code></pre></p> <p>Option </p> <ul> <li><code>-m, --model</code>: Path to the compiled model file  (<code>.dxnn</code>)  </li> <li><code>-h, --help</code>: Show help message  </li> </ul> <p>Example <pre><code>$./parse_model -m model.dxnn\n</code></pre></p>"},{"location":"docs/05_Command_Line_Interface.html#run>_model","title":"Run Model","text":"<p>This tool runs a compiled model and performs a basic inference test. It measures inference time, validates output data against a reference, and optionally runs in a loop for stress testing.  </p> <p>Source: <code>bin/run_model.cpp</code></p> <p>Usage <pre><code>run_model -m &lt;model_dir&gt; -i &lt;input_bin&gt; -o &lt;output_bin&gt; -r &lt;reference output_bin&gt; -l &lt;number of loops&gt;\n</code></pre></p> <p>Option </p> <ul> <li><code>-c, --config</code>: Path to a JSON configuration file  </li> <li><code>-m, --model</code>: Path to the compiled model file (<code>.dxnn</code>)  </li> <li><code>-i, --input</code>: Input binary file  </li> <li><code>-o, --output</code>: Output file to save inference results  </li> <li><code>-r, --ref</code>: Reference output file to compare results  </li> <li><code>-l, --loop</code>: Number of inference iteration to run (loop test)</li> <li><code>--use-ort</code>: use ONNX Runtime</li> <li><code>-h, --help</code>: Show help message  </li> </ul> <p>Example <pre><code>$ run_model -m /.../model.dxnn -i /.../input.bin -l 100\n</code></pre></p>"},{"location":"docs/05_Command_Line_Interface.html#dx-rt>_cli>_tool>_firmware>_interface","title":"DX-RT CLI Tool (Firmware Interface)","text":"<p>This tool provides a command-line interface to interact with DX-RT accelerator devices. It supports querying device status, resetting hardware, updating firmware, and more.  </p> <p>Note. This tool is applicable only for accelerator devices.  </p> <p>Usage <pre><code>dxrt-cli &lt;option&gt; &lt;argument&gt;\n</code></pre></p> <p>Option </p> <ul> <li><code>-s, --status</code>: Get current device status  </li> <li><code>-i, --info</code>: Display basic device information  </li> <li><code>-m, --monitor</code>: Monitoring device status every [arg] seconds (arg &gt; 0)  </li> <li><code>-r, --reset</code>: Reset device (0: NPU only, 1: full device) (default: 0)  </li> <li><code>-d, --device</code>: Specify device ID (default: -1 for all device)  </li> <li><code>-u, --fwupdate</code>: Update firmware with a Deepx firmware file (options: force:, unreset)  </li> <li><code>-w, --fwupload</code>: Update firmware file (2nd_boot or rtos)  </li> <li><code>-g, --fwversion</code>: Check firmware version from a firmware file  </li> <li><code>-p, --dump</code>: Dump initial device state to a file  </li> <li><code>-l, --fwlog</code>: Extract firmware logs to a file  </li> <li><code>-h, --help</code>: Show help message  </li> </ul> <p>Example <pre><code>$ dxrt-cli --status\n\n$ dxrt-cli --reset 0\n\n$ dxrt-cli --fwupdate fw.bin\n\n$ dxrt-cli -m 1\n</code></pre></p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html","title":"C++ Tutorials","text":""},{"location":"docs/06_01_C%2B%2B_Tutorials.html#c>_tutorials","title":"C++ Tutorials","text":""},{"location":"docs/06_01_C%2B%2B_Tutorials.html#run>_synchronous","title":"Run (Synchronous)","text":"<p>The synchronous Run method uses a single NPU core to perform inference in a blocking manner. It can be configured to utilize multiple NPU cores simultaneously by employing threads to run each core independently.</p> <p>  Figure. Synchronous Inference Operation    </p> <p>Inference Engine Run synchronous  </p> <ul> <li>Inference synchronously  </li> <li>Use only one npu core  </li> </ul> <p>The following is the simplest example of synchronous inference.  </p> <p><code>run_sync_model.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\nint main()\n{\n    std::string modelPath = \"model-path\";\n\n    try \n    {\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        // inference loop\n        for(int i = 0; i &lt; 100; ++i)\n        {\n            // inference synchronously\n            // use only one npu core\n            auto outputs = ie.Run(inputPtr.data());\n\n            // post processing\n            postProcessing(outputs);\n\n        } // for i\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine  \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#runasync>_asynchronous","title":"RunAsync (Asynchronous)","text":"<p>The asynchronous Run mode is a method that performs inference asynchronously while utilizing multiple NPU cores simultaneously. It can be implemented to maximize NPU resources through a callback function or a thread wait mechanism.</p> <p>  Figure. Asynchronous Inference Operation    </p> <p>Inference Engine RunAsync, Callback, User Argument  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>The following is an example of asynchronous inference using a callback function. A user argument can be used to synchronize the input with the output of the callback.  </p> <p><code>run_async_model.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\n\nstatic std::atomic&lt;int&gt; gCallbackCnt = {0};\nstatic ConcurrentQueue&lt;int&gt; gResultQueue(1);\nstatic std::mutex gCBMutex;\n\n// invoke this function asynchronously after the inference is completed\nstatic int onInferenceCallbackFunc(dxrt::TensorPtrs &amp;outputs, void *userArg)\n{\n\n    // user data type casting\n    std::pair&lt;int, int&gt;* user_data = reinterpret_cast&lt;std::pair&lt;int, int&gt;*&gt;(userArg);\n\n    // post processing with outputs\n    // ...\n    (void)outputs;\n\n\n    {\n        // Mutex locks should be properly adjusted \n        // to ensure that callback functions are thread-safe.\n        std::lock_guard&lt;std::mutex&gt; lock(gCBMutex);\n\n        gCallbackCnt ++;\n\n        // end of the loop\n        if ( user_data-&gt;second == gCallbackCnt.load() ) // check loop count\n        {\n            gResultQueue.push(gCallbackCnt);\n        }\n    }\n\n    // delete argument object\n    delete user_data;\n\n    return 0;\n}\n\n\nint main(int argc, char* argv[])\n{\n    ...\n\n    int total_callback_count = 0;\n\n    try \n    {\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // register call back function\n        ie.RegisterCallback(onInferenceCallbackFunc);\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        // inference loop\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            // user argument\n            std::pair&lt;int, int&gt; *userData = new std::pair&lt;int, int&gt;(i, loop_count);\n\n            // inference asynchronously, use all npu cores\n            ie.RunAsync(inputPtr.data(), userData);\n\n        }\n\n        // wait until all callbacks have been processed\n        total_callback_count = gResultQueue.pop();\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return (total_callback_count == loop_count ? 0 : -1);\n}\n</code></pre></p> <p>The following is an example where multiple threads start input and inference, and a single callback processes the output.  </p> <p>Inference Engine RunAsync, Callback, User Argument, Thread  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p><code>run_async_model_thread.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\nstatic const int THREAD_COUNT = 3;\nstatic std::atomic&lt;int&gt; gResultCount = {0};\nstatic std::atomic&lt;int&gt; gTotalCount = {0};\nstatic ConcurrentQueue&lt;int&gt; gResultQueue(1);\nstatic std::mutex gCBMutex;\n\nstatic int inferenceThreadFunc(dxrt::InferenceEngine&amp; ie, std::vector&lt;uint8_t&gt;&amp; inputPtr, int threadIndex, int loopCount)\n{\n\n    // inference loop\n    for(int i = 0; i &lt; loopCount; ++i) \n    {\n        // user argument\n        UserData *userData = new UserData();\n\n        // thread index \n        userData-&gt;setThreadIndex(threadIndex);\n\n        // total loop count\n        userData-&gt;setLoopCount(loopCount);\n\n        // loop index\n        userData-&gt;setLoopIndex(i);\n\n        try\n        {\n            // inference asynchronously, use all npu cores\n            // if device-load &gt;= max-load-value, this function will block  \n            ie.RunAsync(inputPtr.data(), userData);\n        }\n        catch(const dxrt::Exception&amp; e)\n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n        catch(const std::exception&amp; e)\n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n\n    } // for i\n\n    return 0;\n\n}\n\n// invoke this function asynchronously after the inference is completed\nstatic int onInferenceCallbackFunc(dxrt::TensorPtrs &amp;outputs, void *userArg)\n{\n\n    // the outputs are guaranteed to be valid only within this callback function\n    // processing this callback functions as quickly as possible is beneficial \n    // for improving inference performance\n\n    // user data type casting\n    UserData *user_data = reinterpret_cast&lt;UserData*&gt;(userArg);\n\n    // thread index\n    int thread_index = user_data-&gt;getThreadIndex();\n\n    // loop index\n    int loop_index = user_data-&gt;getLoopIndex();\n\n    // post processing\n    // transfer outputs to the target thread by thread_index\n    // postProcessing(outputs, thread_index);\n    (void)outputs;\n\n\n    // result count \n    {\n        // Mutex locks should be properly adjusted \n        // to ensure that callback functions are thread-safe.\n        std::lock_guard&lt;std::mutex&gt; lock(gCBMutex);\n\n        gResultCount++;\n        if ( gResultCount.load() == gTotalCount.load() ) gResultQueue.push(0);\n    }\n\n    // delete argument object \n    delete user_data;\n\n    return 0;\n}\n\n\nint main(int argc, char* argv[])\n{\n    ...\n\n    bool result = false;\n\n    try\n    {\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // register call back function\n        ie.RegisterCallback(onInferenceCallbackFunc);    \n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        gTotalCount.store(loop_count * THREAD_COUNT);\n\n        // thread vector \n        std::vector&lt;std::thread&gt; thread_array;\n\n        for(int i = 0; i &lt; THREAD_COUNT; ++i)\n        {\n            // create thread\n            thread_array.push_back(std::thread(inferenceThreadFunc, std::ref(ie), std::ref(inputPtr), i, loop_count));\n        }\n\n        for(auto &amp;t : thread_array)\n        {\n            t.join();\n        } // for t\n\n\n        // wait until all callbacks have been processed\n        gResultQueue.pop();\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return result ? 0 : -1;\n}\n</code></pre></p> <p>The following is an example of performing asynchronous inference by creating an inference wait thread. The main thread starts input and inference, and the inference wait thread retrieves the output data corresponding to the input.  </p> <p>Inference Engine RunAsync, Wait  </p> <ul> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p><code>run_async_model_wait.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\n// concurrent queue is a thread-safe queue data structure \n// designed to be used in a multi-threaded environment\nstatic ConcurrentQueue&lt;int&gt; gJobIdQueue;\n\n// user thread to wait for the completion of inference \nstatic int inferenceThreadFunc(dxrt::InferenceEngine&amp; ie, int loopCount)\n{\n    int count = 0;\n\n    while(...)\n    {\n        // pop item from queue \n        int jobId = gJobIdQueue.pop();\n\n        try \n        {\n            // waiting for the inference to complete by jobId \n            auto outputs = ie.Wait(jobId);\n\n            // post processing \n            postProcessing(outputs);\n\n        }\n        catch(const dxrt::Exception&amp; e)  // exception for inference engine \n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n        catch(const std::exception&amp; e)\n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n\n        // something to do\n\n        count++;\n        if ( count &gt;= loopCount ) break;\n\n    } // while\n\n    return 0;\n}\n\nint main()\n{\n    const int LOOP_COUNT = 100;\n    std::string modelPath = \"model-path\";\n\n    try\n    {\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // do not register call back function\n        // inferenceEngine.RegisterCallback(onInferenceCallbackFunc);\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        // create thread\n        auto t1 = std::thread(inferenceThreadFunc, std::ref(ie), LOOP_COUNT);\n\n        // inference loop\n        for(int i = 0; i &lt; LOOP_COUNT; ++i)\n        {\n\n            // no need user argument\n            // UserData *userData = getUserDataInstanceFromDataPool();\n\n            // inference asynchronously, use all npu cores\n            // if device-load &gt;= max-load-value, this function will block\n            auto jobId = ie.RunAsync(inputPtr.data());\n\n            // push jobId in global queue variable\n            gJobIdQueue.push(jobId);\n\n        } // for i\n\n        t1.join();\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#run>_batch","title":"Run (Batch)","text":"<p>The following is an example of batch inference with multiple inputs and multiple outputs.</p> <p><code>run_batch_model.cpp</code></p> <pre><code>int main(int argc, char* argv[])\n{\n    ...\n\n    try\n    {\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputBuffer(ie.GetInputSize(), 0);\n\n        // input buffer vector\n        std::vector&lt;void*&gt; inputBuffers;\n        for(int i = 0; i &lt; batch_count; ++i)\n        {\n            // assigns the same buffer pointer in this example\n            inputBuffers.emplace_back(inputBuffer.data());\n        }\n\n        // output buffer vector\n        std::vector&lt;void*&gt; output_buffers(batch_count, 0);\n\n        // create user output buffers\n        for(auto&amp; ptr : output_buffers)\n        {\n            ptr = new uint8_t[ie.GetOutputSize()];\n        } // for i\n\n        // batch inference loop\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            // inference asynchronously, use all npu core\n            auto outputPtrs = ie.Run(inputBuffers, output_buffers);\n\n            // postProcessing(outputs);\n            (void)outputPtrs;\n        }\n\n        // Deallocated the user's output buffers\n        for(auto&amp; ptr : output_buffers)\n        {\n            delete[] static_cast&lt;uint8_t*&gt;(ptr);\n        } // for i\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#run>_runasync","title":"Run &amp; RunAsync","text":"<p>The method for converting a synchronous inference approach using one NPU core into an asynchronous inference approach using multiple NPU cores is as follows. It requires the use of callbacks or threads, as well as the implementation of multiple input buffers to support concurrent operations effectively.</p> <p>Converting Run(Sync) to RunAsync </p> <ul> <li>Shift from Single NPU Core to Multiple Cores     : Modify the existing Run(Sync) structure, which utilizes a single NPU core, to RunAsync structure capable of leveraging multiple NPU cores simultaneously.  </li> <li>Create Multiple Input/Output Buffers     : Implement multiple input/output buffers to prevent overwriting. Ensure an appropriate number of buffers are created to support concurrent operations effectively.  </li> <li>Introduce Multi-Buffer Concept     : To handle simultaneous inference processes, integrate a multi-buffer mechanism. This is essential for managing concurrent inputs and outputs without data conflicts.  </li> <li>Asynchronous Inference with Threads or Callbacks     : Adjust the code to ensure that inference inputs and outputs operate asynchronously using threads or callbacks for efficient processing.  </li> <li>Thread-Safe Data Exchange     : For data exchange between threads or callbacks, use a thread-safe queue or structured data mechanisms to avoid race conditions and ensure integrity.  </li> </ul> <p>  Figure. Converting Run(Sync) to RunAsync    </p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#inference>_option","title":"Inference Option","text":"<p>The following inference options allow you to specify an NPU core for performing inference.</p> <p>Inference Engine Run, Inference Option  </p> <ul> <li>Select devices     : default devices is <code>{}</code>     : Choose devices to utilize  </li> <li>Select bound option per device     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_ALL</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_0</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_1</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_2</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_01</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_12</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_02</code> </li> <li>Use onnx runtime library (<code>ORT</code>)     : <code>useORT</code> on or off  </li> </ul> <p><code>run_sync_model_bound.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\nint main()\n{\n    std::string modelPath = \"model-path\";\n\n    try\n    {\n\n        // select bound option NPU_0 to NPU_2 per device  \n        dxrt::InferenceOption op;\n\n        // first device only, default null\n        op.devices.push_back(0); // use device 0 \n        op.devices.push_back(3); // use device 3 \n\n        // use BOUND_OPTION::NPU_0 only\n        op.boundOption = dxrt::InferenceOption::BOUND_OPTION::NPU_0; \n\n        // use ORT\n        op.useORT = false;\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath, op);\n\n        // create temporary input buffer for example \n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        // inference loop\n        for(int i = 0; i &lt; 100; ++i)\n        {\n            // input\n            uint8_t* inputPtr = readInputData();\n\n            // inference synchronously with boundOption\n            // use only one npu core\n            // ownership of the outputs is transferred to the user \n            auto outputs = ie.Run(inputPtr.data());\n\n            // post processing\n            postProcessing(outputs);\n\n        } // for i\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#camera>_inference>_display","title":"Camera / Inference / Display","text":"<p>The following is an example of a pattern that performs inference using two models on a single camera input and combines the results from both models for display.</p> <p>  Figure. Multi-model and Multi-output    </p> <p>Multi-model, Async, Wait Thread <code>(CPU_1 \u2192 {NPU_1 + NPU_2} \u2192 CPU_2</code></p> <p><code>display_async_wait.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\n// input processing main thread with 2 InferenceEngine (asynchronous) \n// display thread \n\nstruct FrameJobId {\n    int jobId_A = -1;\n    int jobId_B = -1;\n    void* frameBuffer = nullptr;\n    int loopIndex = -1;\n};\n\nstatic const int BUFFER_POOL_SIZE = 10;\nstatic const int QUEUE_SIZE = 10;\n\nstatic ConcurrentQueue&lt;FrameJobId&gt; gFrameJobIdQueue(QUEUE_SIZE);\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_A;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_B;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gFrameBufferPool;\n\n// total display count\nstatic std::atomic&lt;int&gt; gTotalDisplayCount = {0};\n\nstatic int displayThreadFunc(int loopCount, dxrt::InferenceEngine&amp; ieA, dxrt::InferenceEngine&amp; ieB)\n{\n\n    while(gTotalDisplayCount.load() &lt; loopCount)\n    {\n        // consumer framebuffer &amp; jobIds\n        auto frameJobId = gFrameJobIdQueue.pop();\n\n        // output data of ieA\n        auto outputA = ieA.Wait(frameJobId.jobId_A);\n\n        // output data of ieB\n        auto outputB = ieB.Wait(frameJobId.jobId_B);\n\n        // post-processing w/ outputA &amp; outputB\n        postProcessing(outputA, outputB);\n\n        gTotalDisplayCount++;\n\n        // display (update framebuffer)\n    }\n\n    return 0;\n}\n\n\nint main(int argc, char* argv[])\n{\n    ...\n\n    try\n    {\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieA(modelPath_A);\n\n        gInputBufferPool_A = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieA.GetInputSize());\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieB(modelPath_B);\n\n        gInputBufferPool_B = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieB.GetInputSize());\n\n        const int W = 512, H = 512, CH = 3;\n        gFrameBufferPool = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, W*H*CH);\n\n        // create thread\n        std::thread displayThread(displayThreadFunc, loop_count, std::ref(ieA), std::ref(ieB));\n\n        // input processing\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            uint8_t* frameBuffer = gFrameBufferPool-&gt;pointer(); \n            readFrameBuffer(frameBuffer, W, H, CH);\n\n            uint8_t* inputA = gInputBufferPool_A-&gt;pointer();\n            preProcessing(inputA, frameBuffer);\n\n            uint8_t* inputB = gInputBufferPool_B-&gt;pointer();\n            preProcessing(inputB, frameBuffer);\n\n            // struct to pass to cpu operation thread \n            FrameJobId frameJobId;\n\n            // start inference of A model\n            frameJobId.jobId_A = ieA.RunAsync(inputA);\n\n            // start inference of B model\n            frameJobId.jobId_B = ieB.RunAsync(inputB);\n\n            // framebuffer used for input data\n            frameJobId.frameBuffer = frameBuffer;\n            frameJobId.loopIndex = i;\n\n            // producer frame &amp; jobId\n            gFrameJobIdQueue.push(frameJobId);\n\n        }\n\n        displayThread.join();\n\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p> <p>The following is an example of a pattern that sequentially performs operations using two models and CPU processing. The inference result from Model A is processed through CPU computation and then used as input data for Model B. Finally, the result from Model B is handled for display.</p> <p>  Figure. CPU and NPU Pipeline Operation    </p> <p>Multi-model, Async, Wait Thread <code>(CPU_1 \u2192 NPU_1 \u2192 CPU_2 \u2192 NPU_2 \u2192 CPU_3)</code></p> <p><code>display_async_pipe.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\n// input main thread \n// 1 cpu processing thread  \n// 1 display thread \n\nstruct FrameJobId {\n    int jobId_A = -1;\n    int jobId_B = -1;\n    uint8_t* inputBufferA;\n    uint8_t* inputBufferB;\n    void* frameBuffer = nullptr;\n\n    int loopIndex;\n};\n\nstatic const int BUFFER_POOL_SIZE = 10;\nstatic const int QUEUE_SIZE = 10;\n\nstatic ConcurrentQueue&lt;FrameJobId&gt; gCPUOPQueue(QUEUE_SIZE);\nstatic ConcurrentQueue&lt;FrameJobId&gt; gDisplayQueue(QUEUE_SIZE);\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_A;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_B;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gFrameBufferPool;\n\n// total display count\nstatic std::atomic&lt;int&gt; gTotalDisplayCount = {0};\n\n\nstatic int displayThreadFunc(int loopCount, dxrt::InferenceEngine&amp; ieB)\n{\n\n    while(gTotalDisplayCount.load() &lt; loopCount)\n    {\n        // consumer framebuffer &amp; jobIds\n        auto frameJobId = gDisplayQueue.pop();\n\n        // output data of ieB\n        auto outputB = ieB.Wait(frameJobId.jobId_B);\n\n        // post-processing w/ outputA &amp; outputB\n        postProcessingB(outputB);\n\n        gTotalDisplayCount++;\n\n        // display (update framebuffer)\n        if ( frameJobId.loopIndex == (loopCount - 1)) break;\n    }\n\n    return 0;\n}\n\nstatic int cpuOperationThreadFunc(int loopCount, dxrt::InferenceEngine&amp; ieA, dxrt::InferenceEngine&amp; ieB)\n{\n\n    while(gTotalDisplayCount.load() &lt; loopCount)\n    {\n        // consumer framebuffer &amp; jobIds\n        auto frameJobIdA = gCPUOPQueue.pop();\n\n        // output data of ieA\n        auto outputA = ieA.Wait(frameJobIdA.jobId_A);\n\n        // post-processing w/ outputA\n        postProcessingA(frameJobIdA.inputBufferB, outputA);\n\n        FrameJobId frameJobIdB;\n        frameJobIdB.loopIndex = frameJobIdA.loopIndex;\n        frameJobIdB.jobId_B = ieB.RunAsync(frameJobIdA.inputBufferB);\n\n        gDisplayQueue.push(frameJobIdB);\n\n        // display (update framebuffer)\n        if ( frameJobIdA.loopIndex == (loopCount - 1)) break;\n    }\n\n    return 0;\n}\n\n\nint main(int argc, char* argv[])\n{\n\n    ...\n\n    try\n    {\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieA(modelPath);\n\n        gInputBufferPool_A = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieA.GetInputSize());\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieB(modelPath);\n\n        gInputBufferPool_B = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieB.GetInputSize());\n\n        const int W = 512, H = 512, CH = 3;\n        gFrameBufferPool = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, W*H*CH);\n\n        // create thread\n        std::thread cpuOperationThread(cpuOperationThreadFunc, loop_count, std::ref(ieA), std::ref(ieB));\n        std::thread displayThread(displayThreadFunc, loop_count, std::ref(ieB));\n\n\n        // input processing\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            uint8_t* frameBuffer = gFrameBufferPool-&gt;pointer(); \n            readFrameBuffer(frameBuffer, W, H, CH);\n\n            uint8_t* inputA = gInputBufferPool_A-&gt;pointer();\n            preProcessing(inputA, frameBuffer);\n\n            // struct to pass to a thread \n            FrameJobId frameJobId;\n\n            frameJobId.inputBufferA = inputA;\n            frameJobId.inputBufferB = gInputBufferPool_B-&gt;pointer();\n\n            // start inference of A model\n            frameJobId.jobId_A = ieA.RunAsync(inputA);\n\n            // framebuffer used for input data\n            frameJobId.frameBuffer = frameBuffer;\n            frameJobId.loopIndex = i;\n\n            // producer frame &amp; jobId\n            gCPUOPQueue.push(frameJobId);\n\n        }\n\n        cpuOperationThread.join();\n        displayThread.join();\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#exception","title":"Exception","text":"<p>The error codes and types of exceptions for error handling are as follows.</p> <pre><code>enum ERROR_CODE {\n        DEFAULT = 0x0100,\n        FILE_NOT_FOUND,\n        NULL_POINTER,\n        FILE_IO,\n        INVALID_ARGUMENT,\n        INVALID_OPERATION,\n        INVALID_MODEL,\n        MODEL_PARSING,\n        SERVICE_IO,\n        DEVICE_IO\n    };\n</code></pre> <ul> <li>FileNotFoundException  </li> <li>NullPointerException  </li> <li>FileIOException  </li> <li>InvalidArgumentException  </li> <li>InvalidOperationException  </li> <li>InvalidModelException  </li> <li>ModelParsingException  </li> <li>ServiceIOException  </li> <li>DeviceIOException  </li> </ul> <pre><code>    // try/catch prototype\n\n    try\n    {\n        // DX-RT APIs ...\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1; // or std::exit(-1);\n    }\n    catch(std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;  // or std::exit(-1);\n    }\n</code></pre>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#examples","title":"Examples","text":"<p>The examples provided earlier are actual code samples that can be executed. Please refer to them for practical use.  </p> <ul> <li><code>display_async_pipe</code>     : An example using <code>[CPU_1 \u2192 {NPU_1 + NPU_2} \u2192 CPU_2]</code> pattern  </li> <li><code>display_async_wait</code>     : An example using <code>[CPU_1 \u2192 NPU_1 \u2192 CPU_2 \u2192 NPU_2 \u2192 CPU_3]</code> pattern  </li> <li><code>display_async_thread</code>     : An example using single model and multi threads  </li> <li><code>display_async_models_1</code>     : An example using multi models and multi threads (Inference Engine is created within each thread)  </li> <li><code>display_async_models_2</code>     : An example using multi models and multi threads (Inference Engine is created in the main thread)  </li> <li><code>run_async_model</code>     : A performance-optimized example using a callback function  </li> <li><code>run_async_model_thread</code>     : An example using a single inference engine, callback function, and thread     : Usage method when there is a single AI model and multiple inputs  </li> <li><code>run_async_model_wait</code>     : An example using threads and waits  </li> <li><code>run_sync_model</code>     : An example using a single thread  </li> <li><code>run_sync_model_thread</code>     : An example running an inference engine on multiple threads  </li> <li><code>run_sync_model_bound</code>     : An example of specifying an NPU using the bound option  </li> </ul>"},{"location":"docs/06_02_Python_Tutorials.html","title":"Python Tutorials","text":""},{"location":"docs/06_02_Python_Tutorials.html#run>_synchronous","title":"Run (Synchronous)","text":"<p>The synchronous Run method uses a single NPU core to perform inference in a blocking manner. It can be configured to utilize multiple NPU cores simultaneously by employing threads to run each core independently. (Refer to Figure in Section 5.2. Inference Workflow)  </p> <p>Inference Engine Run (Python) </p> <p><code>run_sync_model.py</code></p> <pre><code># DX-RT importes\nfrom dx_engine import InferenceEngine\n...\n\n\nif __name__ == \"__main__\":\n    ...    \n\n    # create inference engine instance with model\n    ie = InferenceEngine(modelPath)\n\n    input = [np.zeros(ie.GetInputSize(), dtype=np.uint8)]\n\n    # inference loop\n    for i in range(loop_count):\n\n        # inference synchronously \n        # use only one npu core \n        outputs = ie.Run(input)\n\n        # post processing \n        postProcessing(outputs)\n\n    exit(0)\n</code></pre>"},{"location":"docs/06_02_Python_Tutorials.html#runasync>_asynchronous","title":"RunAsync (Asynchronous)","text":"<p>The asynchronous Run mode is a method that performs inference asynchronously while utilizing multiple NPU cores simultaneously. It can be implemented to maximize NPU resources through a callback function or a thread wait mechanism. </p> <p>Inference Engine RunAsync, Callback, User Argument  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>The following is an example of asynchronous inference using a callback function. A user argument can be used to synchronize the input with the output of the callback.  </p> <p>Inference Engine RunAsync, Callback, User Argument (Python)</p> <p><code>run_async_model.py</code></p> <pre><code>from dx_engine import InferenceEngine\n...\n\nq = queue.Queue()\ngLoopCount = 0\n\nlock = threading.Lock()\n\ndef onInferenceCallbackFunc(outputs, user_arg):\n\n    # the outputs are guaranteed to be valid only within this callback function\n    # processing this callback functions as quickly as possible is beneficial \n    # for improving inference performance\n\n    global gLoopCount\n\n    # Mutex locks should be properly adjusted \n    # to ensure that callback functions are thread-safe.\n    with lock:\n        # user data type casting\n        index, loop_count = user_arg\n\n\n        # post processing\n        #postProcessing(outputs);\n\n        # something to do\n\n        print(\"Inference output (callback) index=\", index)\n\n        gLoopCount += 1\n        if ( gLoopCount == loop_count ) :\n            print(\"Complete Callback\")\n            q.put(0)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n\n    ...    \n\n    # create inference engine instance with model\n    ie = InferenceEngine(modelPath)\n\n    # register call back function\n    ie.register_callback(onInferenceCallbackFunc)\n\n\n    input = [np.zeros(ie.GetInputSize(), dtype=np.uint8)]\n\n    # inference loop\n    for i in range(loop_count):\n\n        # inference asynchronously, use all npu cores\n        # if device-load &gt;= max-load-value, this function will block  \n        ie.RunAsync(input, user_arg=[i, loop_count])\n\n        print(\"Inference start (async)\", i)\n\n    exit(q.get())\n</code></pre> <p>The following is an example where multiple threads start input and inference, and a single callback processes the output.</p> <p>Inference Engine RunAsync, Callback, User Argument, Thread  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>Inference Engine RunAsync, Callback, User Argument, Thread (Python)</p> <p><code>run_async_model_thread.py</code> <pre><code>from dx_engine import InferenceEngine\n...\n\nTHRAD_COUNT = 3\ntotal_count = 0\nq = queue.Queue()\n\nlock = threading.Lock()\n\n\ndef inferenceThreadFunc(ie, threadIndex, loopCount):\n\n    # input\n    input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]\n\n    # inference loop\n    for i in range(loopCount):\n\n        # inference asynchronously, use all npu cores\n        # if device-load &gt;= max-load-value, this function will block  \n        ie.RunAsync(input,user_arg = [i, loopCount, threadIndex])\n\n    return 0\n\ndef onInferenceCallbackFunc(outputs, user_arg):\n    # the outputs are guaranteed to be valid only within this callback function\n    # processing this callback functions as quickly as possible is beneficial \n    # for improving inference performance\n\n    global total_count\n\n    # Mutex locks should be properly adjusted \n    # to ensure that callback functions are thread-safe.\n    with lock:\n        # user data type casting\n        index = user_arg[0]\n        loop_count = user_arg[1]\n        thread_index = user_arg[2]\n\n        # post processing\n        #postProcessing(outputs);\n\n        # something to do\n\n        total_count += 1\n\n        if ( total_count ==  loop_count * THRAD_COUNT) :\n            q.put(0)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    ...    \n\n    # create inference engine instance with model\n    ie = InferenceEngine(modelPath)\n\n    # register call back function\n    ie.register_callback(onInferenceCallbackFunc)\n\n\n    t1 = threading.Thread(target=inferenceThreadFunc, args=(ie, 0, loop_count))\n    t2 = threading.Thread(target=inferenceThreadFunc, args=(ie, 1, loop_count))\n    t3 = threading.Thread(target=inferenceThreadFunc, args=(ie, 2, loop_count))\n\n    # Start and join\n    t1.start()\n    t2.start()\n    t3.start()\n\n\n    # join\n    t1.join()\n    t2.join()\n    t3.join()\n\n\n    exit(q.get())\n</code></pre></p> <p>The following is an example of performing asynchronous inference by creating an inference wait thread. The main thread starts input and inference, and the inference wait thread retrieves the output data corresponding to the input.</p> <p>Inference Engine RunAsync, Wait  </p> <ul> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>Inference Engine RunAsync, Wait (Python)</p> <p><code>run_async_model_wait.py</code> <pre><code># DX-RT imports\nfrom dx_engine import InferenceEngine\n...\n\nq = queue.Queue()\n\n\ndef inferenceThreadFunc(ie, loopCount):\n\n    count = 0\n\n    while(True):\n\n        # pop item from queue \n        jobId = q.get()\n\n        # waiting for the inference to complete by jobId\n        # ownership of the outputs is transferred to the user \n        outputs = ie.Wait(jobId)\n\n        # post processing\n        # postProcessing(outputs);\n\n        # something to do\n\n        count += 1\n        if ( count &gt;= loopCount ):\n            break\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    ...\n\n    # create inference engine instance with model\n    with InferenceEngine(modelPath) as ie:\n\n        # do not register call back function\n        # ie.register_callback(onInferenceCallbackFunc)\n\n        t1 = threading.Thread(target=inferenceThreadFunc, args=(ie, loop_count))\n\n        t1.start()\n\n        input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]\n\n        # inference loop\n        for i in range(loop_count):\n\n            # inference asynchronously, use all npu cores\n            # if device-load &gt;= max-load-value, this function will block  \n            jobId = ie.run_async(input, user_arg=0)\n\n            q.put(jobId)\n\n        t1.join()\n\n    exit(0)\n</code></pre></p>"},{"location":"docs/06_02_Python_Tutorials.html#run>_batch","title":"Run (Batch)","text":"<p>The following is an example of batch inference with multiple inputs and multiple outputs.</p> <p><code>run_batch_model.py</code></p> <pre><code>import numpy as np\nimport sys\nfrom dx_engine import InferenceEngine\nfrom dx_engine import InferenceOption\n\n\nif __name__ == \"__main__\":\n    ...\n\n    # create inference engine instance with model\n    with InferenceEngine(modelPath) as ie:\n\n        input_buffers = []\n        output_buffers = []\n        index = 0\n        for b in range(batch_count):\n            input_buffers.append([np.array([np.random.randint(0, 255)],  dtype=np.uint8)])\n            output_buffers.append([np.zeros(ie.get_output_size(), dtype=np.uint8)])\n            index = index + 1\n\n        # inference loop\n        for i in range(loop_count):\n\n            # batch inference\n            # It operates asynchronously internally \n            # for the specified number of batches and returns the results\n            results = ie.run_batch(input_buffers, output_buffers)\n\n            # post processing \n\n    exit(0)\n</code></pre>"},{"location":"docs/06_02_Python_Tutorials.html#inference>_option","title":"Inference Option","text":"<p>The following inference options allow you to specify an NPU core for performing inference.</p> <p>Inference Engine Run, Inference Option  </p> <ul> <li> select devices default device is <code>[]</code> Choose the device to utilize  (ex. <code>[0, 2]</code>)   </li> <li> select bound option per device <code>InferenceOption.BOUND_OPTION.NPU_ALL</code> <code>InferenceOption.BOUND_OPTION.NPU_0</code> <code>InferenceOption.BOUND_OPTION.NPU_1</code> <code>InferenceOption.BOUND_OPTION.NPU_2</code> </li> <li> use onnx runtime library (<code>ORT</code>) <code>set_use_ort / get_use_ort</code> </li> </ul> <p>NPU_ALL / NPU_0 / NPU_1 / NPU_2 <pre><code># DX-RT imports\nfrom dx_engine import InferenceEngine, InferenceOption\n...\n\nif __name__ == \"__main__\":\n    ...\n\n    # inference option\n    option = InferenceOption()\n\n    print(\"Inference Options:\")\n\n    # select devices\n    option.devices = [0]\n\n    # NPU bound opion (NPU_ALL or NPU_0 or NPU_1 or NPU_2)\n    option.bound_option = InferenceOption.BOUND_OPTION.NPU_ALL\n\n    # use ONNX Runtime (True or False)\n    option.use_ort = False\n\n    # create inference engine instance with model\n    with InferenceEngine(modelPath, option) as ie:\n\n        input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]\n\n        # inference loop\n        for i in range(loop_count):\n\n            # inference synchronously \n            # use only one npu core \n            # ownership of the outputs is transferred to the user \n            outputs = ie.run(input)\n\n            # post processing \n            #postProcessing(outputs)\n            print(\"Inference outputs \", i)\n\n    exit(0)\n</code></pre></p>"},{"location":"docs/06_02_Python_Tutorials.html#examples","title":"Examples","text":"<p>The examples provided earlier are actual code samples that can be executed. Please refer to them for practical use. (<code>examples/python</code>)  </p> <ul> <li> <code>run_async_model.py</code> A performance-optimized example using a callback function   </li> <li> <code>run_async_model_thread.py</code> An example using a single inference engine, callback function, and thread   Usage method when there is a single AI model and multiple inputs   </li> <li> <code>run_async_model_wait.py</code> An example using threads and waits   </li> <li> <code>run_sync_model.py</code> An example using a single thread   </li> <li> <code>run_sync_model_thread.py</code> An example running an inference engine on multiple threads   </li> <li> <code>run_sync_model_bound.py</code> An example of specifying an NPU using the bound option   </li> </ul>"},{"location":"docs/07_01_C%2B%2B_API_Reference.html","title":"C++ API Reference","text":""},{"location":"docs/07_01_C%2B%2B_API_Reference.html#c>_api>_reference","title":"C++ API Reference","text":""},{"location":"docs/07_01_C%2B%2B_API_Reference.html#inference>_engine","title":"Inference Engine","text":"<pre><code>class InferenceEngine\nThis class provides an abstraction for the runtime inference executor of the user's compiled model. Once the user loads a compiled model into the InferenceEngine, real-time device tasks are managed and scheduled by internal runtime libraries. It supports both synchronous and asynchronous inference modes, depending on the user's request.\n\n\nInferenceEngine(const std::string&amp; modelPath, InferenceOption&amp; option = DefaultInferenceOption)\n   Constructs an InferenceEngine instance.\n   parameters:\n      [in] modelPath    : Path to the model file.\n      [in] option   : Inference options, including device and NPU core settings. \n                        (default: DefaultInferenceOption).\n\n\nstd::vector&lt;TensorPtrs&gt; GetAllTaskOutputs()\n   Retrieves the output tensors for all tasks.\n   returns:\n      A vector containing the output tensors of all tasks.\n\n\nstd::vector&lt;TensorPtrs&gt; get_outputs()\n   Retrieves the output tensors for all tasks.\n   returns:\n      A vector containing the output tensors of all tasks.\n\n\nstd::string GetCompileType()\n   Retrieves the compile type of the model.\n   returns:\n      The compile type of the model as a std::string.\n\n\nstd::string get_compile_type()\n   [[deprecated(\"Use GetCompileType() instead\")]]\n   Retrieves the compile type of the model.\n   returns:\n      The compile type of the model as a std::string.\n\n\nTensors GetInputs(void* ptr = nullptr, uint64_t phyAddr = 0)\n   Retrieves the input tensor.\n   This function provides access to the input tensor based on the provided \n   virtual and physical address pointers.\n   parameters:\n      [in] ptr      : pointer to virtual the address (optional)\n      [in] phyAddr  : pointer to physical the address (optional)\n   returns:\n     If `ptr` is `nullptr`, returns the input memory area in the engine.\n     If both `ptr` and `phyAddr` are provided, returns input tensors containing \n     output addresses.\n\n\nTensors inputs(void* ptr = nullptr, uint64_t phyAddr = 0)\n   [[deprecated(\"Use GetInputs() instead\")]]\n   Retrieves the input tensor.\n   This function provides access to the input tensor based on the provided \n   virtual and physical address pointers.\n   parameters:\n      [in] ptr      : pointer to virtual the address (optional)\n      [in] phyAddr  : pointer to physical the address (optional)\n   returns:\n     If `ptr` is `nullptr`, returns the input memory area in the engine.\n     If both `ptr` and `phyAddr` are provided, returns input tensors containing \n     output addresses.\n\n\nstd::vector&lt;Tensors&gt; GetInputs(int devId)\n   Retrieves the input tensors.\n   parameters:\n      [in] devId        : Device Id.\n   returns:\n      A vector of the input tensors.\n\n\nstd::vector&lt;Tensors&gt; inputs(int devId)\n   [[deprecated(\"Use GetInputs() instead\")]]\n   Retrieves the input tensors.\n   parameters:\n      [in] devId        : Device Id.\n   returns:\n      A vector of the input tensors.\n\n\nuint64_t GetInputSize()\n   Retrieves the total size of input tensors.\n   This function returns the total memory size required for the input tensors \n   of a single inference operation.\n   returns:\n      Input size in bytes for one inference.\n\n\nuint64_t input_size()\n   [[deprecated(\"Use GetInputSize() instead\")]]\n   Retrieves the total size of input tensors.\n   This function returns the total memory size required for the input tensors \n   of a single inference operation.\n   returns:\n      Input size in bytes for one inference.\n\n\nint GetLatency()\n   Retrieves the most recent inference latency.\n   This function returns the latency time taken for the most recent inference operation.\n   returns:\n      Latency in microseconds\n\n\nint latency()\n   [deprecated(\"Use GetLatency() instead\")]]\n   Retrieves the most recent inference latency.\n   This function returns the latency time taken for the most recent inference operation.\n   returns:\n      Latency in microseconds\n\n\nint GetLatencyCnt()\n   Retrieves the number of latency measurements.  \n   This function returns the total count of latency measurements. \n   returns:\n      The count of latency measurements.\n\n\nstd::vector&lt;int&gt; GetLatencyVector()\n   Retrieves a vector of inference latency values.\n   returns:\n      A vector of inference latency values in microseconds.\n\n\ndouble GetLatencyMean()\n   Retrieves the average inference latency.  \n   This function returns the mean latency of inference execution.\n   returns:\n      The average inference latency in microseconds.\n\n\ndouble GetLatencyStdDev()\n   Retrieves the standard deviation of inference latency.  \n   This function calculates the variation in inference latency time.\n   returns:\n      The standard deviation of inference latency in microseconds.\n\n\nstd::string GetModelName()\n   Retrieves the name of the model.\n   This function returns the model's name.\n   returns:\n      The name of the model as a string.\n\n\nstd::string name()\n   [[deprecated(\"Use GetModelName() instead\")]]\n   Retrieves the name of the model.\n   This function returns the model's name.\n   returns:\n      The name of the model as a string.\n\n\nuint32_t GetNpuInferenceTime()\n   Retrieves the most recent inference time.\n   This function returns the duration of the most recent inference operation.\n   returns:\n      Inference time in microseconds.\n\n\nuint32_t inference_time()\n   [[deprecated(\"Use GetNpuInferenceTime() instead\")]]\n   Retrieves the most recent inference time.\n   This function returns the duration of the most recent inference operation.\n   returns:\n      Inference time in microseconds.\n\n\nint GetNpuInferenceTimeCnt()\n   Retrieves the number of NPU inference time measurements.  \n   This function returns the total count of NPU inference time measurements.\n   returns:\n      The count of NPU inference time measurements.\n\n\nstd::vector&lt;uint32_t&gt; GetNpuInferenceTimeVector()\n   Retrieves a vector of NPU inference time values.\n   returns:\n      A vector of inference time values in microseconds.\n\n\ndouble GetNpuInferenceTimeMean()\n   Retrieves the average NPU inference time.  \n   This function returns the mean execution time for NPU inference. \n   returns:\n      The average NPU inference time in microseconds.\n\n\ndouble GetNpuInferenceTimeStdDev()\n   Retrieves the standard deviation of NPU inference time.  \n   This function returns the standard deviation of the inference time.\n   returns:\n      The standard deviation of inference time in microseconds.\n\n\nint GetNumTailTasks()\n   Retrieves the number of tail tasks in the model.\n   Tail tasks are tasks that do not have any subsequent tasks in the model's task chain.\n   This function returns the count of such tail tasks. \n   returns:\n      The number of tail tasks.\n\n\nint get_num_tails()\n   [[deprecated(\"Use GetNumTailTasks() instead\")]]\n   Retrieves the number of tail tasks in the model.\n   Tail tasks are tasks that do not have any subsequent tasks in the model's task chain.\n   This function returns the count of such tail tasks. \n   returns:\n      The number of tail tasks.\n\n\nTensors GetOutputs(void *ptr=nullptr, uint64_t phyAddr=0)\n   Retrieves the output tensor.\n   parameters:\n      [in] ptr      : Pointer to the virtual address.\n      [in] phyAddr  : Pointer to the physical address.\n   returns:\n      If `ptr` is `nullptr`, the output memory area in the engine is returned\n      If both `ptr` and `phyAddr` are provided, returns output tensors containing \n      the output addresses.\n\n\nTensors outputs(void *ptr=nullptr, uint64_t phyAddr=0)\n   [[deprecated(\"Use GetOutputs() instead\")]]\n   Retrieves the output tensor.\n   parameters:\n      [in] ptr      : Pointer to the virtual address.\n      [in] phyAddr  : Pointer to the physical address.\n   returns:\n      If ptr is null, the output memory area in the engine is returned\n      If both ptr and phyAddr are provided, returns output tensors containing \n      the output addresses.\n\n\nuint64_t GetOutputSize()\n   Retrieves the total size of output tensors.\n   This function returns the total memory size required for the output tensors \n   of a single inference operation.\n   returns:\n      Output size in bytes for one inference.\n\n\nuint64_t output_size()\n   [[deprecated(\"Use GetOutputSize() instead\")]]\n   Retrieves the total size of output tensors.\n   This function returns the total memory size required for the output tensors \n   of a single inference operation.\n   returns:\n      Output size in bytes for one inference.\n\n\nstd::vector&lt;string&gt; GetTaskOrder()\n   Retrieves the execution order of tasks in the model.\n   This function returns the sequence of tasks as they are executed within the model.\n   returns:\n      A vector of strings representing the task execution order.\n\n\nstd::vector&lt;string&gt; task_order()\n   [[deprecated(\"Use GetTaskOrder() instead\")]]\n   Retrieves the execution order of tasks in the model.\n   This function returns the sequence of tasks as they are executed within the model.\n   returns:\n      A vector of strings representing the task execution order.\n\n\nbool IsPPU()\n   Checks whether PPU is utilized.\n   returns:\n     true if PPU is being utilized, otherwise false.\n\n\nbool is_PPU()\n   [[deprecated(\"Use IsPPU() instead\")]]\n   Checks whether PPU is utilized.\n   returns:\n     true if PPU is being utilized, otherwise false.\n\n\nvoid RegisterCallback(std::function&lt;int(TensorPtrs &amp;outputs, void *userArg)&gt;    callbackFunc)   \n   Register user callback function to be called by inference completion.\n   parameters:\n      [in] callbackFunc : A function that is called when inference is complete.  \n                          It receives two arguments: outputs and userArg.\n                          outputs : output tensors data\n                          userArg : userArg given by RunAsync();\n\n\n\nvoid RegisterCallBack(std::function&lt;int(TensorPtrs &amp;outputs, void *userArg)&gt;    callbackFunc)   \n   [[deprecated(\"Use RegisterCallback() instead\")]]\n   Register user callback function to be called by inference completion.\n   parameters:\n      [in] callbackFunc : A function that is called when inference is complete.  \n                          It receives two arguments: outputs and userArg.\n                          outputs : output tensors data\n                          userArg : userArg given by RunAsync();\n\n\nTensorPtrs Run(void* inputPtr, void * userArg = nullptr, void* outputPtr = nullptr)\n   Runs the inference engine synchronously using the specified input pointer.\n   This function executes inference with the given input data and returns the output tensors.\n   parameters:\n      [in] inputPtr : Pointer to the input data for inference.\n      [in] userArg  : User-defined arguments as a void pointer (optional).\n                        (e.g. original frame data, metadata about input, etc.)\n      [out] outputPtr   : Pointer to store the output data. If `nullptr`, the output data is \n                         stored in an internal buffer.\n   returns:\n      Output tensors as a vector of smart pointer instances.\n\n\nint RunAsync(void* inputPtr, void * userArg = nullptr, void* outputPtr = nullptr)\n   Initiates an asynchronous inference request and returns a job ID.\n   This function performs a non-blocking inference operation using the specified input\n   pointer. It returns a job ID that can be used with the `wait()` function to retrieve \n   the results.\n   parameters:\n      [in] inputPtr : Pointer to the input data for inference.\n      [in] userArg  : User-defined arguments as a void pointer (optional).\n                        (e.g. original frame data, metadata about input, etc.)\n      [out] outputPtr   : Pointer to store the output data. If `nullptr`, the output data is \n                         stored in an internal buffer.\n   returns:\n      Job ID that can be used with the `wait()` function to retrieve the inference result.\n\n\nfloat RunBenchmark(int num, void* inputPtr = nullptr)\n   Runs a benchmark test by executing inference multiple times.\n   This function performs inference in a loop for the specified number of times \n   and calculates the average frames per second (FPS).\n   parameters:\n      [in] num      : Number of inference iterations.\n      [in] inputPtr : Pointer to the input data for inference (optional). \n                         If `nullptr`, default input data is used.\n   returns:\n      Average FPS (frames per second) over the benchmark runs.\n\n\nfloat RunBenchMark(int num, void* inputPtr = nullptr)\n   [[deprecated(\"Use RunBenchmark() instead\")]]\n   Runs a benchmark test by executing inference multiple times.\n   This function performs inference in a loop for the specified number of times \n   and calculates the average frames per second (FPS).\n   parameters:\n      [in] num      : Number of inference iterations.\n      [in] inputPtr : Pointer to the input data for inference (optional). \n                         If `nullptr`, default input data is used.\n   returns:\n      Average FPS (frames per second) over the benchmark runs.\n\n\nTensorPtrs Wait(int jobId)\n   Waits for the completion of an asynchronous inference request.\n   This function blocks execution until the specified inference job, identified by `jobId`, \n   is complete and then returns the output tensors.   \n   parameters:\n      [in] jobId    : Job ID returned by `RunAsync()`, used to track the inference request.\n   returns:\n      Output tensors as a vector of smart pointer instances.\n</code></pre>"},{"location":"docs/07_01_C%2B%2B_API_Reference.html#inference>_option","title":"Inference Option","text":"<pre><code>class InferenceOption\nThis struct specifies inference options applied to dxrt::InferenceEngine.\nUser can configure which npu device is used to inference.\n\nenum BOUND_OPTION {\n        NPU_ALL = 0,\n        NPU_0,\n        NPU_1, \n        NPU_2,\n        NPU_01,\n        NPU_12,\n        NPU_02\n    };\n\nuint32_t boundOption = BOUND_OPTION::NPU_ALL\n   variables:\n   Select the NPU core inside the device.\n   NPU_ALL is an option that uses all NPU cores simultaneously. NPU_0, NPU_1, NPU_2, NPU_01, NPU_12 and NPU 02 are   \n   options that allow using only a single NPU core.  \n\n\nstd::vector&lt; int&gt; devices = {}\n   variables:\n   device ID list to use\n   make a list which contains a list of device ID to use. if it is empty\n   (or use default value), \n   then all devices are used. list of device ID to use (it is empty by default, then all \n   devices are used.)\n</code></pre>"},{"location":"docs/07_01_C%2B%2B_API_Reference.html#profiler","title":"Profiler","text":"<pre><code>class Profiler\nProvides a time measurement API based on timestamps.\nThe `Profiler` class is used to measure execution time using timestamps,\nenabling performance analysis of various operations.\n\nvoid Add(const std::string&amp; event)\n   Registers an event in the profiler.\n   This function records an event with the specified name. If the profiler is used \n   in a multi-threaded environment, this function should be called first to ensure \n   proper event tracking.\n   parameters:\n      [in] event    : Name of the event to be registered.\n\n\nvoid AddTimePoint(const std::string&amp; event, TimePointPtr tp)\n   Adds a timing data point for the specified event.\n   This function records a time point associated with a given event name, \n   allowing precise measurement of execution timing.\n   parameters:\n      [in] event    : Name of the event to associate with the time point.\n      [in] tp       : Pointer to the timing data.\n\n\nvoid End(const std::string&amp; event)\n   Records the end point of a specified event.\n   This function marks the completion of an event, allowing for \n   measurement of the event's duration when used with corresponding start points.\n   parameters:\n      [in] event    : Name of the event to mark as completed.\n\n\nvoid Erase(const std::string&amp; event)\n   Clears the timing data of a specified event.\n   parameters:\n      [in] event    : Name of the event whose timing data should be cleared.\n\n\nuint64_t Get(const std::string&amp; event)\n   Retrieves the most recent elapsed time of a specified event.\n   This function returns the elapsed time (in microseconds) for the given event, \n   based on the most recent recorded start and end points.\n   parameters:\n      [in] event    : Name of the event for which the elapsed time is requested.\n   returns:\n      Elapsed time in microseconds.\n\n\ndouble GetAverage(const std::string&amp; event)\n   Retrieves the average elapsed time of a specified event.\n   This function returns the average elapsed time (in microseconds) for the given event, \n   calculated from all recorded start and end points.\n   parameters:\n      [in] event    : Name of the event for which the average elapsed time is requested.\n   returns:\n      Average elapsed time in microseconds.\n\n\nstatic Profiler&amp; GetInstance()\n   Retrieves the singleton instance of the Profiler.\n   This function returns a reference to the pre-created singleton instance of \n   the `Profiler`. Users should not create their own instance.\n   returns:\n      A reference to the singleton instance of `dxrt::Profiler`.\n\n\nvoid Save(const std::string&amp; file)\n   Saves the timing data of all events to a specified file.\n   This function exports all recorded timing data for each event and saves it to the \n   given file. The data can be used for further analysis or reporting.\n   parameters:\n      [in] file     : Name of the file where the timing data will be saved.\n\n\nvoid Show(bool  showDurations = false)\n   Displays the elapsed times for all events.\n   This function prints the elapsed times for each event. If `showDurations` is set to true, \n   the durations of each event will also be shown.\n   parameters:\n      [in] showDurations : If true, displays the durations of each event.\n                           If false, only the elapsed times are shown.\n\n\nvoid Start(const std::string&amp; event)\n   Records the start point of a specified event.\n   This function marks the beginning of an event, allowing for the calculation of \n   its duration when paired with an end point.\n   parameters:\n      [in] event    : Name of the event to mark as started.\n</code></pre>"},{"location":"docs/07_01_C%2B%2B_API_Reference.html#tensor","title":"Tensor","text":"<pre><code>class Tensor\nRepresents a DX-RT tensor object, which defines a data array composed of uniform elements.\nThe `Tensor` class abstracts a tensor object that holds a multi-dimensional array of data \nelements, typically used in machine learning models. This tensor is generally connected to \ninference engine objects for computations.\n\nvoid* data(int  height, int width, int channel)\n   Retrieves a pointer to a specific element in the tensor by its indices.\n   This function returns the address of the element at the specified indices \n   (height, width, channel) for a tensor in NHWC (height, width, channel) data format.\n   parameters:\n      [in] height   : The height index of the desired element.\n      [in] width    : The width index of the desired element.\n      [in] channel  : The channel index of the desired element.\n   returns:\n      A pointer to the specified tensor element at the given indices (height, width, channel).\n</code></pre>"},{"location":"docs/07_02_Python_API_Reference.html","title":"Python API Reference","text":""},{"location":"docs/07_02_Python_API_Reference.html#python>_api>_reference","title":"Python API Reference","text":""},{"location":"docs/07_02_Python_API_Reference.html#inference>_engine","title":"Inference Engine","text":"<pre><code>class InferenceEngine\nThis class provides an abstraction for the runtime inference executor of the user's compiled model. Once the user loads a compiled model into the InferenceEngine, real-time device tasks are managed and scheduled by internal runtime libraries. It supports both synchronous and asynchronous inference modes, depending on the user's request.\n\nInferenceEngine(model_path: str, inference_option=None)\n   Constructs an InferenceEngine instance.\n   parameters:\n      [in] model_path       : Path to the model file.\n      [in] inference_option     : Inference options, including device and NPU core settings. \n                                (default: DefaultInferenceOption).\n\n\nget_all_task_outputs()\n   Retrieves the outputs from all tasks in sequence.\n   returns:\n      The outputs from all tasks in order.\n\n\nget_outputs()\n   @deprecated: Use `get_all_task_outputs()` instead.\n   Retrieves the outputs from all tasks in sequence.\n   returns:\n       The outputs from all tasks in order.\n\n\nget_compile_type()\n   Retrieves the compile type of the model.\n   returns:\n      The compile type of the model.\n\n\nget_input_data_type()\n   Retrieves the required output data types as a list of strings.\n   returns:\n      A list of strings representing the input data types.\n\n\ninput_dtype()\n   @deprecated: Use `get_input_data_type()` instead.\n   Retrieves the required output data types as a list of strings.\n   returns:\n      A list of strings representing the input data types.\n\n\nget_input_size()\n   Retrieves the total size of input tensors.\n   This function returns the total memory size required for the input tensors \n   of a single inference operation.\n   returns:\n      Input size in bytes for one inference.\n\n\ninput_size()\n   @deprecated: Use `get_input_size()` instead.\n   Retrieves the total size of input tensors.\n   This function returns the total memory size required for the input tensors \n   of a single inference operation.\n   returns:\n      Input size in bytes for one inference.\n\n\nget_latency()\n   Retrieves the most recent inference latency.\n   This function returns the latency taken for the most recent inference operation.\n   returns:\n      Latency in microseconds.\n\n\nlatency()\n   @deprecated: Use `get_latency()` instead.\n   Retrieves the most recent inference latency.\n   This function returns the latency taken for the most recent inference operation.\n   returns:\n      Latency in microseconds.\n\n\nget_latency_count()\n   Retrieves the number of latency measurements.  \n   This function returns the total count of latency measurements. \n   returns:\n      The count of latency measurements.\n\n\nget_latency_list()\n   Retrieves a list of inference latency values.\n   returns:\n      A list of inference latency values in microseconds.\n\n\nget_latency_mean()\n   Retrieves the average inference latency.  \n   This function returns the mean latency of inference execution.\n   returns:\n      The average inference latency in microseconds.\n\n\nget_latency_std()\n   Retrieves the standard deviation of inference latency.  \n   This function calculates the variation in inference latency time.\n   returns:\n      The standard deviation of inference latency in microseconds.\n\n\nget_npu_inference_time()\n   Retrieves the most recent inference time.\n   This function returns the duration of the most recent inference execution.\n   returns:\n      Inference time in microseconds.\n\n\ninference_time()\n   @deprecated: Use `get_npu_inference_time()` instead.\n   Retrieves the most recent inference time.\n   This function returns the duration of the most recent inference execution.\n   returns:\n      Inference time in microseconds.\n\n\nget_npu_inference_time_count()\n   Retrieves the number of NPU inference time measurements.  \n   This function returns the total count of NPU inference time measurements.\n   returns:\n      The count of NPU inference time measurements.\n\n\nget_npu_inference_time_list()\n   Retrieves a list of NPU inference time values.\n   returns:\n      A list of inference time values in microseconds.\n\n\nget_npu_inference_time_mean()\n   Retrieves the average NPU inference time.  \n   This function returns the mean execution time for NPU inference. \n   returns:\n      The average NPU inference time in microseconds.\n\n\nget_npu_inference_time_std()\n   Retrieves the standard deviation of NPU inference time.  \n   This function returns the standard deviation of the inference time.\n   returns:\n      The standard deviation of inference time in microseconds.\n\n\nget_num_tail_tasks()\n   Retrieves the number of tail tasks in the model.\n   Tail tasks are tasks that do not have any subsequent tasks in the model's task chain.\n   This function returns the count of such tail tasks. \n   returns:\n      The number of tail tasks.\n\n\nget_num_tails()\n   @deprecated: Use `get_num_tail_tasks()` instead.\n   Retrieves the number of tail tasks in the model.\n   Tail tasks are tasks that do not have any subsequent tasks in the model's task chain.\n   This function returns the count of such tail tasks. \n   returns:\n      The number of tail tasks.\n\n\nget_output_data_type()\n   @deprecated: Use `get_output_data_type()` instead.\n   Retrieves the required output data types as a list of strings.\n   returns:\n      A list of strings representing the output data types.\n\n\noutput_dtype()\n   Retrieves the required output data types as a list of strings.\n   returns:\n      A list of strings representing the output data types.\n\n\nget_output_size()\n   Retrieves the total size of output tensors.\n   This function returns the total memory size required for the output tensors \n   of a single inference operation.\n   returns:\n     Output size in bytes for one inference.\n\n\noutput_size()\n   @deprecated: Use `get_output_size()` instead.\n   Retrieves the total size of output tensors.\n   This function returns the total memory size required for the output tensors \n   of a single inference operation.\n   returns:\n     Output size in bytes for one inference.\n\n\nget_task_order()\n   Retrieves the execution order of tasks in the model.\n   This function returns the sequence of tasks as they are executed within the model.\n   returns:\n      A vector of strings representing the task execution order.\n\n\ntask_order()\n   @deprecated: Use `get_task_order()` instead.\n   Retrieves the execution order of tasks in the model.\n   This function returns the sequence of tasks as they are executed within the model.\n   returns:\n      A vector of strings representing the task execution order.\n\n\nis_ppu()\n   Checks whether PPU is utilized.\n   returns:\n     True if PPU is being utilized, otherwise False.\n\n\nis_PPU()\n   @deprecated: Use `is_ppu()` instead.\n   Checks whether PPU is utilized.\n   returns:\n     True if PPU is being utilized, otherwise False.\n\n\nregister_callBack(callback)\n   Register user callback function to be called by inference completion.\n   parameters:\n      [in] callback : A function that is called when inference is complete.  \n                      It receives two arguments: outputs and user_arg.\n                      - outputs: The data from the output tensors.  \n                      - user_arg: A user-defined argument passed via RunAsync().\n\n\nRegisterCallBack(callback)\n   @deprecated: Use `register_callback()` instead.\n   Register user callback function to be called by inference completion.\n   parameters:\n      [in] callback : A function that is called when inference is complete.  \n                      It receives two arguments: outputs and user_arg.\n                      - outputs: The data from the output tensors.  \n                      - user_arg: A user-defined argument passed via RunAsync().\n\n\nrun(input_feed_list: List[np.ndarray])\n   Runs the inference engine synchronously using the specified input data.  \n   This function executes inference with the provided input data and returns\n   the output tensors.\n   parameters:\n      [in] input_feed_list  : A list of Numpy arrays representing the input data.\n   returns:\n      A list of Numpy arrays representing the output tensors.\n\n\nRun(input_feed_list: List[np.ndarray])\n   @deprecated: Use run() instead.\n   Runs the inference engine synchronously using the specified input data.  \n   This function executes inference with the provided input data and returns\n   the output tensors.\n   parameters:\n      [in] input_feed_list  : A list of Numpy arrays representing the input data.\n   returns:\n      A list of Numpy arrays representing the output tensors.\n\n\nrun_async(input_feed_list: List[np.ndarray], user_arg)\n   Initiates an asynchronous inference request and returns a job ID.\n   This function performs a non-blocking inference operation using the specified input\n   data. It returns a job ID that can be used with the `wait()` function to retrieve \n   the results.\n   parameters:\n      [in] input_feed_list  : input data for inference.\n      [in] user_arg     : user-defined arguments.\n                            (e.g. original frame data, metadata about input, ... )\n   returns:\n      Job ID that can be used with the `wait()` function to retrieve the inference result.\n\n\nRunAsync(input_feed_list: List[np.ndarray], user_arg)\n   @deprecated: Use `run_async()` instead.\n   Initiates an asynchronous inference request and returns a job ID.\n   This function performs a non-blocking inference operation using the specified input\n   data. It returns a job ID that can be used with the `wait()` function to retrieve \n   the results.\n   parameters:\n      [in] input_feed_list  : input data for inference.\n      [in] user_arg     : user-defined arguments.\n                            (e.g. original frame data, metadata about input, ... )\n   returns:\n      Job ID that can be used with the `wait()` function to retrieve the inference result.\n\n\nrun_benchmark(loop_cnt,input_feed_list)\n   Runs a benchmark test by executing inference multiple times.\n   This function performs inference in a loop for the specified number of times \n   and calculates the average frames per second (FPS).\n   parameters:\n      [in] loop_cnt     : Number of inference iterations.\n      [in] input_feed_list  : the input data for inference\n   returns:\n      Average FPS (frames per second) over the benchmark runs.\n\n\nRunBenchMark(loop_cnt,input_feed_list)\n   @deprecated: Use `run_benchmark()` instead.\n   Runs a benchmark test by executing inference multiple times.\n   This function performs inference in a loop for the specified number of times \n   and calculates the average frames per second (FPS).\n   parameters:\n      [in] loop_cnt     : Number of inference iterations.\n      [in] input_feed_list  : the input data for inference\n   returns:\n      Average FPS (frames per second) over the benchmark runs.\n\n\nwait(int jobId)\n   Wait until a request is complete and returns output.\n   parameters:\n      [in] jobId    : job Id returned by run_async()\n   returns:\n      output tensors as vector\n\n\nwait(int jobId)\n   @deprecated: Use `wait()` instead.\n   Wait until a request is complete and returns output.\n   parameters:\n      [in] jobId    : job Id returned by RunAsync()\n   returns:\n      output tensors as vector\n</code></pre>"},{"location":"docs/Appendix_Change_Log.html","title":"Change Log","text":""},{"location":"docs/Appendix_Change_Log.html#v295>_may>_2025","title":"v2.9.5 (May 2025)","text":"<ul> <li>Added full support for Python run_model.  </li> <li>Updated the run_model option and its description  </li> <li>Improve the Python API  <ul> <li>InferenceOption is now supported identically to the C++ API.  </li> <li>set_devices(...) \u2192 devices = [0]  </li> <li>set_bound_option(...) \u2192 bound_option = InferenceOption.BOUND_OPTION.NPU_ALL</li> <li>set_use_ort(...) \u2192 use_ort = True</li> <li>Callback functions registered via register_callback now accept user_arg of custom types. (removed .value)</li> <li>user_arg.value \u2192 user_arg</li> <li>run() now supports both single-input and batch-input modes, depending on the input format.</li> </ul> </li> <li>Modify the build.sh script according to cmake options.  <ul> <li>CMake option USE_ORT=ON, running build.sh --clean installs ONNX Runtime.  </li> <li>CMake option USE_PYTHON=ON, running build.sh installs the Python package.  </li> <li>CMake option USE_SERVICE=ON, running build.sh starts or restarts the service.  </li> </ul> </li> <li>Add dxrt-cli -v to display minimum driver &amp; compiler versions  </li> <li>Addressed multithreading issues by implementing additional locks, improving stability under heavy load.  </li> <li>Fix crash on multi-device environments with more than 2 H1 cards. (&gt;=8 devices)  </li> <li>Resolved data corruption errors that could occur in different scenarios, ensuring data integrity.  </li> <li>Fix profiler bugs.  </li> <li>Addressed issues identified by static analysis and other tools, enhancing code quality and reliability.  </li> <li>Add --use_ort flag to the run_model.py example for ONNX Runtime.  </li> <li>Add run batch function. (Python &amp; C++)  <ul> <li>batch inference with multiple inputs and multiple outputs.  </li> </ul> </li> <li>Minimum model file versions  <ul> <li>.dxnn file format version &gt;= v6  </li> <li>compiler version &gt;= v1.15.2  </li> </ul> </li> <li>Minimum Driver and Firmware versions  <ul> <li>RT Driver Version &gt;= v1.5.0  </li> <li>PCIe Driver Version &gt;= v1.4.0  </li> <li>Firmware Version &gt;= v2.0.5  </li> </ul> </li> </ul>"},{"location":"docs/Appendix_Change_Log.html#v282>_april>_2025","title":"v2.8.2 (April 2025)","text":"<ul> <li>Modify Inference Engine to be used with 'with' statements, and update relevant examples.  </li> <li>Add Python inference option interface with the following configurations  </li> <li>NPU Device Selection / NPU Bound Option / ORT Usage Flag  </li> <li>Display dxnn versions in parse_model (.dxnn file format version &amp; compiler version)  </li> <li>Added instructions on how to retrieve device status information  </li> <li>Driver and Firmware versions  <ul> <li>RT Driver &gt;= v1.3.3  </li> <li>Firmware &gt;= v1.6.3  </li> </ul> </li> </ul>"},{"location":"temp/configuraiton_guide_python_example.html","title":"Configuraiton guide python example","text":""},{"location":"temp/configuraiton_guide_python_example.html#configuration>_class>_guide","title":"<code>Configuration</code> Class Guide","text":"<p>The <code>Configuration</code> class is used to manage the overall behavior and environmental settings of the <code>InferenceEngine</code>. With this class, you can check version information or enable/disable specific features like displaying model info and profiling.</p>"},{"location":"temp/configuraiton_guide_python_example.html#key>_features>_and>_methods","title":"Key Features and Methods","text":""},{"location":"temp/configuraiton_guide_python_example.html#1>_initialization","title":"1. Initialization","text":"<p>Creates a <code>Configuration</code> object.</p> <pre><code># Create a Configuration object\nconfig = Configuration()\n</code></pre>"},{"location":"temp/configuraiton_guide_python_example.html#2>_configuration>_items>_configurationitem","title":"2. Configuration Items (<code>Configuration.ITEM</code>)","text":"<p>This is an enumeration type used with the <code>set_enable()</code> and <code>get_enable()</code> methods. The items shown in your code are:</p> <ul> <li><code>Configuration.ITEM.SHOW_MODEL_INFO</code>: When set to <code>True</code>, it prints detailed information about the model to the console when the <code>InferenceEngine</code> is initialized. This includes input/output shapes, layer configurations, and more. It's very useful for debugging.</li> <li><code>Configuration.ITEM.SHOW_PROFILE</code>: When set to <code>True</code>, it prints performance profiling data, such as the processing time for each layer during inference. This is used to analyze performance bottlenecks in the model.</li> </ul>"},{"location":"temp/configuraiton_guide_python_example.html#3>_enabledisable>_a>_setting>_set>_enable","title":"3. Enable/Disable a Setting (<code>set_enable</code>)","text":"<p>Turns a specific setting on (<code>True</code>) or off (<code>False</code>).</p> <ul> <li> <p>Parameters</p> <ul> <li><code>item</code> (<code>Configuration.ITEM</code>): The item to configure.</li> <li><code>value</code> (<code>bool</code>): The value to set (<code>True</code> or <code>False</code>).</li> </ul> </li> <li> <p>Example Usage</p> <pre><code># Enable displaying model info and profiling data\nconfig.set_enable(Configuration.ITEM.SHOW_MODEL_INFO, True)\nconfig.set_enable(Configuration.ITEM.SHOW_PROFILE, True)\n</code></pre> </li> </ul>"},{"location":"temp/configuraiton_guide_python_example.html#4>_check>_a>_settings>_status>_get>_enable","title":"4. Check a Setting's Status (<code>get_enable</code>)","text":"<p>Checks if a setting is currently enabled.</p> <ul> <li> <p>Parameters</p> <ul> <li><code>item</code> (<code>Configuration.ITEM</code>): The configuration item to check.</li> </ul> </li> <li> <p>Returns</p> <ul> <li><code>bool</code>: The current status of the item (<code>True</code> or <code>False</code>).</li> </ul> </li> <li> <p>Example Usage</p> <pre><code>if config.get_enable(Configuration.ITEM.SHOW_MODEL_INFO):\n    print('SHOW_MODEL_INFO configuration is enabled')\n</code></pre> </li> </ul>"},{"location":"temp/configuraiton_guide_python_example.html#5>_check>_version>_information","title":"5. Check Version Information","text":"<p>Retrieves version information for the runtime framework and drivers. This is useful for checking your system environment or for reporting issues.</p> <ul> <li> <p><code>get_version()</code>: Returns the runtime framework version as a string.</p> </li> <li> <p><code>get_driver_version()</code>: Returns the device driver version as a string.</p> </li> <li> <p><code>get_pcie_driver_version()</code>: Returns the PCIe driver version as a string.</p> </li> <li> <p>Example Usage</p> <pre><code>print('Runtime framework version:', config.get_version())\nprint('Device driver version:', config.get_driver_version())\n</code></pre> </li> </ul>"},{"location":"temp/configuraiton_guide_python_example.html#devicestatus>_class>_guide","title":"<code>DeviceStatus</code> Class Guide","text":"<p>The <code>DeviceStatus</code> class is used to query the physical status of the NPU (Neural Processing Unit) devices installed in your system. This class allows you to monitor real-time hardware information such as the number of devices, and the temperature, voltage, and clock speed of each device.</p>"},{"location":"temp/configuraiton_guide_python_example.html#key>_features>_and>_methods_1","title":"Key Features and Methods","text":""},{"location":"temp/configuraiton_guide_python_example.html#1>_get>_device>_count>_get>_device>_count","title":"1. Get Device Count (<code>get_device_count</code>)","text":"<p>A static method that returns the total number of available NPU devices in the system.</p> <ul> <li> <p>Returns</p> <ul> <li><code>int</code>: The total number of detected devices.</li> </ul> </li> <li> <p>Example Usage</p> <pre><code>device_count = DeviceStatus.get_device_count()\nprint(f\"Found {device_count} devices.\")\n</code></pre> </li> </ul>"},{"location":"temp/configuraiton_guide_python_example.html#2>_get>_current>_device>_status>_get>_current>_status","title":"2. Get Current Device Status (<code>get_current_status</code>)","text":"<p>A static method that returns a <code>DeviceStatus</code> object containing the current status information for a specific device.</p> <ul> <li> <p>Parameters</p> <ul> <li><code>device_id</code> (<code>int</code>): The ID of the device to query (starting from 0).</li> </ul> </li> <li> <p>Returns</p> <ul> <li>A <code>DeviceStatus</code> object containing the status information for that device.</li> </ul> </li> <li> <p>Example Usage</p> <pre><code># Get the status object for the first device (ID=0)\ndevice_status = DeviceStatus.get_current_status(0)\n</code></pre> </li> </ul>"},{"location":"temp/configuraiton_guide_python_example.html#3>_retrieve>_specific>_status>_information","title":"3. Retrieve Specific Status Information","text":"<p>You can query specific hardware details from the object returned by <code>get_current_status()</code>.</p> <ul> <li> <p><code>get_id()</code>: Returns the unique ID of the device.</p> </li> <li> <p><code>get_temperature(core_id)</code>: Returns the temperature of a specific core in Celsius (\u00b0C).</p> </li> <li> <p><code>get_npu_voltage(core_id)</code>: Returns the voltage of a specific core in Volts (V).</p> </li> <li> <p><code>get_npu_clock(core_id)</code>: Returns the clock speed of a specific core in Megahertz (MHz).</p> </li> <li> <p>Example Usage</p> <pre><code>device_count = DeviceStatus.get_device_count()\n\n# Iterate through all devices and print their status\nfor i in range(device_count):\n    device_status = DeviceStatus.get_current_status(i)\n    print(f'Device ID: {device_status.get_id()}')\n\n    # Assuming the device has 3 cores\n    for c in range(3):\n        temp = device_status.get_temperature(c)\n        voltage = device_status.get_npu_voltage(c)\n        clock = device_status.get_npu_clock(c)\n        print(f'  NPU Core {c}: Temp={temp}\u00b0C, Voltage={voltage}V, Clock={clock}MHz')\n</code></pre> </li> </ul>"},{"location":"temp/configuration_guide.html","title":"Programming Guide: <code>dxrt::Configuration</code>","text":""},{"location":"temp/configuration_guide.html#1>_overview","title":"1. Overview","text":"<p>The <code>dxrt::Configuration</code> class is a central component within the DXRT library responsible for managing global application settings. It is designed as a thread-safe singleton, ensuring that there is only one instance of the configuration manager throughout the application's lifecycle. This provides a consistent and safe point of access for querying and modifying configuration parameters from any part of the codebase, even in multi-threaded environments.</p> <p>Key Features:</p> <ul> <li>Singleton Pattern: Guarantees a single, globally accessible instance.</li> <li>Thread Safety: Integrated mutex protects against race conditions during concurrent access.</li> <li>Dynamic Configuration: Allows enabling/disabling features and setting specific attributes at runtime.</li> <li>Configuration Locking: Supports making settings read-only to prevent changes after initialization.</li> <li>Version Reporting: Provides methods to retrieve versions of the library, drivers, and firmware.</li> </ul>"},{"location":"temp/configuration_guide.html#2>_core>_concepts","title":"2. Core Concepts","text":""},{"location":"temp/configuration_guide.html#singleton>_pattern","title":"Singleton Pattern","text":"<p>The <code>Configuration</code> class cannot be instantiated directly. Instead, you must access its single instance through the static <code>GetInstance()</code> method. This design ensures that all parts of the application refer to the same set of configuration data.</p> <pre><code>// Correct way to get the configuration instance\ndxrt::Configuration&amp; config = dxrt::Configuration::GetInstance();\n\n// Incorrect - The following lines will cause a compile error\n// dxrt::Configuration myConfig; // Error: constructor is private\n// dxrt::Configuration* pConfig = new dxrt::Configuration(); // Error: constructor is private\n</code></pre>"},{"location":"temp/configuration_guide.html#thread>_safety","title":"Thread Safety","text":"<p>All public methods that modify or read configuration data (e.g., <code>SetEnable</code>, <code>GetEnable</code>, <code>SetAttribute</code>) are internally protected by a <code>std::mutex</code>. This makes it safe to call them from multiple threads without needing external locking mechanisms, preventing data corruption and race conditions.</p>"},{"location":"temp/configuration_guide.html#items>_and>_attributes","title":"Items and Attributes","text":"<p>Configuration is organized around two main concepts:</p> <ul> <li><code>ITEM</code>: An enumeration representing a configurable feature or module (e.g., <code>DEBUG</code>, <code>PROFILER</code>). These items can typically be enabled or disabled.</li> <li><code>ATTRIBUTE</code>: An enumeration representing a specific property of an <code>ITEM</code>. Attributes hold string-based values that provide more detailed settings beyond a simple on/off state (e.g., a file path for <code>PROFILER_SAVE_DATA</code>).</li> </ul>"},{"location":"temp/configuration_guide.html#3>_getting>_started>_obtaining>_the>_instance","title":"3. Getting Started: Obtaining the Instance","text":"<p>To begin working with the configuration, first retrieve the singleton instance. It's recommended to get a reference to it once and reuse it.</p> <pre><code>#include \"dxrt/common.h\" // Assuming Configuration is in this header\n#include &lt;iostream&gt;\n\nint main() {\n    // Get the single, global instance of the Configuration class\n    dxrt::Configuration&amp; config = dxrt::Configuration::GetInstance();\n\n    // You can now use the 'config' object to manage settings\n    std::cout &lt;&lt; \"Successfully obtained Configuration instance.\" &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"temp/configuration_guide.html#4>_key>_operations>_and>_code>_examples","title":"4. Key Operations and Code Examples","text":""},{"location":"temp/configuration_guide.html#41>_loading>_configuration>_from>_a>_file","title":"4.1. Loading Configuration from a File","text":"<p>You can initialize settings from an external file using <code>LoadConfigFile()</code>. The specific format of the file is implementation-dependent, but it typically allows you to define which items are enabled and what their attribute values are.</p> <pre><code>dxrt::Configuration&amp; config = dxrt::Configuration::GetInstance();\ntry {\n    // Load settings from a configuration file\n    config.LoadConfigFile(\"settings.ini\");\n} catch (const std::runtime_error&amp; e) {\n    std::cerr &lt;&lt; \"Failed to load config file: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"temp/configuration_guide.html#42>_enabling>_and>_disabling>_features","title":"4.2. Enabling and Disabling Features","text":"<p>You can turn features on or off using <code>SetEnable()</code> and check their status with <code>GetEnable()</code>.</p> <pre><code>dxrt::Configuration&amp; config = dxrt::Configuration::GetInstance();\n\n// Enable the profiler\nconfig.SetEnable(dxrt::Configuration::ITEM::PROFILER, true);\n\n// Check if the profiler is enabled\nif (config.GetEnable(dxrt::Configuration::ITEM::PROFILER)) {\n    std::cout &lt;&lt; \"Profiler is enabled.\" &lt;&lt; std::endl;\n} else {\n    std::cout &lt;&lt; \"Profiler is disabled.\" &lt;&lt; std::endl;\n}\n\n// Disable debug mode\nconfig.SetEnable(dxrt::Configuration::ITEM::DEBUG, false);\n</code></pre>"},{"location":"temp/configuration_guide.html#43>_working>_with>_attributes","title":"4.3. Working with Attributes","text":"<p>For more fine-grained control, use <code>SetAttribute()</code> and <code>GetAttribute()</code> to manage string-based values for an item's attributes.</p> <pre><code>dxrt::Configuration&amp; config = dxrt::Configuration::GetInstance();\n\n// First, ensure the parent item is enabled\nconfig.SetEnable(dxrt::Configuration::ITEM::PROFILER, true);\n\n// Set the path where profiler data should be saved\nstd::string profile_path = \"/var/log/my_app_profile.json\";\nconfig.SetAttribute(dxrt::Configuration::ITEM::PROFILER,\n                    dxrt::Configuration::ATTRIBUTE::PROFILER_SAVE_DATA,\n                    profile_path);\n\n// Retrieve the attribute value later\nstd::string saved_path = config.GetAttribute(dxrt::Configuration::ITEM::PROFILER,\n                                          dxrt::Configuration::ATTRIBUTE::PROFILER_SAVE_DATA);\n\nif (!saved_path.empty()) {\n    std::cout &lt;&lt; \"Profiler data will be saved to: \" &lt;&lt; saved_path &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"temp/configuration_guide.html#44>_locking>_configuration","title":"4.4. Locking Configuration","text":"<p>After initial setup, you can lock an item to prevent any further changes to its <code>enabled</code> status or attributes. This is useful for ensuring configuration stability during the main application logic.</p> <pre><code>dxrt::Configuration&amp; config = dxrt::Configuration::GetInstance();\n\n// Set initial configuration for the profiler\nconfig.SetEnable(dxrt::Configuration::ITEM::PROFILER, true);\n\n// Lock the PROFILER item\nconfig.LockEnable(dxrt::Configuration::ITEM::PROFILER);\n\n// Any subsequent attempt to change this item will be ignored (silently)\nconfig.SetEnable(dxrt::Configuration::ITEM::PROFILER, false); // This will have no effect\n\n// The profiler remains enabled\nbool is_enabled = config.GetEnable(dxrt::Configuration::ITEM::PROFILER); // will return true\n</code></pre>"},{"location":"temp/configuration_guide.html#45>_retrieving>_version>_information","title":"4.5. Retrieving Version Information","text":"<p>The class provides several methods to query version information, which is critical for debugging, logging, and ensuring system compatibility. These methods may throw an exception if a version mismatch or other error occurs.</p> <pre><code>#include &lt;vector&gt;\n#include &lt;utility&gt;\n#include &lt;string&gt;\n\ndxrt::Configuration&amp; config = dxrt::Configuration::GetInstance();\n\ntry {\n    std::cout &lt;&lt; \"DXRT Library Version: \" &lt;&lt; config.GetVersion() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Driver Version: \" &lt;&lt; config.GetDriverVersion() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"ONNX Runtime Version: \" &lt;&lt; config.GetONNXRuntimeVersion() &lt;&lt; std::endl;\n\n    std::vector&lt;std::pair&lt;int, std::string&gt;&gt; fw_versions = config.GetFirmwareVersions();\n    for (const auto&amp; fw : fw_versions) {\n        std::cout &lt;&lt; \"Device \" &lt;&lt; fw.first &lt;&lt; \" Firmware Version: \" &lt;&lt; fw.second &lt;&lt; std::endl;\n    }\n\n} catch (const std::runtime_error&amp; e) { // Or a more specific exception type if defined\n    std::cerr &lt;&lt; \"Error retrieving version information: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"temp/configuration_guide.html#5>_api>_reference>_summary","title":"5. API Reference Summary","text":""},{"location":"temp/configuration_guide.html#main>_methods","title":"Main Methods","text":"Method Signature Description <code>static Configuration&amp; GetInstance()</code> Returns the unique singleton instance of the class. <code>void LoadConfigFile(const std::string&amp; fileName)</code> Loads and applies configurations from a specified file. <code>void SetEnable(const ITEM item, bool enabled)</code> Sets the enabled (<code>true</code>) or disabled (<code>false</code>) status for an item. <code>bool GetEnable(const ITEM item)</code> Retrieves the current enabled status of an item. <code>void SetAttribute(const ITEM item, const ATTRIBUTE attrib, const std::string&amp; value)</code> Sets a string value for a specific attribute of an item. <code>std::string GetAttribute(const ITEM item, const ATTRIBUTE attrib)</code> Retrieves the string value of an attribute. Returns empty if not found. <code>void LockEnable(const ITEM item)</code> Locks an item's configuration, making it read-only. <code>std::string GetVersion() const</code> Gets the version of the DXRT library. <code>std::string GetDriverVersion() const</code> Gets the version of the device driver. <code>std::string GetPCIeDriverVersion() const</code> Gets the version of the PCIe driver. <code>std::vector&lt;std::pair&lt;int, std::string&gt;&gt; GetFirmwareVersions() const</code> Gets the firmware versions for all detected devices. <code>std::string GetONNXRuntimeVersion() const</code> Gets the version of the linked ONNX Runtime library."},{"location":"temp/configuration_guide.html#enumerations","title":"Enumerations","text":""},{"location":"temp/configuration_guide.html#enum>_class>_item","title":"<code>enum class ITEM</code>","text":"Enumerator Description <code>DEBUG</code> Configuration related to debug mode. <code>PROFILER</code> Configuration related to profiler functionality. <code>SERVICE</code> Configuration related to service operation. <code>DYNAMIC_CPU_THREAD</code> Configuration for dynamic CPU thread management. <code>TASK_FLOW</code> Configuration related to task flow management. <code>SHOW_THROTTLING</code> Whether to display throttling information. <code>SHOW_PROFILE</code> Whether to display profile information. <code>SHOW_MODEL_INFO</code> Whether to display model information."},{"location":"temp/configuration_guide.html#enum>_class>_attribute","title":"<code>enum class ATTRIBUTE</code>","text":"Enumerator Description <code>PROFILER_SHOW_DATA</code> Attribute for showing profiler data. <code>PROFILER_SAVE_DATA</code> Attribute for saving profiler data."},{"location":"temp/inference_api_reference.html","title":"DXRT Inference API \uc644\uc804 \ucc38\uc870 \uac00\uc774\ub4dc","text":""},{"location":"temp/inference_api_reference.html#_1","title":"\uac1c\uc694","text":"<p>\uc774 \ubb38\uc11c\ub294 DXRT\uc758 \ubaa8\ub4e0 \ucd94\ub860 \uad00\ub828 API\ub4e4\uc5d0 \ub300\ud55c \uc0c1\uc138\ud55c \ucc38\uc870 \uac00\uc774\ub4dc\uc785\ub2c8\ub2e4. C++\uacfc Python \ubc84\uc804 \ubaa8\ub450\uc758 \ud5c8\uc6a9 \uc785\ub825 \ud615\ud0dc\uc640 \uc608\uc0c1 \ucd9c\ub825 \ud615\ud0dc\ub97c comprehensive\ud558\uac8c \uc815\ub9ac\ud588\uc2b5\ub2c8\ub2e4.</p>"},{"location":"temp/inference_api_reference.html#_2","title":"\ubaa9\ucc28","text":"<ol> <li>C++ Inference Engine API</li> <li>Python Inference Engine API</li> <li>\uc785\ub825 \ud615\ud0dc \ubd84\uc11d \ub85c\uc9c1</li> <li>\ucd9c\ub825 \ud615\ud0dc \uaddc\uce59</li> <li>\ud2b9\uc218 \ucf00\uc774\uc2a4</li> </ol>"},{"location":"temp/inference_api_reference.html#c>_inference>_engine>_api","title":"C++ Inference Engine API","text":""},{"location":"temp/inference_api_reference.html#1>_api","title":"1. \ub3d9\uae30 \ucd94\ub860 API","text":""},{"location":"temp/inference_api_reference.html#11>_run>_single>_inputoutput","title":"1.1 Run (Single Input/Output)","text":"<pre><code>TensorPtrs Run(void *inputPtr, void *userArg = nullptr, void *outputPtr = nullptr)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc124\uba85 \ubaa8\ub378 \ud0c0\uc785 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>void* inputPtr</code> \ub2e8\uc77c \uc785\ub825 \ud3ec\uc778\ud130 Single-Input <code>TensorPtrs</code> (Vector) \uc804\ud1b5\uc801\uc778 \ubc29\uc2dd <code>void* inputPtr</code> \uc5f0\uacb0\ub41c \ubc84\ud37c \ud3ec\uc778\ud130 Multi-Input <code>TensorPtrs</code> (Vector) Auto-split \uc801\uc6a9 <p>\uc608\uc81c: <pre><code>// Single input model\nauto outputs = ie.Run(inputData);\n\n// Multi-input model (auto-split)\nauto outputs = ie.Run(concatenatedInput);\n</code></pre></p>"},{"location":"temp/inference_api_reference.html#12>_run>_batch","title":"1.2 Run (Batch)","text":"<pre><code>std::vector&lt;TensorPtrs&gt; Run(\n    const std::vector&lt;void*&gt;&amp; inputBuffers,\n    const std::vector&lt;void*&gt;&amp; outputBuffers,\n    const std::vector&lt;void*&gt;&amp; userArgs\n)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc870\uac74 \ud574\uc11d \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>vector&lt;void*&gt;</code> (size=1) Single-Input \ub2e8\uc77c \ucd94\ub860 <code>vector&lt;TensorPtrs&gt;</code> (size=1) \ud2b9\uc218 \ucf00\uc774\uc2a4 <code>vector&lt;void*&gt;</code> (size=N) Single-Input \ubc30\uce58 \ucd94\ub860 <code>vector&lt;TensorPtrs&gt;</code> (size=N) N\uac1c \uc0d8\ud50c <code>vector&lt;void*&gt;</code> (size=M) Multi-Input, M==input_count \ub2e8\uc77c \ucd94\ub860 <code>vector&lt;TensorPtrs&gt;</code> (size=1) Multi-input \ub2e8\uc77c <code>vector&lt;void*&gt;</code> (size=N*M) Multi-Input, N*M==\ubc30\uc218 \ubc30\uce58 \ucd94\ub860 <code>vector&lt;TensorPtrs&gt;</code> (size=N) N\uac1c \uc0d8\ud50c, M\uac1c \uc785\ub825 <p>\uc608\uc81c: <pre><code>// Single input batch\nstd::vector&lt;void*&gt; batchInputs = {sample1, sample2, sample3};\nauto batchOutputs = ie.Run(batchInputs, outputBuffers, userArgs);\n\n// Multi-input single\nstd::vector&lt;void*&gt; multiInputs = {input1, input2}; // M=2\nauto singleOutput = ie.Run(multiInputs, {outputBuffer}, {userArg});\n\n// Multi-input batch  \nstd::vector&lt;void*&gt; multiBatch = {s1_i1, s1_i2, s2_i1, s2_i2}; // N=2, M=2\nauto batchOutputs = ie.Run(multiBatch, outputBuffers, userArgs);\n</code></pre></p>"},{"location":"temp/inference_api_reference.html#13>_runmultiinput>_dictionary","title":"1.3 RunMultiInput (Dictionary)","text":"<pre><code>TensorPtrs RunMultiInput(\n    const std::map&lt;std::string, void*&gt;&amp; inputTensors, \n    void *userArg = nullptr, \n    void *outputPtr = nullptr\n)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>map&lt;string, void*&gt;</code> \ubaa8\ub4e0 \uc785\ub825 \ud150\uc11c \uc774\ub984 \ud3ec\ud568 <code>TensorPtrs</code> Multi-input \uc804\uc6a9 <p>\uc608\uc81c: <pre><code>std::map&lt;std::string, void*&gt; inputs = {\n    {\"input1\", data1},\n    {\"input2\", data2}\n};\nauto outputs = ie.RunMultiInput(inputs);\n</code></pre></p>"},{"location":"temp/inference_api_reference.html#14>_runmultiinput>_vector","title":"1.4 RunMultiInput (Vector)","text":"<pre><code>TensorPtrs RunMultiInput(\n    const std::vector&lt;void*&gt;&amp; inputPtrs, \n    void *userArg = nullptr, \n    void *outputPtr = nullptr\n)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>vector&lt;void*&gt;</code> size == input_tensor_count <code>TensorPtrs</code> \uc21c\uc11c\ub294 GetInputTensorNames()"},{"location":"temp/inference_api_reference.html#2>_api","title":"2. \ube44\ub3d9\uae30 \ucd94\ub860 API","text":""},{"location":"temp/inference_api_reference.html#21>_runasync>_single","title":"2.1 RunAsync (Single)","text":"<pre><code>int RunAsync(void *inputPtr, void *userArg = nullptr, void *outputPtr = nullptr)\n</code></pre> \uc785\ub825 \ud615\ud0dc \ubaa8\ub378 \ud0c0\uc785 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>void* inputPtr</code> Single-Input <code>int</code> (jobId) Wait(jobId)\ub85c \uacb0\uacfc \uc218\uc2e0 <code>void* inputPtr</code> Multi-Input <code>int</code> (jobId) Auto-split \uc801\uc6a9"},{"location":"temp/inference_api_reference.html#22>_runasync>_vector","title":"2.2 RunAsync (Vector)","text":"<pre><code>int RunAsync(const std::vector&lt;void*&gt;&amp; inputPtrs, void *userArg = nullptr, void *outputPtr = nullptr)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc870\uac74 \ud574\uc11d \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>vector&lt;void*&gt;</code> (size==input_count) Multi-Input Multi-input \ub2e8\uc77c <code>int</code> (jobId) \uad8c\uc7a5 \ubc29\uc2dd <code>vector&lt;void*&gt;</code> (size!=input_count) Any \uccab \ubc88\uc9f8 \uc694\uc18c\ub9cc \uc0ac\uc6a9 <code>int</code> (jobId) Fallback"},{"location":"temp/inference_api_reference.html#23>_runasyncmultiinput>_dictionary","title":"2.3 RunAsyncMultiInput (Dictionary)","text":"<pre><code>int RunAsyncMultiInput(\n    const std::map&lt;std::string, void*&gt;&amp; inputTensors, \n    void *userArg = nullptr, \n    void *outputPtr = nullptr\n)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>map&lt;string, void*&gt;</code> Multi-input \ubaa8\ub378 \uc804\uc6a9 <code>int</code> (jobId) \uac00\uc7a5 \uba85\ud655\ud55c \ubc29\uc2dd"},{"location":"temp/inference_api_reference.html#24>_runasyncmultiinput>_vector","title":"2.4 RunAsyncMultiInput (Vector)","text":"<pre><code>int RunAsyncMultiInput(\n    const std::vector&lt;void*&gt;&amp; inputPtrs, \n    void *userArg = nullptr, \n    void *outputPtr = nullptr\n)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>vector&lt;void*&gt;</code> size == input_tensor_count <code>int</code> (jobId) Dictionary\ub85c \ubcc0\ud658\ub428"},{"location":"temp/inference_api_reference.html#3>_api","title":"3. \uc7a5\uce58 \uac80\uc99d API","text":""},{"location":"temp/inference_api_reference.html#31>_validatedevice>_single","title":"3.1 ValidateDevice (Single)","text":"<pre><code>TensorPtrs ValidateDevice(void *inputPtr, int deviceId = 0)\n</code></pre> \uc785\ub825 \ud615\ud0dc \ubaa8\ub378 \ud0c0\uc785 \ucd9c\ub825 \ud615\ud0dc \uc81c\uc57d \uc870\uac74 <code>void* inputPtr</code> Any <code>TensorPtrs</code> Debug \ubaa8\ub4dc \ubaa8\ub378\ub9cc"},{"location":"temp/inference_api_reference.html#32>_validatedevice>_vector","title":"3.2 ValidateDevice (Vector)","text":"<pre><code>TensorPtrs ValidateDevice(const std::vector&lt;void*&gt;&amp; inputPtrs, int deviceId = 0)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc870\uac74 \ud574\uc11d \ucd9c\ub825 \ud615\ud0dc <code>vector&lt;void*&gt;</code> (size==input_count) Multi-Input Multi-input \uac80\uc99d <code>TensorPtrs</code> <code>vector&lt;void*&gt;</code> (other) Any \uccab \ubc88\uc9f8 \uc694\uc18c\ub9cc \uc0ac\uc6a9 <code>TensorPtrs</code>"},{"location":"temp/inference_api_reference.html#33>_validatedevicemultiinput","title":"3.3 ValidateDeviceMultiInput","text":"<pre><code>TensorPtrs ValidateDeviceMultiInput(const std::map&lt;std::string, void*&gt;&amp; inputTensors, int deviceId = 0)\nTensorPtrs ValidateDeviceMultiInput(const std::vector&lt;void*&gt;&amp; inputPtrs, int deviceId = 0)\n</code></pre>"},{"location":"temp/inference_api_reference.html#python>_inference>_engine>_api","title":"Python Inference Engine API","text":""},{"location":"temp/inference_api_reference.html#1>_api_1","title":"1. \ub3d9\uae30 \ucd94\ub860 API","text":""},{"location":"temp/inference_api_reference.html#11>_run>_unified>_api","title":"1.1 run (Unified API)","text":"<pre><code>def run(\n    input_data: Union[np.ndarray, List[np.ndarray], List[List[np.ndarray]]],\n    output_buffers: Optional[Union[List[np.ndarray], List[List[np.ndarray]]]] = None,\n    user_args: Optional[Union[Any, List[Any]]] = None\n) -&gt; Union[List[np.ndarray], List[List[np.ndarray]]]\n</code></pre> <p>\uc0c1\uc138 \uc785\ub825/\ucd9c\ub825 \ub9e4\ud2b8\ub9ad\uc2a4:</p> \uc785\ub825 \ud0c0\uc785 \uc785\ub825 \uc870\uac74 \ubaa8\ub378 \ud0c0\uc785 \ud574\uc11d \ucd9c\ub825 \ud0c0\uc785 \ucd9c\ub825 \uad6c\uc870 <code>np.ndarray</code> size == total_input_size Multi-Input Auto-split \ub2e8\uc77c <code>List[np.ndarray]</code> \ub2e8\uc77c \uc0d8\ud50c \ucd9c\ub825 <code>np.ndarray</code> size != total_input_size Single-Input \ub2e8\uc77c \ucd94\ub860 <code>List[np.ndarray]</code> \ub2e8\uc77c \uc0d8\ud50c \ucd9c\ub825 <code>List[np.ndarray]</code> len == 1 Single-Input \ub2e8\uc77c \ucd94\ub860 <code>List[np.ndarray]</code> \ub2e8\uc77c \uc0d8\ud50c \ucd9c\ub825 <code>List[np.ndarray]</code> len == input_count Multi-Input \ub2e8\uc77c \ucd94\ub860 <code>List[np.ndarray]</code> \ub2e8\uc77c \uc0d8\ud50c \ucd9c\ub825 <code>List[np.ndarray]</code> len == N*input_count Multi-Input \ubc30\uce58 \ucd94\ub860 (N\uc0d8\ud50c) <code>List[List[np.ndarray]]</code> N\uac1c \uc0d8\ud50c \ucd9c\ub825 <code>List[np.ndarray]</code> len &gt; 1 Single-Input \ubc30\uce58 \ucd94\ub860 <code>List[List[np.ndarray]]</code> len\uac1c \uc0d8\ud50c \ucd9c\ub825 <code>List[List[np.ndarray]]</code> \uba85\uc2dc\uc801 \ubc30\uce58 Any \ubc30\uce58 \ucd94\ub860 <code>List[List[np.ndarray]]</code> \uc678\ubd80 \ub9ac\uc2a4\ud2b8 \ud06c\uae30\ub9cc\ud07c <p>Auto-split \ud2b9\uc218 \ucf00\uc774\uc2a4:</p> \uc870\uac74 \uc785\ub825 \uc608\uc2dc \ud574\uc11d \ucd9c\ub825 Multi-input + \uccab \ubc88\uc9f8 \uc694\uc18c\uac00 total_size <code>[concatenated_array]</code> Auto-split \ub2e8\uc77c <code>List[np.ndarray]</code> Multi-input + \ubaa8\ub4e0 \uc694\uc18c\uac00 total_size <code>[concat1, concat2, concat3]</code> Auto-split \ubc30\uce58 <code>List[List[np.ndarray]]</code> <p>\uc608\uc81c: <pre><code># 1. Single array auto-split (multi-input)\nconcatenated = np.zeros(ie.get_input_size(), dtype=np.uint8)\noutputs = ie.run(concatenated)  # List[np.ndarray]\n\n# 2. Multi-input single\ninput_list = [input1_array, input2_array]  # len == 2\noutputs = ie.run(input_list)  # List[np.ndarray]\n\n# 3. Multi-input batch (flattened)\nflattened = [s1_i1, s1_i2, s2_i1, s2_i2]  # 2 samples, 2 inputs each\noutputs = ie.run(flattened)  # List[List[np.ndarray]], len=2\n\n# 4. Multi-input batch (explicit)\nexplicit_batch = [[s1_i1, s1_i2], [s2_i1, s2_i2]]\noutputs = ie.run(explicit_batch)  # List[List[np.ndarray]], len=2\n\n# 5. Single-input batch\nsingle_batch = [sample1, sample2, sample3]\noutputs = ie.run(single_batch)  # List[List[np.ndarray]], len=3\n</code></pre></p>"},{"location":"temp/inference_api_reference.html#12>_run>_multi>_input>_dictionary","title":"1.2 run_multi_input (Dictionary)","text":"<pre><code>def run_multi_input(\n    input_tensors: Dict[str, np.ndarray],\n    output_buffers: Optional[List[np.ndarray]] = None,\n    user_arg: Any = None\n) -&gt; List[np.ndarray]\n</code></pre> \uc785\ub825 \ud0c0\uc785 \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud0c0\uc785 \ube44\uace0 <code>Dict[str, np.ndarray]</code> \ubaa8\ub4e0 \uc785\ub825 \ud150\uc11c \ud3ec\ud568 <code>List[np.ndarray]</code> Multi-input \uc804\uc6a9"},{"location":"temp/inference_api_reference.html#2>_api_1","title":"2. \ube44\ub3d9\uae30 \ucd94\ub860 API","text":""},{"location":"temp/inference_api_reference.html#21>_run>_async","title":"2.1 run_async","text":"<pre><code>def run_async(\n    input_data: Union[np.ndarray, List[np.ndarray]],\n    user_arg: Any = None,\n    output_buffer: Optional[Union[np.ndarray, List[np.ndarray]]] = None\n) -&gt; int\n</code></pre> \uc785\ub825 \ud0c0\uc785 \uc870\uac74 \ud574\uc11d \ucd9c\ub825 \ud0c0\uc785 \uc81c\uc57d <code>np.ndarray</code> Any \ub2e8\uc77c \ucd94\ub860 <code>int</code> (jobId) \ubc30\uce58 \uc9c0\uc6d0 \uc548\ud568 <code>List[np.ndarray]</code> len == input_count Multi-input \ub2e8\uc77c <code>int</code> (jobId) \ubc30\uce58 \uc9c0\uc6d0 \uc548\ud568 <code>List[np.ndarray]</code> len == 1 Single-input \ub2e8\uc77c <code>int</code> (jobId) \ubc30\uce58 \uc9c0\uc6d0 \uc548\ud568"},{"location":"temp/inference_api_reference.html#22>_run>_async>_multi>_input","title":"2.2 run_async_multi_input","text":"<pre><code>def run_async_multi_input(\n    input_tensors: Dict[str, np.ndarray],\n    user_arg: Any = None,\n    output_buffer: Optional[List[np.ndarray]] = None\n) -&gt; int\n</code></pre> \uc785\ub825 \ud0c0\uc785 \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud0c0\uc785 \ube44\uace0 <code>Dict[str, np.ndarray]</code> Multi-input \ubaa8\ub378 \uc804\uc6a9 <code>int</code> (jobId) \ub2e8\uc77c \ucd94\ub860\ub9cc"},{"location":"temp/inference_api_reference.html#3>_api_1","title":"3. \uc7a5\uce58 \uac80\uc99d API","text":""},{"location":"temp/inference_api_reference.html#31>_validate>_device","title":"3.1 validate_device","text":"<pre><code>def validate_device(\n    input_data: Union[np.ndarray, List[np.ndarray]], \n    device_id: int = 0\n) -&gt; List[np.ndarray]\n</code></pre> \uc785\ub825 \ud0c0\uc785 \uc870\uac74 \ud574\uc11d \ucd9c\ub825 \ud0c0\uc785 <code>np.ndarray</code> Any \ub2e8\uc77c \uac80\uc99d <code>List[np.ndarray]</code> <code>List[np.ndarray]</code> len == input_count Multi-input \uac80\uc99d <code>List[np.ndarray]</code> <code>List[np.ndarray]</code> len != input_count \uccab \ubc88\uc9f8 \uc694\uc18c\ub9cc <code>List[np.ndarray]</code>"},{"location":"temp/inference_api_reference.html#32>_validate>_device>_multi>_input","title":"3.2 validate_device_multi_input","text":"<pre><code>def validate_device_multi_input(\n    input_tensors: Dict[str, np.ndarray], \n    device_id: int = 0\n) -&gt; List[np.ndarray]\n</code></pre>"},{"location":"temp/inference_api_reference.html#4>_api","title":"4. \uae30\ud0c0 API","text":""},{"location":"temp/inference_api_reference.html#41>_run>_benchmark","title":"4.1 run_benchmark","text":"<pre><code>def run_benchmark(\n    num_loops: int, \n    input_data: Optional[List[np.ndarray]] = None\n) -&gt; float\n</code></pre> \uc785\ub825 \ud0c0\uc785 \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud0c0\uc785 \ube44\uace0 <code>List[np.ndarray]</code> Single input format <code>float</code> (FPS) \uccab \ubc88\uc9f8 \uc694\uc18c \ubc18\ubcf5 \uc0ac\uc6a9"},{"location":"temp/inference_api_reference.html#42>_wait","title":"4.2 wait","text":"<pre><code>def wait(job_id: int) -&gt; List[np.ndarray]\n</code></pre>"},{"location":"temp/inference_api_reference.html#_3","title":"\uc785\ub825 \ud615\ud0dc \ubd84\uc11d \ub85c\uc9c1","text":""},{"location":"temp/inference_api_reference.html#python","title":"Python \uc785\ub825 \ubd84\uc11d \ud50c\ub85c\uc6b0","text":"<pre><code>def _analyze_input_format(input_data):\n    # 1. np.ndarray \uac80\uc0ac\n    if isinstance(input_data, np.ndarray):\n        if should_auto_split_input(input_data):\n            return auto_split_single_inference()\n        else:\n            return single_inference()\n\n    # 2. List \uac80\uc0ac\n    if isinstance(input_data, list):\n        if isinstance(input_data[0], list):\n            # List[List[np.ndarray]] - \uba85\uc2dc\uc801 \ubc30\uce58\n            return explicit_batch_inference()\n        else:\n            # List[np.ndarray] - \ubcf5\uc7a1\ud55c \ubd84\uc11d \ud544\uc694\n            return analyze_list_ndarray(input_data)\n</code></pre>"},{"location":"temp/inference_api_reference.html#listnpndarray","title":"List[np.ndarray] \ubd84\uc11d \uc0c1\uc138","text":"<pre><code>def analyze_list_ndarray(input_data):\n    input_count = len(input_data)\n\n    if is_multi_input_model():\n        expected_count = get_input_tensor_count()\n\n        if input_count == expected_count:\n            return single_inference()\n        elif input_count % expected_count == 0:\n            batch_size = input_count // expected_count\n            return batch_inference(batch_size)\n        elif all(should_auto_split_input(arr) for arr in input_data):\n            return auto_split_batch_inference()\n        else:\n            raise ValueError(\"Invalid input count\")\n    else:  # Single-input model\n        if input_count == 1:\n            return single_inference()\n        else:\n            return batch_inference(input_count)\n</code></pre>"},{"location":"temp/inference_api_reference.html#_4","title":"\ucd9c\ub825 \ud615\ud0dc \uaddc\uce59","text":""},{"location":"temp/inference_api_reference.html#1","title":"1. \ub2e8\uc77c \ucd94\ub860 \ucd9c\ub825","text":"API \ucd9c\ub825 \ud615\ud0dc \uad6c\uc870 C++ Run <code>TensorPtrs</code> <code>vector&lt;shared_ptr&lt;Tensor&gt;&gt;</code> Python run <code>List[np.ndarray]</code> <code>[output1, output2, ...]</code>"},{"location":"temp/inference_api_reference.html#2","title":"2. \ubc30\uce58 \ucd94\ub860 \ucd9c\ub825","text":"API \ucd9c\ub825 \ud615\ud0dc \uad6c\uc870 C++ Run (batch) <code>vector&lt;TensorPtrs&gt;</code> <code>[sample1_outputs, sample2_outputs, ...]</code> Python run (batch) <code>List[List[np.ndarray]]</code> <code>[[s1_o1, s1_o2], [s2_o1, s2_o2], ...]</code>"},{"location":"temp/inference_api_reference.html#3","title":"3. \ube44\ub3d9\uae30 \ucd9c\ub825","text":"API \uc989\uc2dc \ubc18\ud658 Wait \ud6c4 C++ RunAsync <code>int</code> (jobId) <code>TensorPtrs</code> Python run_async <code>int</code> (jobId) <code>List[np.ndarray]</code>"},{"location":"temp/inference_api_reference.html#_5","title":"\ud2b9\uc218 \ucf00\uc774\uc2a4","text":""},{"location":"temp/inference_api_reference.html#1>_auto-split","title":"1. Auto-Split \uc870\uac74","text":"<p>C++: <pre><code>bool shouldAutoSplitInput() const {\n    return _isMultiInput &amp;&amp; _inputTasks.size() == 1;\n}\n</code></pre></p> <p>Python: <pre><code>def _should_auto_split_input(input_data: np.ndarray) -&gt; bool:\n    if not self.is_multi_input_model():\n        return False\n\n    expected_total_size = self.get_input_size()\n    actual_size = input_data.nbytes\n\n    return actual_size == expected_total_size\n</code></pre></p>"},{"location":"temp/inference_api_reference.html#2_1","title":"2. \ubc30\uce58 \ud06c\uae30 \uacb0\uc815","text":"\uc870\uac74 \ubc30\uce58 \ud06c\uae30 \uacc4\uc0b0 Single-input + List[np.ndarray] <code>len(input_data)</code> Multi-input + List[np.ndarray] <code>len(input_data) // input_tensor_count</code> List[List[np.ndarray]] <code>len(input_data)</code>"},{"location":"temp/inference_api_reference.html#3_1","title":"3. \uc5d0\ub7ec \uc870\uac74","text":"\uc870\uac74 \uc5d0\ub7ec \ud0c0\uc785 \uba54\uc2dc\uc9c0 Multi-input + \uc798\ubabb\ub41c \ud06c\uae30 <code>ValueError</code> \"Invalid input count for multi-input model\" \ube44\ub3d9\uae30 + \ubc30\uce58 <code>ValueError</code> \"Batch inference not supported in async\" \ube48 \uc785\ub825 <code>ValueError</code> \"Input data cannot be empty\" \ud0c0\uc785 \ubd88\uc77c\uce58 <code>TypeError</code> \"Expected np.ndarray or List[np.ndarray]\""},{"location":"temp/inference_api_reference.html#4>_output>_buffer","title":"4. Output Buffer \ucc98\ub9ac","text":""},{"location":"temp/inference_api_reference.html#python>_output>_buffer","title":"Python Output Buffer \ub9e4\ud2b8\ub9ad\uc2a4","text":"\uc785\ub825 \ud615\ud0dc Output Buffer \ud615\ud0dc \ucc98\ub9ac \ubc29\uc2dd \ub2e8\uc77c \ucd94\ub860 <code>None</code> \uc790\ub3d9 \ud560\ub2f9 \ub2e8\uc77c \ucd94\ub860 <code>List[np.ndarray]</code> \uc0ac\uc6a9\uc790 \uc81c\uacf5 \ub2e8\uc77c \ucd94\ub860 <code>np.ndarray</code> (total_size) Auto-split \ud6c4 \uc0ac\uc6a9 \ubc30\uce58 \ucd94\ub860 <code>List[List[np.ndarray]]</code> \uba85\uc2dc\uc801 \ubc30\uce58 \ubc84\ud37c \ubc30\uce58 \ucd94\ub860 <code>List[np.ndarray]</code> \ud50c\ub798\ud2bc\ub41c \ubc30\uce58 \ubc84\ud37c"},{"location":"temp/inference_api_reference.html#_6","title":"\uc131\ub2a5 \uace0\ub824\uc0ac\ud56d","text":""},{"location":"temp/inference_api_reference.html#1_1","title":"1. \uba54\ubaa8\ub9ac \ud560\ub2f9","text":"\ubc29\uc2dd \uc7a5\uc810 \ub2e8\uc810 \uc790\ub3d9 \ud560\ub2f9 (No Buffer) \uc0ac\uc6a9 \ud3b8\uc758\uc131 \ub9e4\ubc88 \uba54\ubaa8\ub9ac \ud560\ub2f9 \uc0ac\uc6a9\uc790 \uc81c\uacf5 (With Buffer) \uc131\ub2a5 \ucd5c\uc801\ud654 \uba54\ubaa8\ub9ac \uad00\ub9ac \ubcf5\uc7a1"},{"location":"temp/inference_api_reference.html#2_2","title":"2. \ucd94\ub860 \ubc29\uc2dd","text":"\ubc29\uc2dd \uc6a9\ub3c4 \ud2b9\uc9d5 \ub3d9\uae30 \ucd94\ub860 \uac04\ub2e8\ud55c \ucc98\ub9ac \uc21c\ucc28 \uc2e4\ud589 \ube44\ub3d9\uae30 \ucd94\ub860 \ub192\uc740 \ucc98\ub9ac\ub7c9 \ucf5c\ubc31 \uad00\ub9ac \ud544\uc694 \ubc30\uce58 \ucd94\ub860 \ub300\ub7c9 \ucc98\ub9ac \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9 \uc99d\uac00 <p>\uc774 \ubb38\uc11c\ub294 DXRT\uc758 \ubaa8\ub4e0 \ucd94\ub860 API\uc5d0 \ub300\ud55c \uc644\uc804\ud55c \ucc38\uc870\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uac01 API\uc758 \uc815\ud655\ud55c \uc0ac\uc6a9\ubc95\uacfc \uc608\uc0c1 \ub3d9\uc791\uc744 \uc774\ud574\ud558\uc5ec \uc62c\ubc14\ub978 \ucd94\ub860 \ucf54\ub4dc\ub97c \uc791\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. </p>"},{"location":"temp/multi_input_model_inference_guide.html","title":"Multi-Input \ubaa8\ub378 \ucd94\ub860 API \uac00\uc774\ub4dc","text":""},{"location":"temp/multi_input_model_inference_guide.html#_1","title":"\uac1c\uc694","text":"<p>DXRT\ub294 \uc5ec\ub7ec \uac1c\uc758 \uc785\ub825 \ud150\uc11c\ub97c \uac00\uc9c4 multi-input \ubaa8\ub378\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ucd94\ub860 \ubc29\uc2dd\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc774 \ubb38\uc11c\ub294 multi-input \ubaa8\ub378\uc758 \ucd94\ub860 API \ud65c\uc6a9 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_model_inference_guide.html#multi-input","title":"Multi-Input \ubaa8\ub378 \ud655\uc778","text":""},{"location":"temp/multi_input_model_inference_guide.html#c","title":"C++","text":"<pre><code>dxrt::InferenceEngine ie(modelPath);\n\n// \ubaa8\ub378\uc774 multi-input\uc778\uc9c0 \ud655\uc778\nbool isMultiInput = ie.IsMultiInputModel();\n\n// \uc785\ub825 \ud150\uc11c \uac1c\uc218 \ud655\uc778\nint inputCount = ie.GetInputTensorCount();\n\n// \uc785\ub825 \ud150\uc11c \uc774\ub984\ub4e4 \ud655\uc778\nstd::vector&lt;std::string&gt; inputNames = ie.GetInputTensorNames();\n\n// \uc785\ub825 \ud150\uc11c\uc640 \ud0dc\uc2a4\ud06c \ub9e4\ud551 \ud655\uc778\nstd::map&lt;std::string, std::string&gt; mapping = ie.GetInputTensorToTaskMapping();\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#python","title":"Python","text":"<pre><code>from dx_engine import InferenceEngine\n\nie = InferenceEngine(model_path)\n\n# \ubaa8\ub378\uc774 multi-input\uc778\uc9c0 \ud655\uc778\nis_multi_input = ie.is_multi_input_model()\n\n# \uc785\ub825 \ud150\uc11c \uac1c\uc218 \ud655\uc778\ninput_count = ie.get_input_tensor_count()\n\n# \uc785\ub825 \ud150\uc11c \uc774\ub984\ub4e4 \ud655\uc778\ninput_names = ie.get_input_tensor_names()\n\n# \uc785\ub825 \ud150\uc11c\uc640 \ud0dc\uc2a4\ud06c \ub9e4\ud551 \ud655\uc778\nmapping = ie.get_input_tensor_to_task_mapping()\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#multi-input_1","title":"Multi-Input \ucd94\ub860 \ubc29\uc2dd","text":""},{"location":"temp/multi_input_model_inference_guide.html#1>_no>_output>_buffer>_tests","title":"1. No Output Buffer Tests (\uc790\ub3d9 \ud560\ub2f9)","text":"<p>\ucd9c\ub825 \ubc84\ud37c\ub97c \uc81c\uacf5\ud558\uc9c0 \uc54a\uace0 \ucd94\ub860 \uc5d4\uc9c4\uc774 \uc790\ub3d9\uc73c\ub85c \uba54\ubaa8\ub9ac\ub97c \ud560\ub2f9\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_model_inference_guide.html#11>_dictionary>_format","title":"1.1 Dictionary Format (\uad8c\uc7a5)","text":"<p>\uc785\ub825 \ud150\uc11c\ub97c \uc774\ub984\uc73c\ub85c \ub9e4\ud551\ud558\uc5ec \uc81c\uacf5\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \uac00\uc7a5 \uba85\ud655\ud558\uace0 \uc2e4\uc218\uac00 \uc801\uc740 \ubc29\ubc95\uc785\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_model_inference_guide.html#c_1","title":"C++","text":"<pre><code>// Dictionary format \uc0ac\uc6a9 (\uc790\ub3d9 \ud560\ub2f9)\nstd::map&lt;std::string, void*&gt; inputTensors;\ninputTensors[\"input1\"] = input1_data;\ninputTensors[\"input2\"] = input2_data;\n\n// \ucd9c\ub825 \ubc84\ud37c \uc5c6\uc774 \ub3d9\uae30 \ucd94\ub860 (\uc790\ub3d9 \ud560\ub2f9)\nauto outputs = ie.RunMultiInput(inputTensors);\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#python_1","title":"Python","text":"<pre><code># Dictionary format \uc0ac\uc6a9 (\uc790\ub3d9 \ud560\ub2f9)\ninput_tensors = {\n    \"input1\": input1_array,\n    \"input2\": input2_array\n}\n\n# \ucd9c\ub825 \ubc84\ud37c \uc5c6\uc774 \ub3d9\uae30 \ucd94\ub860 (\uc790\ub3d9 \ud560\ub2f9)\noutputs = ie.run_multi_input(input_tensors)\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#12>_vector>_format","title":"1.2 Vector Format","text":"<p>\uc785\ub825 \ud150\uc11c\ub97c \uc21c\uc11c\ub300\ub85c \ubca1\ud130/\ub9ac\uc2a4\ud2b8\ub85c \uc81c\uacf5\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. <code>GetInputTensorNames()</code>\ub85c \ubc18\ud658\ub41c \uc21c\uc11c\uc640 \uc77c\uce58\ud574\uc57c \ud569\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_model_inference_guide.html#c_2","title":"C++","text":"<pre><code>// Vector format \uc0ac\uc6a9 (GetInputTensorNames() \uc21c\uc11c\uc640 \uc77c\uce58)\nstd::vector&lt;void*&gt; inputPtrs = {input1_data, input2_data};\n\n// \ucd9c\ub825 \ubc84\ud37c \uc5c6\uc774 \ub3d9\uae30 \ucd94\ub860 (\uc790\ub3d9 \ud560\ub2f9)\nauto outputs = ie.RunMultiInput(inputPtrs);\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#python_2","title":"Python","text":"<pre><code># Vector format \uc0ac\uc6a9 (get_input_tensor_names() \uc21c\uc11c\uc640 \uc77c\uce58)\ninput_list = [input1_array, input2_array]\n\n# \ucd9c\ub825 \ubc84\ud37c \uc5c6\uc774 \ub3d9\uae30 \ucd94\ub860 (\uc790\ub3d9 \ud560\ub2f9)\noutputs = ie.run(input_list)\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#13>_auto-split>_format","title":"1.3 Auto-Split Format","text":"<p>\ub2e8\uc77c \uc5f0\uacb0\ub41c \ubc84\ud37c\ub97c \uc790\ub3d9\uc73c\ub85c \uc5ec\ub7ec \uc785\ub825\uc73c\ub85c \ubd84\ud560\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \ucd1d \uc785\ub825 \ud06c\uae30\uac00 \uc77c\uce58\ud560 \ub54c \uc790\ub3d9\uc73c\ub85c \uc801\uc6a9\ub429\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_model_inference_guide.html#c_3","title":"C++","text":"<pre><code>// \ubaa8\ub4e0 \uc785\ub825\uc744 \uc5f0\uacb0\ud55c \ub2e8\uc77c \ubc84\ud37c\nstd::vector&lt;uint8_t&gt; concatenatedInput(ie.GetInputSize());\n// ... \ub370\uc774\ud130 \ucc44\uc6b0\uae30 ...\n\n// \uc790\ub3d9 \ubd84\ud560\ub418\uc5b4 \ucc98\ub9ac\ub428 (\ucd9c\ub825 \ubc84\ud37c \uc790\ub3d9 \ud560\ub2f9)\nauto outputs = ie.Run(concatenatedInput.data());\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#python_3","title":"Python","text":"<pre><code># \ubaa8\ub4e0 \uc785\ub825\uc744 \uc5f0\uacb0\ud55c \ub2e8\uc77c \ubc30\uc5f4\nconcatenated_input = np.zeros(ie.get_input_size(), dtype=np.uint8)\n# ... \ub370\uc774\ud130 \ucc44\uc6b0\uae30 ...\n\n# \uc790\ub3d9 \ubd84\ud560\ub418\uc5b4 \ucc98\ub9ac\ub428 (\ucd9c\ub825 \ubc84\ud37c \uc790\ub3d9 \ud560\ub2f9)\noutputs = ie.run(concatenated_input)\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#2>_with>_output>_buffer>_tests","title":"2. With Output Buffer Tests (\uc0ac\uc6a9\uc790 \uc81c\uacf5)","text":"<p>\uc0ac\uc6a9\uc790\uac00 \ucd9c\ub825 \ubc84\ud37c\ub97c \ubbf8\ub9ac \ud560\ub2f9\ud558\uc5ec \uc81c\uacf5\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \uba54\ubaa8\ub9ac \uad00\ub9ac\uc640 \uc131\ub2a5 \ucd5c\uc801\ud654\uc5d0 \uc720\ub9ac\ud569\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_model_inference_guide.html#21>_dictionary>_format","title":"2.1 Dictionary Format","text":""},{"location":"temp/multi_input_model_inference_guide.html#c_4","title":"C++","text":"<pre><code>// Dictionary format \uc0ac\uc6a9\nstd::map&lt;std::string, void*&gt; inputTensors;\ninputTensors[\"input1\"] = input1_data;\ninputTensors[\"input2\"] = input2_data;\n\n// \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// \ub3d9\uae30 \ucd94\ub860 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\nauto outputs = ie.RunMultiInput(inputTensors, userArg, outputBuffer.data());\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#python_4","title":"Python","text":"<pre><code># Dictionary format \uc0ac\uc6a9\ninput_tensors = {\n    \"input1\": input1_array,\n    \"input2\": input2_array\n}\n\n# \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\noutput_buffers = [np.zeros(size, dtype=np.uint8) for size in ie.get_output_tensor_sizes()]\n\n# \ub3d9\uae30 \ucd94\ub860 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\noutputs = ie.run_multi_input(input_tensors, output_buffers=output_buffers)\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#22>_vector>_format","title":"2.2 Vector Format","text":""},{"location":"temp/multi_input_model_inference_guide.html#c_5","title":"C++","text":"<pre><code>// Vector format \uc0ac\uc6a9 (GetInputTensorNames() \uc21c\uc11c\uc640 \uc77c\uce58)\nstd::vector&lt;void*&gt; inputPtrs = {input1_data, input2_data};\n\n// \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// \ub3d9\uae30 \ucd94\ub860 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\nauto outputs = ie.RunMultiInput(inputPtrs, userArg, outputBuffer.data());\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#python_5","title":"Python","text":"<pre><code># Vector format \uc0ac\uc6a9 (get_input_tensor_names() \uc21c\uc11c\uc640 \uc77c\uce58)\ninput_list = [input1_array, input2_array]\n\n# \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\noutput_buffers = [np.zeros(size, dtype=np.uint8) for size in ie.get_output_tensor_sizes()]\n\n# \ub3d9\uae30 \ucd94\ub860 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\noutputs = ie.run(input_list, output_buffers=output_buffers)\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#23>_auto-split>_format","title":"2.3 Auto-Split Format","text":""},{"location":"temp/multi_input_model_inference_guide.html#c_6","title":"C++","text":"<pre><code>// \ubaa8\ub4e0 \uc785\ub825\uc744 \uc5f0\uacb0\ud55c \ub2e8\uc77c \ubc84\ud37c\nstd::vector&lt;uint8_t&gt; concatenatedInput(ie.GetInputSize());\n// ... \ub370\uc774\ud130 \ucc44\uc6b0\uae30 ...\n\n// \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// \uc790\ub3d9 \ubd84\ud560\ub418\uc5b4 \ucc98\ub9ac\ub428 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\nauto outputs = ie.Run(concatenatedInput.data(), userArg, outputBuffer.data());\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#python_6","title":"Python","text":"<pre><code># \ubaa8\ub4e0 \uc785\ub825\uc744 \uc5f0\uacb0\ud55c \ub2e8\uc77c \ubc30\uc5f4\nconcatenated_input = np.zeros(ie.get_input_size(), dtype=np.uint8)\n# ... \ub370\uc774\ud130 \ucc44\uc6b0\uae30 ...\n\n# \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\noutput_buffers = [np.zeros(size, dtype=np.uint8) for size in ie.get_output_tensor_sizes()]\n\n# \uc790\ub3d9 \ubd84\ud560\ub418\uc5b4 \ucc98\ub9ac\ub428 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\noutputs = ie.run(concatenated_input, output_buffers=output_buffers)\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#multi-input>_batch","title":"Multi-Input Batch \ucd94\ub860","text":""},{"location":"temp/multi_input_model_inference_guide.html#explicit>_batch>_format","title":"Explicit Batch Format","text":"<p>\uac01 \ubc30\uce58 \uc544\uc774\ud15c\uc5d0 \ub300\ud574 \uba85\uc2dc\uc801\uc73c\ub85c \uc785\ub825 \ud150\uc11c\ub4e4\uc744 \uc81c\uacf5\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_model_inference_guide.html#python_7","title":"Python","text":"<pre><code># List[List[np.ndarray]] \ud615\ud0dc\nbatch_inputs = [\n    [sample1_input1, sample1_input2],  # \uccab \ubc88\uc9f8 \uc0d8\ud50c\n    [sample2_input1, sample2_input2],  # \ub450 \ubc88\uc9f8 \uc0d8\ud50c\n    [sample3_input1, sample3_input2]   # \uc138 \ubc88\uc9f8 \uc0d8\ud50c\n]\n\nbatch_outputs = [\n    [sample1_output1, sample1_output2],  # \uccab \ubc88\uc9f8 \uc0d8\ud50c \ucd9c\ub825 \ubc84\ud37c\n    [sample2_output1, sample2_output2],  # \ub450 \ubc88\uc9f8 \uc0d8\ud50c \ucd9c\ub825 \ubc84\ud37c\n    [sample3_output1, sample3_output2]   # \uc138 \ubc88\uc9f8 \uc0d8\ud50c \ucd9c\ub825 \ubc84\ud37c\n]\n\n# \ubc30\uce58 \ucd94\ub860\nresults = ie.run(batch_inputs, output_buffers=batch_outputs)\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#c_7","title":"C++","text":"<pre><code>// \ubc30\uce58 \uc785\ub825 \ubc84\ud37c\ub4e4 (\uc5f0\uacb0\ub41c \ud615\ud0dc)\nstd::vector&lt;void*&gt; batchInputs = {sample1_ptr, sample2_ptr, sample3_ptr};\nstd::vector&lt;void*&gt; batchOutputs = {output1_ptr, output2_ptr, output3_ptr};\nstd::vector&lt;void*&gt; userArgs = {userArg1, userArg2, userArg3};\n\n// \ubc30\uce58 \ucd94\ub860\nauto results = ie.Run(batchInputs, batchOutputs, userArgs);\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#flattened>_batch>_format","title":"Flattened Batch Format","text":"<p>\ubaa8\ub4e0 \uc785\ub825\uc744 \ud50c\ub798\ud2bc\ub41c \ud615\ud0dc\ub85c \uc81c\uacf5\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_model_inference_guide.html#python_8","title":"Python","text":"<pre><code># \ud50c\ub798\ud2bc\ub41c \ud615\ud0dc: [sample1_input1, sample1_input2, sample2_input1, sample2_input2, ...]\nflattened_inputs = [\n    sample1_input1, sample1_input2,  # \uccab \ubc88\uc9f8 \uc0d8\ud50c\n    sample2_input1, sample2_input2,  # \ub450 \ubc88\uc9f8 \uc0d8\ud50c\n    sample3_input1, sample3_input2   # \uc138 \ubc88\uc9f8 \uc0d8\ud50c\n]\n\n# \uc790\ub3d9\uc73c\ub85c \ubc30\uce58\ub85c \uc778\uc2dd\ub428 (\uc785\ub825 \uac1c\uc218\uac00 \ubaa8\ub378 \uc785\ub825 \uac1c\uc218\uc758 \ubc30\uc218)\nresults = ie.run(flattened_inputs, output_buffers=batch_outputs)\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#_2","title":"\ube44\ub3d9\uae30 \ucd94\ub860","text":""},{"location":"temp/multi_input_model_inference_guide.html#1","title":"1. \ucf5c\ubc31 \uae30\ubc18 \ube44\ub3d9\uae30 \ucd94\ub860","text":""},{"location":"temp/multi_input_model_inference_guide.html#c_8","title":"C++","text":"<pre><code>// \ucf5c\ubc31 \ud568\uc218 \ub4f1\ub85d\nie.RegisterCallback([](dxrt::TensorPtrs&amp; outputs, void* userArg) -&gt; int {\n    // \ucd9c\ub825 \ucc98\ub9ac\n    return 0;\n});\n\n// Dictionary format \ube44\ub3d9\uae30 \ucd94\ub860\nint jobId = ie.RunAsyncMultiInput(inputTensors, userArg);\n\n// Vector format \ube44\ub3d9\uae30 \ucd94\ub860\nint jobId = ie.RunAsyncMultiInput(inputPtrs, userArg);\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#python_9","title":"Python","text":"<pre><code># \ucf5c\ubc31 \ud568\uc218 \uc815\uc758\ndef callback_handler(outputs, user_arg):\n    # \ucd9c\ub825 \ucc98\ub9ac \ubc0f \uac80\uc99d\n    return 0\n\n# \ucf5c\ubc31 \ub4f1\ub85d\nie.register_callback(callback_handler)\n\n# Dictionary format \ube44\ub3d9\uae30 \ucd94\ub860\njob_id = ie.run_async_multi_input(input_tensors, user_arg=user_arg)\n\n# Vector format \ube44\ub3d9\uae30 \ucd94\ub860\njob_id = ie.run_async(input_list, user_arg=user_arg)\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#2","title":"2. \uac04\ub2e8\ud55c \ube44\ub3d9\uae30 \ucd94\ub860","text":""},{"location":"temp/multi_input_model_inference_guide.html#c_9","title":"C++","text":"<pre><code>// \ub2e8\uc77c \ubc84\ud37c \ube44\ub3d9\uae30 \ucd94\ub860\nint jobId = ie.RunAsync(inputPtr, userArg);\n\n// \uacb0\uacfc \ub300\uae30\nauto outputs = ie.Wait(jobId);\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#python_10","title":"Python","text":"<pre><code># \ub2e8\uc77c \ubc84\ud37c \ube44\ub3d9\uae30 \ucd94\ub860\njob_id = ie.run_async(input_buffer, user_arg=user_arg)\n\n# \uacb0\uacfc \ub300\uae30\noutputs = ie.wait(job_id)\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#_3","title":"\uc7a5\uce58 \uac80\uc99d","text":"<p>Multi-input \ubaa8\ub378\uc5d0 \ub300\ud55c NPU \uc7a5\uce58 \uac80\uc99d\ub3c4 \uc9c0\uc6d0\ub429\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_model_inference_guide.html#c_10","title":"C++","text":"<pre><code>// Dictionary format\nstd::map&lt;std::string, void*&gt; inputTensors;\ninputTensors[\"input1\"] = input1_data;\ninputTensors[\"input2\"] = input2_data;\n\nauto validationResults = ie.ValidateDeviceMultiInput(inputTensors, deviceId);\n\n// Vector format\nstd::vector&lt;void*&gt; inputPtrs = {input1_data, input2_data};\nauto validationResults = ie.ValidateDevice(inputPtrs, deviceId);\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#python_11","title":"Python","text":"<pre><code># Dictionary format\ninput_tensors = {\"input1\": input1_array, \"input2\": input2_array}\nvalidation_results = ie.validate_device_multi_input(input_tensors, device_id=0)\n\n# Vector format  \ninput_list = [input1_array, input2_array]\nvalidation_results = ie.validate_device(input_list, device_id=0)\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#_4","title":"\ucd9c\ub825 \uac80\uc99d","text":"<p>\uac15\ud654\ub41c \ucd9c\ub825 \uac80\uc99d \uae30\ub2a5\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4:</p>"},{"location":"temp/multi_input_model_inference_guide.html#_5","title":"\uac80\uc99d \ud56d\ubaa9","text":"<ol> <li>\ucd9c\ub825 \uc874\uc7ac \uc5ec\ubd80: None \ub610\ub294 \ube48 \ub9ac\uc2a4\ud2b8 \uac80\uc0ac</li> <li>\ub370\uc774\ud130 \ud0c0\uc785: numpy.ndarray \ud0c0\uc785 \uac80\uc99d</li> <li>\ud150\uc11c \ud06c\uae30: \ube48 \ud150\uc11c (size=0) \uac80\uc0ac</li> <li>\ud615\ud0dc \uc720\ud6a8\uc131: \uc720\ud6a8\ud558\uc9c0 \uc54a\uc740 shape \uac80\uc0ac</li> <li>\uc218\uce58 \uc720\ud6a8\uc131: NaN, Inf \uac12 \uac80\uc0ac (Python)</li> <li>\ud3ec\uc778\ud130 \uc720\ud6a8\uc131: null \ud3ec\uc778\ud130 \uac80\uc0ac (C++)</li> <li>\ubc30\uce58 \uad6c\uc870: \ubc30\uce58 \ucd9c\ub825\uc758 \uc62c\ubc14\ub978 \uc911\ucca9 \uad6c\uc870 \uac80\uc99d</li> </ol>"},{"location":"temp/multi_input_model_inference_guide.html#python_12","title":"\uac80\uc99d \uc608\uc81c (Python)","text":"<pre><code>def validate_outputs(outputs, expected_count, test_name):\n    # 1. \uae30\ubcf8 \uc874\uc7ac \uc5ec\ubd80 \uac80\uc0ac\n    if outputs is None or not isinstance(outputs, list):\n        return False\n\n    # 2. \ubc30\uce58/\ub2e8\uc77c \ucd9c\ub825 \uad6c\ubd84\n    is_batch = isinstance(outputs[0], list)\n\n    # 3. \uac01 \ud150\uc11c\ubcc4 \uc0c1\uc138 \uac80\uc99d\n    for output in outputs:\n        if not isinstance(output, np.ndarray):\n            return False\n        if output.size == 0:  # \ube48 \ud150\uc11c\n            return False\n        if len(output.shape) == 0:  # \uc798\ubabb\ub41c \ud615\ud0dc\n            return False\n        if np.any(np.isnan(output)) or np.any(np.isinf(output)):  # \uc218\uce58 \uc624\ub958\n            return False\n\n    return True\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide.html#_6","title":"\ud14c\uc2a4\ud2b8 \uc2dc\ub098\ub9ac\uc624","text":"<p>\ud604\uc7ac \uc608\uc81c \ucf54\ub4dc\ub294 \ub2e4\uc74c 10\uac00\uc9c0 \uc2dc\ub098\ub9ac\uc624\ub97c \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4:</p>"},{"location":"temp/multi_input_model_inference_guide.html#no>_output>_buffer","title":"\ub2e8\uc77c \ucd94\ub860 (No Output Buffer)","text":"<ol> <li>Dictionary Format (No Buffer): \ub515\uc154\ub108\ub9ac \ud615\ud0dc, \uc790\ub3d9 \ud560\ub2f9</li> <li>Vector Format (No Buffer): \ubca1\ud130 \ud615\ud0dc, \uc790\ub3d9 \ud560\ub2f9</li> <li>Auto-Split (No Buffer): \uc790\ub3d9 \ubd84\ud560, \uc790\ub3d9 \ud560\ub2f9</li> </ol>"},{"location":"temp/multi_input_model_inference_guide.html#with>_output>_buffer","title":"\ub2e8\uc77c \ucd94\ub860 (With Output Buffer)","text":"<ol> <li>Dictionary Format (With Buffer): \ub515\uc154\ub108\ub9ac \ud615\ud0dc, \uc0ac\uc6a9\uc790 \ubc84\ud37c</li> <li>Vector Format (With Buffer): \ubca1\ud130 \ud615\ud0dc, \uc0ac\uc6a9\uc790 \ubc84\ud37c</li> <li>Auto-Split (With Buffer): \uc790\ub3d9 \ubd84\ud560, \uc0ac\uc6a9\uc790 \ubc84\ud37c</li> </ol>"},{"location":"temp/multi_input_model_inference_guide.html#_7","title":"\ubc30\uce58 \ucd94\ub860","text":"<ol> <li>Batch Explicit: \uba85\uc2dc\uc801 \ubc30\uce58 \ud615\ud0dc</li> <li>Batch Flattened: \ud50c\ub798\ud2bc\ub41c \ubc30\uce58 \ud615\ud0dc (Python\ub9cc)</li> </ol>"},{"location":"temp/multi_input_model_inference_guide.html#_8","title":"\ube44\ub3d9\uae30 \ucd94\ub860","text":"<ol> <li>Async Callback: \ucf5c\ubc31 \uae30\ubc18 \ube44\ub3d9\uae30</li> <li>Simple Async: \uac04\ub2e8\ud55c \ube44\ub3d9\uae30</li> </ol>"},{"location":"temp/multi_input_model_inference_guide.html#_9","title":"\uad8c\uc7a5\uc0ac\ud56d","text":"<ol> <li>Dictionary Format \uc0ac\uc6a9: \uac00\uc7a5 \uba85\ud655\ud558\uace0 \uc2e4\uc218\uac00 \uc801\uc74c</li> <li>\uc785\ub825 \ud150\uc11c \uc815\ubcf4 \ud655\uc778: <code>GetInputTensorNames()</code> \ub4f1\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \uc694\uad6c\uc0ac\ud56d \ud655\uc778</li> <li>\ub370\uc774\ud130 \uc5f0\uc18d\uc131: Python\uc5d0\uc11c\ub294 C-contiguous \ubc30\uc5f4 \uc0ac\uc6a9 \uad8c\uc7a5</li> <li>\uba54\ubaa8\ub9ac \uad00\ub9ac: \ucd9c\ub825 \ubc84\ud37c\ub97c \ubbf8\ub9ac \ud560\ub2f9\ud558\uc5ec \uc131\ub2a5 \ud5a5\uc0c1</li> <li>\uc5d0\ub7ec \ucc98\ub9ac: \uc798\ubabb\ub41c \uc785\ub825 \ud615\ud0dc\ub098 \ud06c\uae30\uc5d0 \ub300\ud55c \uc608\uc678 \ucc98\ub9ac</li> <li>\ucd9c\ub825 \uac80\uc99d: \uac15\ud654\ub41c \ucd9c\ub825 \uac80\uc99d\uc73c\ub85c False Positive \ubc29\uc9c0</li> </ol>"},{"location":"temp/multi_input_model_inference_guide.html#_10","title":"\uc81c\uc57d\uc0ac\ud56d","text":"<ol> <li>\ube44\ub3d9\uae30 \ucd94\ub860: \ubc30\uce58 \ucd94\ub860\uc740 \uc9c0\uc6d0\ud558\uc9c0 \uc54a\uc74c (\ub2e8\uc77c \ucd94\ub860\ub9cc)</li> <li>\uc785\ub825 \uc21c\uc11c: Vector format \uc0ac\uc6a9 \uc2dc <code>GetInputTensorNames()</code> \uc21c\uc11c \uc900\uc218 \ud544\uc694</li> <li>\ub370\uc774\ud130 \ud0c0\uc785: \ubaa8\ub378\uc5d0\uc11c \uc694\uad6c\ud558\ub294 \ub370\uc774\ud130 \ud0c0\uc785\uacfc \uc77c\uce58\ud574\uc57c \ud568</li> <li>\ubc84\ud37c \ud06c\uae30: \uc785\ub825/\ucd9c\ub825 \ubc84\ud37c \ud06c\uae30\uac00 \ubaa8\ub378 \uc694\uad6c\uc0ac\ud56d\uacfc \uc77c\uce58\ud574\uc57c \ud568</li> <li>\uc7a5\uce58 \uac80\uc99d: Debug \ubaa8\ub4dc\ub85c \ucef4\ud30c\uc77c\ub41c \ubaa8\ub378\uc5d0\uc11c\ub9cc \uc9c0\uc6d0 </li> </ol>"},{"location":"temp/multi_input_model_inference_guide_example.html","title":"Multi input model inference guide example","text":"<p>Of course! Here is a detailed explanation of the multi-input examples in Markdown (<code>.md</code>) format, based on the C++ code you provided.</p>"},{"location":"temp/multi_input_model_inference_guide_example.html#dxrt>_multi-input>_inference>_examples","title":"DXRT Multi-Input Inference Examples","text":"<p>This document explains various methods for performing inference on multi-input models using the <code>dxrt::InferenceEngine</code>. The examples cover different input formats, synchronous and asynchronous execution, and batch processing.</p>"},{"location":"temp/multi_input_model_inference_guide_example.html#1>_model>_information","title":"1. Model Information","text":"<p>Before running inference, it's useful to inspect the model's properties. The <code>printModelInfo</code> function shows how to query the inference engine for details about the model's input and output tensors.</p> <ul> <li><code>ie.IsMultiInputModel()</code>: Checks if the loaded model has multiple inputs.</li> <li><code>ie.GetInputTensorCount()</code>: Gets the number of input tensors.</li> <li><code>ie.GetInputTensorNames()</code>: Retrieves the names of all input tensors.</li> <li><code>ie.GetInputTensorSizes()</code>: Gets the size (in bytes) of each input tensor.</li> <li><code>ie.GetOutputTensorNames()</code> / <code>ie.GetOutputTensorSizes()</code>: Provide similar information for output tensors.</li> </ul> <pre><code>void printModelInfo(dxrt::InferenceEngine&amp; ie) {\n    if (ie.IsMultiInputModel()) {\n        std::cout &lt;&lt; \"Input tensor count: \" &lt;&lt; ie.GetInputTensorCount() &lt;&lt; std::endl;\n        auto inputNames = ie.GetInputTensorNames();\n        auto inputSizes = ie.GetInputTensorSizes();\n        for (size_t i = 0; i &lt; inputNames.size(); ++i) {\n            std::cout &lt;&lt; \"  \" &lt;&lt; inputNames[i] &lt;&lt; \": \" &lt;&lt; inputSizes[i] &lt;&lt; \" bytes\" &lt;&lt; std::endl;\n        }\n    }\n}\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide_example.html#2>_synchronous>_single>_inference","title":"2. Synchronous Single Inference","text":"<p>These examples demonstrate different ways to run a single inference request synchronously.</p>"},{"location":"temp/multi_input_model_inference_guide_example.html#input>_formats","title":"Input Formats","text":""},{"location":"temp/multi_input_model_inference_guide_example.html#a>_dictionary>_format>_stdmapstdstring>_void","title":"A. Dictionary Format (<code>std::map&lt;std::string, void*&gt;</code>)","text":"<p>This is the most robust method. You provide a map where keys are the tensor names and values are pointers to the input data. This format is not sensitive to the order of tensors.</p> <ul> <li>API: <code>ie.RunMultiInput(inputTensors)</code></li> <li>Use Case: Recommended for clarity and to avoid errors from tensor reordering.</li> </ul> <pre><code>// Create input data\nstd::map&lt;std::string, void*&gt; inputTensors;\ninputTensors[\"input_1\"] = inputData1.data();\ninputTensors[\"input_2\"] = inputData2.data();\n\n// Run inference\nauto outputs = ie.RunMultiInput(inputTensors);\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide_example.html#b>_vector>_format>_stdvectorvoid","title":"B. Vector Format (<code>std::vector&lt;void*&gt;</code>)","text":"<p>You provide a vector of pointers to the input data. The order of pointers in the vector must match the order returned by <code>ie.GetInputTensorNames()</code>.</p> <ul> <li>API: <code>ie.RunMultiInput(inputPtrs)</code></li> <li>Use Case: When tensor order is known and fixed. Can be slightly more performant than the map-based approach due to less overhead.</li> </ul> <pre><code>// Create input data in the correct order\nstd::vector&lt;void*&gt; inputPtrs;\ninputPtrs.push_back(inputData1.data()); // Corresponds to first name in GetInputTensorNames()\ninputPtrs.push_back(inputData2.data()); // Corresponds to second name\n\n// Run inference\nauto outputs = ie.RunMultiInput(inputPtrs);\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide_example.html#c>_auto-split>_concatenated>_buffer","title":"C. Auto-Split Concatenated Buffer","text":"<p>You provide a single, contiguous buffer containing all input data concatenated together. The engine automatically splits this buffer into the correct tensor inputs based on their sizes. The concatenation order must match the order from <code>ie.GetInputTensorNames()</code>.</p> <ul> <li>API: <code>ie.Run(concatenatedInput.data())</code></li> <li>Use Case: Efficient when input data is already in a single block or when interfacing with systems that provide data this way.</li> </ul> <pre><code>// Create a single buffer with all input data concatenated\nauto concatenatedInput = createDummyInput(ie.GetInputSize());\n\n// Run inference\nauto outputs = ie.Run(concatenatedInput.data());\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide_example.html#output>_buffer>_management","title":"Output Buffer Management","text":"<p>For each synchronous method, you can either let the engine allocate output memory automatically or provide a pre-allocated buffer for performance gains.</p> <ul> <li> <p>Auto-Allocated Output (No Buffer Provided): Simpler to use. The engine returns smart pointers to newly allocated memory.</p> <pre><code>// Engine allocates and manages output memory\nauto outputs = ie.RunMultiInput(inputTensors);\n</code></pre> </li> <li> <p>User-Provided Output Buffer: More performant as it avoids repeated memory allocations. The user is responsible for allocating a buffer of size <code>ie.GetOutputSize()</code>.</p> <pre><code>// User allocates the output buffer\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// Run inference, placing results in the provided buffer\nauto outputs = ie.RunMultiInput(inputTensors, nullptr, outputBuffer.data());\n</code></pre> </li> </ul>"},{"location":"temp/multi_input_model_inference_guide_example.html#3>_synchronous>_batch>_inference","title":"3. Synchronous Batch Inference","text":"<p>For processing multiple inputs at once to maximize throughput, you can use the batch inference API. This is more efficient than running single inferences in a loop.</p> <ul> <li>API: <code>ie.Run(batchInputPtrs, batchOutputPtrs, userArgs)</code></li> <li>Input: A vector of pointers, where each pointer is a concatenated buffer for one sample in the batch.</li> <li>Output: A vector of pointers, where each pointer is a pre-allocated buffer for the corresponding sample's output.</li> </ul> <pre><code>int batchSize = 3;\nstd::vector&lt;void*&gt; batchInputPtrs;\nstd::vector&lt;void*&gt; batchOutputPtrs;\n\n// Prepare input and output buffers for each sample in the batch\nfor (int i = 0; i &lt; batchSize; ++i) {\n    // Each input is a full concatenated buffer\n    batchInputData[i] = createDummyInput(ie.GetInputSize());\n    batchInputPtrs.push_back(batchInputData[i].data());\n\n    // Pre-allocate output buffer for each sample\n    batchOutputData[i].resize(ie.GetOutputSize());\n    batchOutputPtrs.push_back(batchOutputData[i].data());\n}\n\n// Run batch inference\nauto batchOutputs = ie.Run(batchInputPtrs, batchOutputPtrs);\n</code></pre>"},{"location":"temp/multi_input_model_inference_guide_example.html#4>_asynchronous>_inference","title":"4. Asynchronous Inference","text":"<p>Asynchronous APIs allow you to submit inference requests without blocking the calling thread. The results are returned later via a callback function. This is ideal for applications that need to remain responsive, such as those with a user interface.</p> <ul> <li>APIs:<ul> <li><code>ie.RunAsyncMultiInput(inputTensors, userArg)</code></li> <li><code>ie.RunAsync(concatenatedInput.data(), userArg)</code></li> </ul> </li> <li>Callback Registration: <code>ie.RegisterCallback(callback_function)</code></li> </ul> <p>The <code>AsyncInferenceHandler</code> class demonstrates how to manage state across multiple asynchronous calls.</p> <ol> <li>Register a Callback: Provide a function that the engine will call upon completion of each async request. The callback receives the output tensors and a <code>userArg</code> pointer for context.</li> <li>Submit Requests: Call an <code>RunAsync</code> variant. This call returns immediately with a job ID.</li> <li>Process in Callback: The callback function is executed in a separate worker thread. Here, you can process the results. It's crucial to ensure thread safety if you modify shared data.</li> </ol> <pre><code>// 1. Create a handler and register its callback method\nAsyncInferenceHandler handler(asyncCount);\nie.RegisterCallback([&amp;handler](dxrt::TensorPtrs&amp; outputs, void* userArg) -&gt; int {\n    return handler.callback(outputs, userArg);\n});\n\n// 2. Submit multiple async requests in a loop\nfor (int i = 0; i &lt; asyncCount; ++i) {\n    void* userArg = reinterpret_cast&lt;void*&gt;(static_cast&lt;uintptr_t&gt;(i));\n    // Each call is non-blocking\n    ie.RunAsyncMultiInput(asyncInputTensors[i], userArg);\n}\n\n// 3. Wait for all callbacks to complete\nhandler.waitForCompletion();\n\n// 4. Clear the callback when done\nie.RegisterCallback(nullptr);\n</code></pre>"},{"location":"temp/profiler_guide_example.html","title":"Profiler guide example","text":""},{"location":"temp/profiler_guide_example.html#dxrt>_asynchronous>_inference>_profiler>_example>_guide","title":"DXRT Asynchronous Inference Profiler Example Guide","text":""},{"location":"temp/profiler_guide_example.html#1>_overview","title":"1. Overview","text":"<p>This document explains the asynchronous inference example code (<code>run_async_model_profiler</code>), which demonstrates how to measure the performance of a model using the <code>dxrt</code> library.</p> <p>This example showcases the following key features:</p> <ul> <li>Reading the model path and loop count from command-line arguments.</li> <li>Activating the profiler feature using <code>dxrt::Configuration</code>.</li> <li>Requesting inference in a non-blocking manner via <code>RunAsync</code>.</li> <li>Registering a callback function to handle the completion of asynchronous tasks.</li> <li>Using <code>std::condition_variable</code> to wait until all asynchronous tasks are finished.</li> <li>Calculating performance metrics such as total execution time, average latency, and frames per second (FPS).</li> </ul>"},{"location":"temp/profiler_guide_example.html#2>_core>_concepts","title":"2. Core Concepts","text":""},{"location":"temp/profiler_guide_example.html#asynchronous>_inference>_runasync","title":"Asynchronous Inference (<code>RunAsync</code>)","text":"<p>The <code>RunAsync</code> function submits an inference job to a background thread and returns immediately. This allows the main thread to continue with other tasks without waiting for the inference to complete, enhancing the application's responsiveness.</p>"},{"location":"temp/profiler_guide_example.html#callback>_mechanism>_registercallback","title":"Callback Mechanism (<code>RegisterCallback</code>)","text":"<p>When an asynchronous inference task is complete, a pre-registered function is needed to process the result. The function registered via <code>RegisterCallback</code> is automatically called each time a <code>RunAsync</code> task finishes. In this example, the callback function is used to count the number of completed tasks.</p>"},{"location":"temp/profiler_guide_example.html#synchronization>_stdcondition>_variable","title":"Synchronization (<code>std::condition_variable</code>)","text":"<p>After submitting all asynchronous inference requests, the main thread must wait until all jobs are actually finished to accurately measure performance. <code>std::condition_variable</code> and <code>std::mutex</code> are used to safely make the main thread wait until the callback function signals that all tasks are complete.</p>"},{"location":"temp/profiler_guide_example.html#profiler>_dxrtconfiguration","title":"Profiler (<code>dxrt::Configuration</code>)","text":"<p>DXRT provides a profiler feature that can analyze detailed performance information, such as the computation time for each layer of the NPU. In this example, the profiler is enabled via the <code>dxrt::Configuration</code> singleton object, and it is configured to display the results on the screen and save them to a file.</p>"},{"location":"temp/profiler_guide_example.html#3>_code>_walkthrough","title":"3. Code Walkthrough","text":""},{"location":"temp/profiler_guide_example.html#1>_initial>_setup>_and>_argument>_parsing","title":"1. Initial Setup and Argument Parsing","text":"<pre><code>int main(int argc, char* argv[])\n{\n    // ...\n    if ( argc &gt; 1 )\n    {\n        modelPath = argv[1];\n        if ( argc &gt; 2 ) \n        {\n            loop_count = std::stoi(argv[2]);\n        }\n    }\n    else\n    {\n        std::cout &lt;&lt; \"[Usage] run_async_model_profiler [dxnn-file-path] [loop-count]\" &lt;&lt; std::endl;\n        return -1;\n    }\n    // ...\n}\n</code></pre> <ul> <li>The program reads required arguments (model file path, loop count) from the command line.</li> <li>If the arguments are missing, it prints the usage instructions and exits.</li> </ul>"},{"location":"temp/profiler_guide_example.html#2>_enabling>_the>_profiler","title":"2. Enabling the Profiler","text":"<pre><code>// enable profiler\ndxrt::Configuration::GetInstance().SetEnable(dxrt::Configuration::ITEM::PROFILER, true);\ndxrt::Configuration::GetInstance().SetAttribute(dxrt::Configuration::ITEM::PROFILER, \n                                        dxrt::Configuration::ATTRIBUTE::PROFILER_SHOW_DATA, \"ON\");\ndxrt::Configuration::GetInstance().SetAttribute(dxrt::Configuration::ITEM::PROFILER, \n                                        dxrt::Configuration::ATTRIBUTE::PROFILER_SAVE_DATA, \"ON\");\n</code></pre> <ul> <li>It accesses the singleton object via <code>dxrt::Configuration::GetInstance()</code>.</li> <li>It calls <code>SetEnable</code> to activate the profiler feature (<code>ITEM::PROFILER</code>).</li> <li>It uses <code>SetAttribute</code> to configure the profiler to display data on the screen (<code>PROFILER_SHOW_DATA</code>) and save it to a file (<code>PROFILER_SAVE_DATA</code>).</li> </ul>"},{"location":"temp/profiler_guide_example.html#3>_preparing>_the>_inference>_engine>_and>_callback","title":"3. Preparing the Inference Engine and Callback","text":"<pre><code>std::mutex cv_mutex;\nstd::condition_variable cv;\n\ndxrt::InferenceEngine ie(modelPath);\n\nie.RegisterCallback([&amp;callback_count, &amp;loop_count, &amp;cv_mutex, &amp;cv] \n    (dxrt::TensorPtrs &amp;outputs, void *userArg) {\n\n    std::unique_lock&lt;std::mutex&gt; lock(cv_mutex);\n    callback_count++;\n    if ( callback_count == loop_count ) cv.notify_one();\n\n    return 0;\n});\n</code></pre> <ul> <li>A <code>mutex</code> and <code>condition_variable</code> are created for synchronization.</li> <li>An <code>dxrt::InferenceEngine</code> instance is created using the model path.</li> <li>A callback is registered using a lambda function.<ul> <li>Each time the callback is invoked, <code>callback_count</code> is incremented.</li> <li>When <code>callback_count</code> equals the total <code>loop_count</code>, <code>cv.notify_one()</code> is called to wake up the waiting main thread.</li> </ul> </li> </ul>"},{"location":"temp/profiler_guide_example.html#4>_executing>_asynchronous>_inference","title":"4. Executing Asynchronous Inference","text":"<pre><code>std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\nauto start = std::chrono::high_resolution_clock::now();\n\nfor(int i = 0; i &lt; loop_count; ++i)\n{\n    std::pair&lt;int, int&gt; *userData = new std::pair&lt;int, int&gt;(i, loop_count);\n    ie.RunAsync(inputPtr.data(), userData);\n    std::cout &lt;&lt; \"Inference request submitted with user_arg(\" &lt;&lt; i &lt;&lt; \")\" &lt;&lt; std::endl;\n}\n</code></pre> <ul> <li>A dummy input buffer of the appropriate size for the model is created.</li> <li>A timer is started.</li> <li>The <code>for</code> loop calls <code>ie.RunAsync()</code> as many times as specified by <code>loop_count</code>.</li> <li><code>RunAsync</code> returns immediately, so the loop executes quickly without waiting for inference to complete.</li> </ul> <p>Note: The <code>userData</code> in this example is allocated with <code>new</code> but is never released, which will cause a memory leak. In a real-world application, you must manage this memory, for example by calling <code>delete</code> within the callback function or by using smart pointers like <code>std::unique_ptr</code>.</p>"},{"location":"temp/profiler_guide_example.html#5>_waiting>_for>_completion>_and>_synchronization","title":"5. Waiting for Completion and Synchronization","text":"<pre><code>std::unique_lock&lt;std::mutex&gt; lock(cv_mutex);\ncv.wait(lock, [&amp;callback_count, &amp;loop_count] { \n    return callback_count == loop_count;\n});\n</code></pre> <ul> <li>At this point, the main thread stops and enters a waiting state.</li> <li><code>cv.wait()</code> will block until <code>cv.notify_one()</code> is called from the callback and the lambda condition (<code>callback_count == loop_count</code>) returns <code>true</code>.</li> </ul>"},{"location":"temp/profiler_guide_example.html#6>_performance>_measurement>_and>_result>_output","title":"6. Performance Measurement and Result Output","text":"<pre><code>auto end = std::chrono::high_resolution_clock::now();\n// ...\ndouble total_time = duration.count();\ndouble avg_latency = total_time / static_cast&lt;double&gt;(loop_count);\ndouble fps = 1000.0 / avg_latency;\n\nstd::cout &lt;&lt; \"-----------------------------------\" &lt;&lt; std::endl;\n// ... (print results)\n</code></pre> <ul> <li>Once the main thread is awakened (meaning all tasks are complete), the timer is stopped.</li> <li>Total time, average latency, and FPS are calculated and printed to the console.</li> </ul>"},{"location":"temp/profiler_guide_example.html#7>_exception>_handling","title":"7. Exception Handling","text":"<pre><code>catch (const dxrt::Exception&amp; e) { /* ... */ }\ncatch (const std::exception&amp; e) { /* ... */ }\ncatch(...) { /* ... */ }\n</code></pre> <ul> <li>The <code>try-catch</code> block is used to handle exceptions from the <code>dxrt</code> library or other standard exceptions, preventing the program from crashing unexpectedly.</li> </ul>"},{"location":"temp/profiler_guide_example.html#4>_how>_to>_build>_and>_run","title":"4. How to Build and Run","text":"<ol> <li> <p>Build     In an environment where the DXRT SDK is installed, build the code using a C++ compiler (e.g., g++). Replace <code>${DXRT_SDK_PATH}</code> with the actual path to your SDK installation.</p> <pre><code>g++ -std=c++17 -o run_async_model_profiler run_async_model_profiler.cpp \\\n    -I${DXRT_SDK_PATH}/include -L${DXRT_SDK_PATH}/lib -ldxrt\n</code></pre> </li> <li> <p>Run     Execute the compiled binary, passing the model file path and the loop count as arguments.</p> <pre><code>./run_async_model_profiler /path/to/your_model.dxnn 100\n</code></pre> <ul> <li><code>/path/to/your_model.dxnn</code>: The path to the model file to be used for inference.</li> <li><code>100</code>: The number of times to repeat the inference (defaults to 1 if omitted).</li> </ul> </li> </ol>"},{"location":"temp/profiler_guide_example.html#5>_expected>_output","title":"5. Expected Output","text":"<p>When you run the program, you can expect to see the following output:</p> <ol> <li>Log messages for each submitted inference request.</li> <li>Since the profiler is enabled, a detailed performance analysis for each layer of the model will be printed to the screen.</li> <li>The final calculated performance metrics after all inference tasks are complete.</li> <li>A profiling result file (e.g., <code>profile_data_... .json</code>) will be generated in the execution directory.</li> </ol> <pre><code>Inference request submitted with user_arg(0)\nInference request submitted with user_arg(1)\n...\nInference request submitted with user_arg(99)\n\n(Profiler data is printed here...)\n-----------------------------------\nDevice #0, NPU Core #0, Model: /path/to/your_model.dxnn\n+-----------+-----------------+-----------+\n| Layer     | Operation       | Time (ms) |\n+-----------+-----------------+-----------+\n| conv1     | CONV            |   0.123   |\n| pool1     | MAX_POOL        |   0.045   |\n...\n+-----------+-----------------+-----------+\n| Total                       |   1.234   |\n+-----------+-----------------+-----------+\n\n-----------------------------------\nTotal Time: 125.678 ms\nAverage Latency: 1.25678 ms\nFPS: 795.68 frames/sec\nTotal callback-count / loop-count: 100 / 100 (Success)\n-----------------------------------\n</code></pre>"}]}