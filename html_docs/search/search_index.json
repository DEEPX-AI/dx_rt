{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"docs/01_DXNN_Runtime_Overview.html","title":"DXNN Runtime Overview","text":"<p>This chapter provides an overview of the DEEPX SDK architecture and explains each core component and its role in the AI development workflow.  </p>"},{"location":"docs/01_DXNN_Runtime_Overview.html#deepx>_sdk>_architecture","title":"DEEPX SDK Architecture","text":"<p>  Figure. DEEPX SDK Architecture    </p> <p>DEEPX SDK is an all-in-one software development platform that streamlines the process of compiling, optimizing, simulating, and deploying AI inference applications on DEEPX NPUs (Neural Processing Units). It provides a complete toolchain, from AI model creation to runtime deployment, optimized for edge and embedded systems, enabling developers to build high-performance AI applications with minimal effort.  </p> <p>DX-COM is the compiler in the DEEPX SDK that converts a pre-trained ONNX model and its associated configuration JSON file into a hardware-optimized .dxnn binary for DEEPX NPUs. The ONNX file contains the model structure and weights, while the JSON file defines pre/post-processing settings and compilation parameters. DX-COM provides a fully compiled .dxnn file, optimized for low-latency and high-efficient inference on DEEPX NPU.  </p> <p>DX-RT is the runtime software responsible for executing ,dxnn models on DEEPX NPU hardware. DX-RT directly interacts with the DEEPX NPU through firmware and device drivers, using PCIe interface for high-speed data transfer between the host and the NPU, and provides C/C++ and Python APIs for application-level inference control. DX-RT offers a complete runtime environment, including model loading, I/O buffer management, inference execution, and real-time hardware monitoring.  </p> <p>DX ModelZoo is a curated collection of pre-trained neural network models optimized for DEEPX NPU, designed to simplify AI development for DEEPX users. It includes pre-trained ONNX models, configuration JSON files, and pre-compiled DXNN binaries, allowing developers to rapidly test and deploy applications. DX ModelZoo also provides benchmark tools for comparing the performance of quantized INT8 models on DEEPX NPU with full-precision FP32 models on CPU or GPU.  </p> <p>DX-STREAM is a custom GStreamer plugin that enables real-time streaming data integration into AI inference applications on DEEPX NPU. It provides a modular pipeline framework with configurable elements for preprocessing, inference, and postprocessing, tailored to vision AI work. DX-Stream allows developers to build flexible, high-performance applications for use cases such as video analytics, smart cameras, and edge AI systems.  </p> <p>DX-APP is a sample application that demonstrates how to run compiled models on actual DEEPX NPU using DX-RT. It includes ready-to-use code for common vision tasks such as object detection, face recognition, and image classification. DX-APP helps developers quickly set up the runtime environment and serves as a template for building and customizing their own AI applications.  </p>"},{"location":"docs/01_DXNN_Runtime_Overview.html#inference>_flow>_of>_dx-rt","title":"Inference Flow of DX-RT","text":"<p>Here is the inference flow of DX-RT.</p> <p>  Figure. Inference Flow of DXNN Runtime    </p> <p>This figure illustrates the inference workflow of the DXNN Runtime SDK, which integrates OpenCV-based input/output handling with efficient NPU-accelerated model execution.</p> <p>Input &amp; Pre-Processing Input data\u2014such as images, camera feeds, or video\u2014is captured using OpenCV. The data is then passed through a Pre-Processing module, which transforms it into \\input tensors suitable for the model.  </p> <p>Feeding Input to the Inference Engine The pre-processed input tensors are fed into the InferenceEngine along with the compiled model (.dxnn). Before execution, you must configure the InferenceOption, which specifies the target device and available resources.  </p> <p>Model Execution The InferenceEngine is the core component of the DXNN Runtime SDK. It: - Initializes and controls the NPU device - Manages memory for input/output tensors - Schedules inference tasks across NPU and CPU, optimizing their interaction for real-time performance</p> <p>Post-Processing &amp; Display The output tensors are processed to a Post-Processing stage, typically involving OpenCV for decoding, formatting, or visualization. Finally, the results are displayed or forwarded to the next processing step. </p>"},{"location":"docs/02_Installation_on_Linux.html","title":"Installation on Linux","text":"<p>This chapter describes the system requirements, source file structure, and the installation instructions for setting up DX-RT on a Linux-based host system.  </p> <p>After you check the system requirements, follow these instructions.  </p> <ul> <li>System Requirement Check  </li> <li>Build Environment Setup  </li> <li>Source File Structure Check  </li> <li>Framework Build  </li> <li>Linux Device Driver Installation  </li> <li>Python Package Installation  </li> <li>Service Registration  </li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#system>_requirements","title":"System Requirements","text":"<p>This section describes the hardware and software requirements for running DX-RT on Linux.</p> <p>Hardware and Software Requirements </p> <ul> <li>CPU: aarch64, x86_64, riscv64</li> <li>RAM: 8GB RAM (16GB RAM or higher is recommended)</li> <li>Storage: 4GB or higher available disk space</li> <li>OS: Ubuntu 20.04 / 22.04 / 24.04 (x64 / aarch64 / riscv64)</li> <li>Hardware: The system must support connection to an M1 M.2 module with the M.2 interface on the host PC. </li> </ul> <p>  Figure. DX-M1 M.2 Module    </p>"},{"location":"docs/02_Installation_on_Linux.html#build>_environment>_setup","title":"Build Environment Setup","text":"<p>DEEPX provides an installation shell script to set up the DX-RT build environment. You can install the entire toolchain installation or perform a partial installation as necessary.</p> <p>DX-RT supports the Target OS of Ubuntu 18.04, Ubuntu 20.04, Ubuntu 22.04, and Ubuntu 24.04. </p> <p>Installation of DX-RT  To install the full DX-RT toolchain, use the following commands.  </p> <pre><code>$ cd dx_rt\n$ ./install.sh --all\n</code></pre> <p>Here are the available <code>install.sh</code> options.  <pre><code>  ./install.sh [ options ]\n    --help            Shows help message\n    --arch [x86_64, aarch64]\n                      Sets target CPU architecture\n    --dep             Installs build dependencies : cmake, gcc, ninja, etc..\n    --onnxruntime     (Optional) Installs onnxruntime library\n    --all             Installs architecture + dependency + onnxruntime library\n</code></pre></p> <p>Installation with ONNX Runtime Use the ONNX Runtime option if you need to offload certain neural network (NN) operations to the CPU that are not supported by the NPU.  </p> <p>We recommend using ONNX Runtime linux x64 version more than v1.20.1. <pre><code>https://github.com/microsoft/onnxruntime/releases/download/v1.20.1/onnxruntime-linux-x64-1.20.1.tgz\n$ sudo tar -xvzf onnxruntime-linux-x64-1.20.1.tgz -C /usr/local --strip-components=1\n$ sudo ldconfig\n</code></pre></p> <p>To install the ONNX Runtime library, run the following command. <pre><code>./install.sh --onnxruntime\n</code></pre></p> <p>Installation for a Specific CPU Architecture The DX-RT targets the x86_64 architecture. If you\u2019re compiling for another architecture (e.g., aarch64), specify it using the <code>--arch</code> option.  <pre><code>./install.sh --arch aarch64 --onnxruntime\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#source>_file>_structure","title":"Source File Structure","text":"<p>The DX-RT source directory is organized as follows.  You can install the full toolchain using the <code>install.sh</code>, and the build and library using <code>build.sh</code>.  </p> <pre><code>\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 cli\n\u251c\u2500\u2500 build.sh\n\u251c\u2500\u2500 build_x86_64\n\u251c\u2500\u2500 build_aarch64\n\u251c\u2500\u2500 cmake\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 examples\n\u251c\u2500\u2500 extern\n\u251c\u2500\u2500 install.sh\n\u251c\u2500\u2500 lib\n\u251c\u2500\u2500 python_package\n\u251c\u2500\u2500 sample\n\u251c\u2500\u2500 service\n\u2514\u2500\u2500 tool\n</code></pre> <ul> <li><code>assets</code>: Images for documentation</li> <li><code>bin</code>: Compiled binary executables</li> <li><code>cli</code>: Command-line application source code</li> <li><code>build.sh</code>: Shell script for building the framework</li> <li><code>build_arch</code>: Build outputs for aarch64 architecture</li> <li><code>cmake</code>: CMake scripts for build configuration</li> <li><code>docs</code>: Markdown documents </li> <li><code>examples</code>: Inference example files</li> <li><code>extern</code>: Third-party libraries</li> <li><code>install.sh</code>: Shell script for toolchain installation</li> <li><code>lib</code>: DX-RT library sources</li> <li><code>python_package</code>: Python modules for DX-RT</li> <li><code>sample</code>: Sample input files for demo apps</li> <li><code>service</code>: Service unit files for runtime management</li> <li><code>tool</code>: Profiler result visualization tools</li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#framework>_build>_on>_linux","title":"Framework Build on Linux","text":"<p>After compiling the DX-RT environment setup, you can build the framework using the provided <code>build.sh</code> shell script.</p>"},{"location":"docs/02_Installation_on_Linux.html#framework>_source>_build","title":"Framework Source Build","text":"<p>DEEPX supports the default target CPU architecture as x86_64, aarch64.  </p> <p>The build script also supports options for build cleaning, specifying build type, and installing libraries to the system paths.  </p> <p>Build Instructions To build the DX-RT framework, run the following command. <pre><code>$ cd dx_rt\n$ ./build.sh\n</code></pre></p> <p>Here are the <code>build.sh</code> options and their descriptions. <pre><code>./build.sh [ options ]\n    --help       Shows help message\n    --clean      Cleans previous build artifacts\n    --verbose    Shows full build commands during execution\n    --type [Release, Debug, RelWithDebInfo]\n                 Specifies the cmake build type\n    --arch [x86_64, aarch64]\n                 Sets target CPU architecture\n    --install &lt;path&gt;\n                 Sets the installation path for built libraries\n    --uninstall  Removes installed DX-RT files\n    --clang      Compiles using clang\n</code></pre></p> <p>Example. Build with <code>clean</code> Option To clean existing build files before rebuilding. <pre><code>$ ./build.sh --clean\n</code></pre></p> <p>Example. Build with <code>library</code> Option To install build library files to <code>/usr/local</code>.  <pre><code># default path is /usr/local\n$ ./build.sh --install /usr/local\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#options>_for>_build>_target","title":"Options for Build Target","text":"<p>DX-RT supports configuration  build targets, allowing you to enable or disable option features such as  ONNX Runtime, Python API, multi-process service support, and shared library builds.  </p> <p>You can configure these options by editing the following file: <code>cmake/dxrt.cfg.cmake</code> </p> <p>Here are the available options for building targets. <pre><code>option(USE_ORT \"Use ONNX Runtime\" OFF)\noption(USE_PYTHON \"Use Python\" OFF)\noption(USE_SERVICE \"Use Service\" OFF)\noption(USE_SHARED_DXRT_LIB \"Build for DX-RT Shared Library\" ON)\n</code></pre></p> <ul> <li><code>USE_ORT</code>: Enables ONNX Runtime for NN (neural network) operations that NPU does not support</li> <li><code>USE_PYTHON</code>: Enables Python API support</li> <li><code>USE_SERVICE</code>: Enables service for multi-process support </li> <li><code>USE_SHARED_DXRT_LIB</code>: Builds DX-RT as shared library (default: ON)</li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#build>_guide>_for>_cross-compile","title":"Build Guide for Cross-compile","text":"<p>Setup Files for Cross-compile  DEEPX supports cross-compilation for the following default target CPU Architecture: x86_64, aarch64. DEEPX supports  the default target CPU architecture as x86_64.  </p> <p>Toolchain Configuration To cross-compile for a specific target, configure the toolchain file. <code>cmake/toolchain.&lt;CMAKE_SYSTEM_PROCESSOR&gt;.cmake</code></p> <p>Example To cross-compile files for aarch64. <pre><code>SET(CMAKE_C_COMPILER /usr/bin/aarch64-linux-gnu-gcc )\nSET(CMAKE_CXX_COMPILER /usr/bin/aarch64-linux-gnu-g++ )\nSET(CMAKE_LINKER /usr/bin/aarch64-linux-gnu-ld )\nSET(CMAKE_NM /usr/bin/aarch64-linux-gnu-nm )\nSET(CMAKE_OBJCOPY /usr/bin/aarch64-linux-gnu-objcopy )\nSET(CMAKE_OBJDUMP /usr/bin/aarch64-linux-gnu-objdump )\nSET(CMAKE_RANLIB /usr/bin/aarch64-linux-gnu-ranlib )\n</code></pre></p> <p>Non Cross-compile Case (Build on Host) To build and install DX-RT on the host system, run the following command. <pre><code>./build.sh --install /usr/local\n</code></pre></p> <p>Recommended install path: <code>/usr/local</code> (commonly included in OS search paths)  </p> <p>Cross-compile Case (Build for Target Architecture) Cross-compile for a specific architecture, run the following command. <pre><code>./build.sh --arch &lt;target_cpu&gt;\n</code></pre></p> <p>Here are the <code>build.sh</code> options and their descriptions. <pre><code>./build.sh [ options ]\n   --help   Shows help message\n   --clean  Cleans previous build artifacts\n   --verbose    Shows full build commands during execution\n   --type   Specifies the cmake build type : [ Release, Debug, RelWithDebInfo ]\n   --arch   Sets target CPU architecture : [ x86_64, aarch64, riscv64 ]\n   --install    Installs build libraries \n   --uninstall  Removes installed DX-RT files\n</code></pre></p> <p>Here are the examples of cross-compile cases. <pre><code>./build.sh --arch aarch64\n./build.sh --arch x86_64\n</code></pre></p> <p>Output Directory After a successful build, output binaries is located under <code>&lt;build directory&gt; /bin/</code> <pre><code>&lt;build directory&gt;/bin/\n \u251c\u2500\u2500 dxrtd\n \u251c\u2500\u2500 dxrt-cli\n \u251c\u2500\u2500 parse_model\n \u251c\u2500\u2500 run_model\n \u2514\u2500\u2500 examples\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#linux>_device>_driver>_installation","title":"Linux Device Driver Installation","text":"<p>After building the DX-RT framework, you can install the Linux device driver for M1 AI Accelerator (NPU).  </p>"},{"location":"docs/02_Installation_on_Linux.html#prerequisites","title":"Prerequisites","text":"<p>Before installing the Linux device driver, you should check that the accelerator device is properly recognized by the system.  </p> <p>To check PCIe device recognition, run the following command. <pre><code>$ lspci -vn | grep 1ff4\n0b:00.0 1200: 1ff4:0000\n</code></pre></p> <p>Note. If there is no output, the PCIe link is not properly connected. Please check the physical connection and system BIOS settings.  </p> <p>Optional. Display the DEEPX name in <code>lspci</code>. If you want to display the DEEPX name in <code>lspci</code>, you can modify the PCI DB. (Only for Ubuntu)  </p> <p>To display the DeepX device name, run the following command. <pre><code>$ sudo update-pciids\n$ lspci\n...\n0b:00.0 Processing accelerators: DEEPX Co., Ltd. DX_M1\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#linux>_device>_driver>_structure","title":"Linux Device Driver Structure","text":"<p>The DX-RT Linux device driver source is structured to support flexible builds across devices, architectures, and modules. The directory layout is as follows. <pre><code>- .gitmodules\n\n- [modules]\n    |\n    - device.mk     \n    - kbuild\n    - Makefile\n    - build.sh\n    - [rt]\n        - Kbuild\n    - [pci_deepx] : submodule\n        - Kbuild\n\n- [utils] : submodule\n</code></pre></p> <ul> <li><code>device.mk</code>: Device configuration file</li> <li><code>kbuild</code>: Top-level build rules</li> <li><code>Makefile</code>: Build entry point</li> <li><code>build.sh</code>: Build automation script</li> <li><code>rt</code>: Runtime driver source (<code>dxrt_driver.ko</code>)</li> <li><code>pci_deepx</code>: PCIe DMA driver (<code>submodule, dx_dma.ko</code>)</li> <li><code>utils</code>: Supporting utilities (<code>submodule</code>)</li> </ul> <p>Here are the descriptions of the key components.  </p> <p><code>device.mk</code> Defines supported device configuration.  </p> <p>To build for a specific device, run the following command. <pre><code>$ make DEVICE=[device]\n</code></pre></p> <p>For example, in the case of a device like M1, you should select a submodule, such as PCIe, that has a dependency on M1. <pre><code>$ make DEVICE=m1 PCIE=[deepx]\n</code></pre></p> <p><code>kbuild</code> Linux kernel build configuration file for each module directory. It instructs the kernel build system on how to compile driver modules.  </p> <p><code>build.sh</code> Shell script to streamline the build process. It runs the Makefile with common options. </p> <p>Here are the options for <code>build.sh</code>. <pre><code>Usage:\nUsage:\n   build.sh &lt;options&gt;\n\noptions:\n   -d, --device   [device]      select target device: m1\n   -m, --module   [module]      select PCIe module: deepx\n   -k, --kernel   [kernel dir]  'KERNEL_DIR=[kernel dir]', The directory where the \n                                kernel source is located \n                                default: /lib/modules/6.5.0-18-generic/build)\n   -a, --arch     [arch]        set 'ARCH=[arch]' Target CPU architecture for \n                                cross-compilation, default: x86_64\n   -t, --compiler [cross tool]  'CROSS_COMPILE=[cross tool]' cross compiler binary, \n                                e.g aarch64-linux-gnu-\n   -i, --install  [install dir] 'INSTALL_MOD_PATH=[install dir]', module install \n                                directory install to: \n                                [install dir]/lib/modules/[KERNELRELEASE]/extra/\n   -c, --command  [command]     clean | install | uninstall\n                                - uninstall: Remove the module files installed \n                                on the host PC.\n   -j, --jops     [jobs]        set build jobs\n   -f, --debug    [debug]       set debug feature [debugfs | log | all]\n   -v, --verbose                build verbose (V=1)\n   -h, --help                   show this help\n</code></pre></p> <p>The build process generates the following kernel modules.  </p> <ul> <li> <p><code>modules/rt</code> -&gt; <code>dxrt_driver.ko</code>     : a core runtime driver for M1 NPU devices. This is responsible for system-level communication, memory control, and device command execution. </p> </li> <li> <p><code>modules/pci_deepx</code> -&gt; <code>dx_dma.ko</code>     : PCIe DMA (Direct Memory Access) kernel module for high-speed data transfer between host and the M1 device. This enables efficient data movement with minimal CPU overhead, ideal for real-time and data intensive AI workloads. </p> </li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#linux>_device>_driver>_build","title":"Linux Device Driver Build","text":"<p>After completing the environment setup of the DXNN Linux Device Driver, you can build the kernel modules using either the make(Makefile) or <code>build.sh</code> script. Both methods are supported by DEEPX.  </p> <p>Option 1. Build Using <code>Makefile</code> </p> <p><code>build</code> <pre><code>e.g $ cd modules\ne.g $ make DEVICE=m1 PCIE=deepx\n</code></pre></p> <p><code>clean</code> <pre><code>e.g $ cd modules\ne.g $ make DEVICE=m1 PCIE=deepx clean\n</code></pre></p> <p><code>install</code> Installs the driver to: <code>/lib/modules/$(KERNELRELEASE)/extra/</code> <pre><code>e.g $ cd modules\ne.g $ make DEVICE=m1 PCIE=deepx install\n</code></pre></p> <p>Option 2. Build Using <code>build.sh</code> </p> <p>Use this method if your system supports self-compiling kernel modules (<code>.ko</code> files).</p> <p><code>build</code> <pre><code>e.g $ ./build.sh -d m1 -m deepx\n(Default device: m1, PCI3 module: deepx)\n</code></pre></p> <p><code>clean</code> <pre><code>e.g $ ./build.sh -c clean\n</code></pre></p> <p><code>install</code> Installs the driver to: <code>/lib/modules/$(KERNELRELEASE)/extra/</code> <pre><code>e.g $ sudo ./build.sh -c install\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#auto-loading>_modules>_at>_boot>_time","title":"Auto-Loading Modules at Boot Time","text":"<p>DEEPX allows kernel modules to be automatically loaded at system boot, either through manual setup or using the <code>build.sh</code> script.  </p> <p>Manual Installation Method </p> <p>Step 1. Install Kernel Modules Installs modules to: <code>/lib/modules/$(KERNELRELEASE)/extra/</code> <pre><code>make DEVICE=m1 PCIE=deepx install\n</code></pre></p> <p>Step 2. Update Module Dependencies Updates: <code>/lib/modules/$(KERNELRELEASE)/modules.dep</code> <pre><code>$ sudo depmod -A\n</code></pre></p> <p>Step 3. Add Module Confiduration Copy the preconfigured module config file. <pre><code>$ sudo cp modules/dx_dma.conf /etc/modprobe.d/\n</code></pre></p> <p>This ensures the modules (<code>dx_dma</code>) are auto-loaded on boot.  </p> <p>Step 4. Test with modprobe To verify the correct installation. <pre><code>$ sudo modprobe dx_dma\n$ lsmod\n  dxrt_driver            40960  0\n  dx_dma                176128  1 dxrt_driver\n</code></pre></p> <p>Automated Installation Using <code>build.sh</code> The <code>build.sh</code> script automates installation and setup, including dependency updates and modprobe configuration.  </p> <p>Run the following command <pre><code>$ sudo ./build.sh -d m1 -m deepx -c install\n- DEVICE        : m1\n- PCIE          : deepx\n- MODULE CONF   : /.../rt_npu_linux_driver/modules/dx_dma.conf\n- ARCH (HOST)   : x86_64\n- KERNEL        : /lib/modules/5.15.0-102-generic/build\n- INSTALL       : /lib/modules/5.15.0-102-generic/extra/\n\n *** Build : install ***\n$ make DEVICE=m1 PCIE=deepx install\n\nmake -C /lib/modules/5.15.0-102-generic/build M=/home/jhk/deepx/dxrt/module/rt_npu_linux_driver/modules  modules_install\n ....\n - SUCCESS\n\n *** Update : /lib/modules/5.15.0-102-generic/modules.dep ***\n $ depmod -A\n $ cp /home/jhk/deepx/dxrt/module/rt_npu_linux_driver/modules/dx_dma.conf /etc/modprobe.d/\n</code></pre></p> <p>Uninstalling Modules To completely remove the installed modules and configs. <pre><code>$ ./build.sh -d m1 -m deepx -c uninstall\n- DEVICE        : m1\n- PCIE          : deepx\n- MODULE CONF   : /.../rt_npu_linux_driver/modules/dx_dma.conf\n- ARCH (HOST)   : x86_64\n- KERNEL        : /lib/modules/5.15.0-102-generic/build\n- INSTALL       : /lib/modules/5.15.0-102-generic/extra/\n\n *** Remove : /lib/modules/5.15.0-102-generic/extra ***\n $ rm -rf /lib/modules/5.15.0-102-generic/extra/pci_deepx\n $ rm -rf /lib/modules/5.15.0-102-generic/extra/rt\n\n *** Remove : /etc/modprobe.d ***\n $ rm /etc/modprobe.d/dx_dma.conf\n\n *** Update : /lib/modules/5.15.0-102-generic/modules.dep ***\n $ depmod\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#python>_package>_installation","title":"Python Package Installation","text":"<p>DEEPX provides a Python package for DX-RT, available under the module name dx-engine. It supports Python 3.8 or later and allows you to interface with DX-RT in Python-based applications. </p> <p>Installation Steps 1. Navigate to the python_package directory.  <pre><code>$ cd python_package\n</code></pre></p> <p>2. Install the package <pre><code>$ pip install .\n</code></pre></p> <p>3. Verify the installation <pre><code>$ pip list | grep dx\ndx-engine          1.0.0\n</code></pre></p> <p>For details on using DX-RT with Python, refer to Section 6.2 Python in 6. Programming Guide.</p>"},{"location":"docs/02_Installation_on_Linux.html#service>_registration","title":"Service Registration","text":"<p>DX-RT supports multi-process operation through the background service (<code>dxrtd daemon</code>). To enable the multi-process feature, you must build the Runtime with Service support and the service must be registered in the system below.  </p> <p>Note.   - DX-RT must be built with <code>USE_SERVICE=ON</code>. (default setting)   - DX-RT must be registered and managed as a system service using <code>systemd</code>.  </p> <p>Registering and Running the DX-RT Service 1. Modify the service unit file.   Ensure the ExecStart path is correctly configured. <pre><code>$ vi ./service/dxrt.service\n</code></pre></p> <p>2. Copy the service file to the system folder. <pre><code>$ sudo cp ./service/dxrt.service /etc/systemd/system\n</code></pre></p> <p>3. Start the service. <pre><code>$ sudo systemctl start dxrt.service\n</code></pre></p> <p>Service Management Commands <pre><code>$ sudo systemctl stop dxrt.service          # Stop the service\n$ sudo systemctl status dxrt.service        # Check service status\n$ sudo systemctl restart dxrt.service       # Restart the service\n$ sudo systemctl enable dxrt.service        # Enable on boot\n$ sudo systemctl disable dxrt.service       # Disable on boot\n$ sudo journalctl -u dxrt.service           # View service logs\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#sanity>_check","title":"Sanity Check","text":"<p>The Sanity Check is a script used to quickly verify that a driver has been installed correctly and that the device is recognized properly.</p> <pre><code>$ sudo ./SanityCheck.sh\n============================================================================\n==== Sanity Check Date : DATE ====\nLog file location : .../dx_rt/dx_report/sanity/result/sanity_check_result_[date]_[hh/mm/ss].log\n\n==== PCI Link-up Check ====\n[OK] Vendor ID 1ff4 is present in the PCI devices.(num=2)\n==== Device File Check ====\n[OK] /dev/dxrt0 exists.\n[OK] /dev/dxrt0 is a character device.\n[OK] /dev/dxrt0 has correct permissions (0666).\n[OK] /dev/dxrt1 exists.\n[OK] /dev/dxrt1 is a character device.\n[OK] /dev/dxrt1 has correct permissions (0666).\n==== Kernel Module Check ====\n[OK] dxrt_driver module is loaded.\n[OK] dx_dma module is loaded.\n[OK] PCIe 02:00.0 driver probe is success.\n[OK] PCIe 07:00.0 driver probe is success.\n\n============================================================================\n** Sanity check PASSED!\n============================================================================\n</code></pre>"},{"location":"docs/03_Installation_on_Windows.html","title":"Installation on Windows","text":"<p>This chapter describes the instructions for installing and using DX-RT on a Windows system.  </p>"},{"location":"docs/03_Installation_on_Windows.html#system>_requirements","title":"System Requirements","text":"<p>This section describes the hardware and software requirements for running DX-RT on Windows.  </p> <ul> <li>RAM: 8GB RAM (16GB RAM or higher is recommended)</li> <li>Storage: 4GB or higher available disk space</li> <li>OS: Windows 10 / 11</li> <li>Python: Version 3.8 or higher (for Python module support)</li> <li>Compiler: Visual Studio 2022 (required for building C++ examples)</li> <li>Hardware: The system must support connection to an M1 M.2 module with the M.2 interface on the host PC.  </li> </ul> <p>The current version only supports Single-process and does not support Multi-process.  </p> <p>  Figure. DX-M1 M.2 Module    </p>"},{"location":"docs/03_Installation_on_Windows.html#execute>_installer","title":"Execute Installer","text":"<p>DEEPX provides the Windows installer executable file for DX-RT.  </p> <ul> <li><code>DXNN_Runtime_v[version]_windows_[architecture].exe</code> </li> </ul> <p>Here is an example of the execution file.  </p> <ul> <li><code>DXNN_Runtime_v2.X.X_windows_amd64.exe</code> </li> </ul> <p>Default Directory Path </p> <ul> <li>'C:/DevTools/DXNN/dxrt_v[version]'  </li> </ul> <p>Once you install the exe file, the driver will be installed automatically. So you can verify the installation via Device Manager under DEEPX_DEVICE.  </p> <p>Note. If Visual Studio 2022 is not installed, you may be prompted to install the Microsoft Visual C++ Redistributable (<code>VC_redist.x64.exe</code>) using administrator permissions.  </p>"},{"location":"docs/03_Installation_on_Windows.html#file>_structure","title":"File Structure","text":"<pre><code>\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 drivers\n\u251c\u2500\u2500 examples\n\u251c\u2500\u2500 firmware\n\u251c\u2500\u2500 include\n\u251c\u2500\u2500 lib\n\u2514\u2500\u2500 python_package\n</code></pre> <ul> <li><code>bin</code>: Compiled binary executables</li> <li><code>docs</code>: Markdown documents </li> <li><code>examples</code>: Inference example files</li> <li><code>include</code>: Header files for DX-RT libraries</li> <li><code>lib</code>: Pre-built DX-RT libraries</li> <li><code>python_package</code>: Python modules for DX-RT</li> <li><code>sample</code>: Sample input files for demo apps</li> <li><code>service</code>: Service unit files for runtime management</li> <li><code>tool</code>: Profiler result visualization tools</li> </ul>"},{"location":"docs/03_Installation_on_Windows.html#running>_examples","title":"Running Examples","text":"<p>DX-RT includes sample programs in both C++ and Python.  </p>"},{"location":"docs/03_Installation_on_Windows.html#building>_c>_examples","title":"Building C++ Examples","text":"<p>Visual Studio 2022 should be installed on your PC.  </p> <p>1. Open the solution file in the following location. <code>examples\\&lt;example-name&gt;\\msvc\\&lt;example-name&gt;.sln</code></p> <p>2. In Visual Studio, Click Rebuild Solution.  </p> <p>Once the build is complete, an <code>x64</code> directory is generated in the same location as the solution file. The executable file of the sample includes the Debug or Release sub-folder.  </p>"},{"location":"docs/03_Installation_on_Windows.html#running>_c>_examples","title":"Running C++ Examples","text":"<p>1. Run the executable file of the sample at the following location. <code>examples\\&lt;example-name&gt;\\msvc\\x64\\[Debug|Release]\\&lt;example-name&gt;.exe</code></p> <p>Example <pre><code>C:\\...&gt; cd .\\examples\\run_async_model\\msvc\\x64\\Release\nC:\\...\\examples\\run_async_model\\msvc\\x64\\Release&gt; .\\run_async_model.exe model.dxnn 100\n</code></pre></p>"},{"location":"docs/03_Installation_on_Windows.html#python>_package>_installation","title":"Python Package Installation","text":"<p>DEEPX provides a Python module named <code>dx_engine</code> for Python 3.8 or later.  </p> <p>1. Build and Install the Package Navigate to the Python package directory and install the module.  </p> <pre><code>C:\\...\\dxrt_v2.8.2&gt; cd python_package/\nC:\\...\\dxrt_v2.8.2\\python_package&gt; pip install .\n</code></pre> <p>2. Verify the Installation Open a Python shell and check the installed version.  </p> <pre><code>C:\\...&gt; python\n... \n&gt;&gt;&gt; from dx_engine import version\n&gt;&gt;&gt; print(version.__version__)\n1.0.1\n</code></pre> <p>Examples <pre><code>cd examples/python\nC:\\...\\examples\\python&gt; python run_async_model.py ...model.dxnn 10\n</code></pre></p>"},{"location":"docs/04_Model_Inference.html","title":"Model Inference","text":""},{"location":"docs/04_Model_Inference.html#model>_file>_format","title":"Model File Format","text":"<p>The original ONNX model is converted by DX-COM into the following structure.</p> <pre><code>Model dir.\n    \u2514\u2500\u2500 graph.dxnn\n</code></pre> <ul> <li> <code>graph.dxnn</code> A unified DEEPX artifact that contains  NPU command data, model metadata, model parameters.   </li> </ul> <p>This file is used directly for inference on DEEPX hardware or simulator.  </p>"},{"location":"docs/04_Model_Inference.html#inference>_workflow","title":"Inference Workflow","text":"<p>Here the inference workflow using the DXNN Runtime as follows.  </p> <p>  Figure. Inference Workflow    </p> <ul> <li>1. Compiled Model and optional InferenceOption are provided to initialize the InferenceEngine.  </li> <li>2. Pre-processed Input Tensors are passed to the InferenceEngine for inference.  </li> <li>3. The InferenceEngine produces Output Tensors as a result of the inference.  </li> <li>4. These outputs are then passed to the Post-Processing stage for interpretation or further action.  </li> </ul>"},{"location":"docs/04_Model_Inference.html#prepare>_the>_model","title":"Prepare the Model","text":"<p>Choose one of the following options.  </p> <ul> <li>Use a pre-built model from DX ModelZoo </li> <li>Compile an ONNX model into the DX-RT format using DX-COM (Refer to the DX-COM User Guide for details.)  </li> </ul>"},{"location":"docs/04_Model_Inference.html#configure>_inference>_options","title":"Configure Inference Options","text":"<p>Create a <code>dxrt::InferenceOption</code> object to configure runtime settings for the inference engine.  </p> <p>Note. This option is temporarily unsupported in the current version, and will be available in the next release.</p>"},{"location":"docs/04_Model_Inference.html#load>_the>_model>_into>_the>_inference>_engine","title":"Load the Model into the Inference Engine","text":"<p>Create a <code>dxrt::InferenceEngine</code> instance using the path to the compiled model directory. Hardware resources are automatically initialized during this step.  </p> <p>If <code>dxrt::InferenceEngine</code> is not provided, a default option is applied.  </p> <pre><code>auto ie = dxrt::InferenceEngine(\"yolov5s.dxnn\");\nauto ie = dxrt::InferenceEngine(\"yolov5s.dxnn\", &amp;option);\n</code></pre>"},{"location":"docs/04_Model_Inference.html#connect>_input>_tensors","title":"Connect Input Tensors","text":"<p>Prepare input buffers for inference.  </p> <p>The following example shows how to initialize the buffer with the appropriate size.  </p> <pre><code>std::vector&lt;uint8_t&gt; inputBuf(ie.GetInputSize(), 0);  \n</code></pre> <p>Refer to DX-APP User Guide for practical examples on connecting inference engines to image sources such as cameras or video, along with the preprocessing routines. </p>"},{"location":"docs/04_Model_Inference.html#inference","title":"Inference","text":"<p>DX-RT provides both synchronous and asynchronous execution modes for flexible inference handling.  </p> <p>1. Run - Synchronous Execution Use the <code>dxrt::InferenceEngine::Run()</code> method for blocking, single-core inference.  </p> <pre><code>auto outputs = ie.Run(inputBuf.data());\n</code></pre> <ul> <li>This method processes input and output on the same thread.  </li> <li>This method is suitable for simple and sequential workloads.  </li> </ul> <p>2. Run - Asynchronous Execution </p> <p>a. With <code>Wait()</code> </p> <p>Use <code>RunAsync()</code> to perform the inference in non-blocking mode, and retrieve results later with <code>Wait()</code>.  </p> <pre><code>auto jobId = ie.RunAsync(inputBuf.data());\nauto outputs = ie.Wait(jobId);\n</code></pre> <ul> <li>This method is ideal for parallel workloads where inference can run in the background.  </li> <li>This method is continuously executed while waiting for the result.  </li> </ul> <p>b. With Callback  </p> <p>Use a callback function to handle output as soon as inference completes. </p> <pre><code>std::function&lt;int(vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt;, void*)&gt; postProcCallBack = \\\n    [&amp;](vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt; outputs, void *arg)\n    {\n        /* Process output tensors here */\n        ... ...\n        return 0;\n    };\nie.RegisterCallback(postProcCallBack)\n</code></pre> <ul> <li>The callback is triggered by a background thread after inference.  </li> <li>You can pass a custom argument to track input/output pairs.</li> </ul> <p>Note. Output data is only valid within the callback scope.  </p>"},{"location":"docs/04_Model_Inference.html#process>_output>_tensors","title":"Process Output Tensors","text":"<p>Once inference is complete, the output tensors are processed using Tensor APIs and custom post-processing logic. You can find the templates and example code in DX-APP to help you implement post-process smoothly. As noted earlier, using callbacks allows for more efficient and real-time post-processing.  </p>"},{"location":"docs/04_Model_Inference.html#multiple>_device>_inference","title":"Multiple Device Inference","text":"<p>This feature is not applicable to single-NPU devices. Basically, the inference engine schedules and manages multiple devices in real time. If the inference option is explicitly set, the inference engine may only use specific devices during real-time inference for the model.  </p>"},{"location":"docs/04_Model_Inference.html#data>_format>_of>_device>_tensor","title":"Data Format of Device Tensor","text":"<p>Compiled models use the NHWC format by default.  </p> <p>However, the input tensor formats on the device side may vary depending on the hardware\u2019s processing type.  </p> <p>Input Tensor Formats </p> Type Compiled Model Format Device Format Data Size <code>Formatter</code> <code>[N, H, W, C]</code> <code>[N, H, W, C]</code> 8-bit <code>IM2COL</code> <code>[N, H, W, C]</code> <code>[N, H, align64(W*C)]</code> 8-bit <ul> <li>Formatter Type Example: <code>[1, 3, 224, 224] (NCHW) -&gt; [1, 224, 224, 3] (NHWC)</code> </li> <li>IM2COL Type Example: <code>[1, 3, 224, 224] (NCHW) -&gt; [1, 224, 224*3+32] (NH, aligned width x channel)</code> </li> </ul> <p>Output Tensor Formats </p> <p>The output tensor format is also aligned with the NHWC format, but with padding applied for alignment.</p> Type Compiled Model Format Device Format <code>Aligned NHWC</code> <code>[N, H, W, C]</code> <code>[N, H, W, align64(C)]</code> <ul> <li>Output Example: <code>[1, 40, 52, 36] (NCHW) -&gt; [1, 52, 36, 40+24]</code>    (Channel size is aligned for optimal memory access.)  </li> </ul> <p>Post-processing can be performed directly without converting formats. API to convert from device format to NCHW/NHWC format will be supported in the next release.  </p>"},{"location":"docs/04_Model_Inference.html#profile>_application","title":"Profile Application","text":""},{"location":"docs/04_Model_Inference.html#gather>_timing>_data>_per>_event","title":"Gather Timing Data per Event","text":"<p>You can profile events within your application using the Profiler APIs. Please refer to Section 8. API reference.  </p> <p>Here is a basic usage example. </p> <pre><code>auto&amp; profiler = dxrt::Profiler::GetInstance();\nprofiler.Start(\"1sec\");\nsleep(1);\nprofiler.End(\"1sec\");\n</code></pre> <p>After the application is finished, <code>profiler.json</code> is created in the working directory.</p>"},{"location":"docs/04_Model_Inference.html#visualize>_profiler>_data","title":"Visualize Profiler Data","text":"<p>You can visualize the profiling results using the following Python script.  </p> <pre><code>python3 tool/profiler/plot.py --input profiler.json\n</code></pre> <p>This generates an image file named <code>profiler.png</code>, providing a detailed view of runtime event timing for performance analysis. </p> <p>  Figure. DX-RT Profiling Report    </p> <p>Script Usage: <code>tool/profiler/plot.py</code> </p> <p>Use this script to draw a timing chart from profiling data generated by DX-RT.</p> <pre><code>usage: plot.py [-h] [-i INPUT] [-o OUTPUT] [-s START] [-e END] [-g]\n</code></pre> <p>Optional Arguments  </p> <ul> <li><code>-h, --help</code>: Show help message and exit  </li> <li><code>-i INPUT, --input INPUT</code>: Input <code>.json</code> file to visualize (e.g., <code>profiler.json</code>)  </li> <li><code>-o OUTPUT, --output OUTPUT</code>: Output image file name to save (e.g., profiler.png)  </li> <li><code>-s START, --start START</code>: Starting position (normalized, &gt; 0.0) within the time interval [0.0-1.0]  </li> <li><code>-e END, --end END</code>: End position (normalized, &lt; 1.0) within the time interval [0.0-1.0]  </li> <li><code>-g, --show_gap</code>: Show time gaps between the start point of each event  </li> </ul> <p>Please refer to usage of <code>tool/profiler/plot.py</code>.  </p> <pre><code>usage: plot.py [-h] [-i INPUT] [-o OUTPUT] [-s START] [-e END] [-g]\n</code></pre>"},{"location":"docs/04_Model_Inference.html#how>_to>_create>_an>_application>_using>_dx-rt","title":"How To Create an Application Using DX-RT","text":"<p>This guide provides step-by-step instructions for creating a new CMake project using the DX-RT library.</p> <p>1. Build the DX-RT Library  Before starting, make sure the DX-RT library is already built.  </p> <p>Refer to Chapter 2. Installation on Linus and Chapter 3. Installation on Windows for detailed build instructions. </p> <p>2. Create a New CMake Project  Create a project directory and an initial <code>CMakeLists.txt</code> file. <pre><code>mkdir MyProject\ncd MyProject\ntouch CMakeLists.txt\n</code></pre></p> <p>3. \u201cHello World\u201d with DX-RT API Create a simple source file (<code>main.cpp</code>) that uses a DX-RT API.  </p> <pre><code>#include \"dxrt/dxrt_api.h\"\nusing namespace std;\n\nint main(int argc, char *argv[])\n{\n auto&amp; devices = dxrt::CheckDevices();\n cout &lt;&lt; \"hello, world\" &lt;&lt; endl;\n return 0;\n}\n</code></pre> <p>4. Modify CMakeLists.txt Edit the <code>CMakeLists.txt</code> file as follows.  </p> <pre><code>cmake_minimum_required(VERSION 3.14)\nproject(app_template)\n\nset(CMAKE_CXX_STANDARD_REQUIRED \"ON\")\nset(CMAKE_CXX_STANDARD \"14\")\n\n# Set the DX-RT library installation path (adjust as needed)\nset(DXRT_LIB_PATH \"/usr/local/lib\") \n\n# Locate the DX-RT library\nfind_library(DXRT_LIBRARY REQUIRED NAMES dxrt_${CMAKE_SYSTEM_PROCESSOR} PATHS $\n{DXRT_LIB_PATH})\n\n# Add executable and link libraries\nadd_executable(HelloWorld main.cpp)\ntarget_link_libraries(HelloWorld PRIVATE ${DXRT_LIBRARY} protobuf)\n</code></pre> <p>Replace <code>/usr/local/lib</code> with the actual path where the DX-RT library is installed.</p> <p>5. Build the Project Compile your project using the following commands. <pre><code>mkdir build\ncd build\ncmake ..\nmake\n</code></pre></p> <p>6. Run the Executable After a successful build, run the generated executable.   </p> <pre><code>./HelloWorld\n</code></pre> <p>You now successfully create and build a CMake project using the DX-RT library. </p>"},{"location":"docs/04_Model_Inference.html#optional>_improving>_cpu>_task>_throughput>_with>_dxrt>_dynamic>_cpu>_thread","title":"(Optional) Improving CPU Task Throughput with DXRT_DYNAMIC_CPU_THREAD","text":"<p>The USE_ORT option allows for enabling ONNX Runtime to handle operations that are not supported by the NPU.  When this option is active, the model's CPU tasks are executed via ONNX Runtime. </p> <p>To mitigate potential bottlenecks in these CPU tasks, especially under varying Host CPU conditions, an optional dynamic multi-threading feature is provided. This feature monitors the input queue load to identify CPU task congestion. If a high load is detected, it dynamically increases the number of threads allocated to CPU tasks, thereby improving their throughput. This dynamic CPU threading can be enabled by setting the DXRT_DYNAMIC_CPU_THREAD=ON environment variable (e.g., export DXRT_DYNAMIC_CPU_THREAD=ON). </p> <p>Additionally, if the system observes that CPU tasks are experiencing significant load, it will display a message: \"To improve FPS, set: 'export DXRT_DYNAMIC_CPU_THREAD=ON'\", recommending the activation of this feature for better performance.</p> <p>Warning: Enabling the DXRT_DYNAMIC_CPU_THREAD=ON option does not always guarantee an FPS increase; its effectiveness can vary depending on the specific workload and system conditions.</p>"},{"location":"docs/05_Command_Line_Interface.html","title":"Command Line Interface","text":"<p>This chapter introduces DX-RT command-line tools for model inspection, execution, and device management.</p>"},{"location":"docs/05_Command_Line_Interface.html#parse>_model","title":"Parse Model","text":"<p>This tool is used to parse and inspect a compiled model file (<code>.dxnn</code>), printing model structure and metadata.  </p> <p>Source: <code>bin/parse_model.cpp</code></p> <p>Usage <pre><code>parse_model -m &lt;model_dir&gt;\n</code></pre></p> <p>Option </p> <ul> <li><code>-m, --model</code>: Path to the compiled model file  (<code>.dxnn</code>)  </li> <li><code>-h, --help</code>: Show help message  </li> </ul> <p>Example <pre><code>$./parse_model -m model.dxnn\n</code></pre></p>"},{"location":"docs/05_Command_Line_Interface.html#run>_model","title":"Run Model","text":"<p>This tool runs a compiled model and performs a basic inference test. It measures inference time, validates output data against a reference, and optionally runs in a loop for stress testing.  </p> <p>Source: <code>bin/run_model.cpp</code></p> <p>Usage <pre><code>run_model -m &lt;model_dir&gt; -i &lt;input_bin&gt; -o &lt;output_bin&gt; -r &lt;reference output_bin&gt; -l &lt;number of loops&gt;\n</code></pre></p> <p>Option </p> <ul> <li><code>-c, --config</code>: Path to a JSON configuration file  </li> <li><code>-m, --model</code>: Path to the compiled model file (<code>.dxnn</code>)  </li> <li><code>-i, --input</code>: Input binary file  </li> <li><code>-o, --output</code>: Output file to save inference results  </li> <li><code>-r, --ref</code>: Reference output file to compare results  </li> <li><code>-l, --loop</code>: Number of inference iteration to run (loop test)</li> <li><code>--use-ort</code>: use ONNX Runtime</li> <li><code>-h, --help</code>: Show help message  </li> </ul> <p>Example <pre><code>$ run_model -m /.../model.dxnn -i /.../input.bin -l 100\n</code></pre></p>"},{"location":"docs/05_Command_Line_Interface.html#dx-rt>_cli>_tool>_firmware>_interface","title":"DX-RT CLI Tool (Firmware Interface)","text":"<p>This tool provides a command-line interface to interact with DX-RT accelerator devices. It supports querying device status, resetting hardware, updating firmware, and more.  </p> <p>Note. This tool is applicable only for accelerator devices.  </p> <p>Usage <pre><code>dxrt-cli &lt;option&gt; &lt;argument&gt;\n</code></pre></p> <p>Option </p> <ul> <li><code>-s, --status</code>: Get current device status  </li> <li><code>-i, --info</code>: Display basic device information  </li> <li><code>-m, --monitor</code>: Monitoring device status every [arg] seconds (arg &gt; 0)  </li> <li><code>-r, --reset</code>: Reset device (0: NPU only, 1: full device) (default: 0)  </li> <li><code>-d, --device</code>: Specify device ID (default: -1 for all device)  </li> <li><code>-u, --fwupdate</code>: Update firmware with a Deepx firmware file (options: force:, unreset)  </li> <li><code>-w, --fwupload</code>: Update firmware file (2nd_boot or rtos)  </li> <li><code>-g, --fwversion</code>: Check firmware version from a firmware file  </li> <li><code>-p, --dump</code>: Dump initial device state to a file  </li> <li><code>-l, --fwlog</code>: Extract firmware logs to a file  </li> <li><code>-h, --help</code>: Show help message  </li> </ul> <p>Example <pre><code>$ dxrt-cli --status\n\n$ dxrt-cli --reset 0\n\n$ dxrt-cli --fwupdate fw.bin\n\n$ dxrt-cli -m 1\n</code></pre></p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html","title":"C++ Tutorials","text":""},{"location":"docs/06_01_C%2B%2B_Tutorials.html#c>_tutorials","title":"C++ Tutorials","text":""},{"location":"docs/06_01_C%2B%2B_Tutorials.html#run>_synchronous","title":"Run (Synchronous)","text":"<p>The synchronous Run method uses a single NPU core to perform inference in a blocking manner. It can be configured to utilize multiple NPU cores simultaneously by employing threads to run each core independently.</p> <p>  Figure. Synchronous Inference Operation    </p> <p>Inference Engine Run synchronous  </p> <ul> <li>Inference synchronously  </li> <li>Use only one npu core  </li> </ul> <p>The following is the simplest example of synchronous inference.  </p> <p><code>run_sync_model.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\nint main()\n{\n    std::string modelPath = \"model-path\";\n\n    try \n    {\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        // inference loop\n        for(int i = 0; i &lt; 100; ++i)\n        {\n            // inference synchronously\n            // use only one npu core\n            auto outputs = ie.Run(inputPtr.data());\n\n            // post processing\n            postProcessing(outputs);\n\n        } // for i\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine  \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#runasync>_asynchronous","title":"RunAsync (Asynchronous)","text":"<p>The asynchronous Run mode is a method that performs inference asynchronously while utilizing multiple NPU cores simultaneously. It can be implemented to maximize NPU resources through a callback function or a thread wait mechanism.</p> <p>  Figure. Asynchronous Inference Operation    </p> <p>Inference Engine RunAsync, Callback, User Argument  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>The following is an example of asynchronous inference using a callback function. A user argument can be used to synchronize the input with the output of the callback.  </p> <p><code>run_async_model.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\n\nstatic std::atomic&lt;int&gt; gCallbackCnt = {0};\nstatic ConcurrentQueue&lt;int&gt; gResultQueue(1);\nstatic std::mutex gCBMutex;\n\n// invoke this function asynchronously after the inference is completed\nstatic int onInferenceCallbackFunc(dxrt::TensorPtrs &amp;outputs, void *userArg)\n{\n\n    // user data type casting\n    std::pair&lt;int, int&gt;* user_data = reinterpret_cast&lt;std::pair&lt;int, int&gt;*&gt;(userArg);\n\n    // post processing with outputs\n    // ...\n    (void)outputs;\n\n\n    {\n        // Mutex locks should be properly adjusted \n        // to ensure that callback functions are thread-safe.\n        std::lock_guard&lt;std::mutex&gt; lock(gCBMutex);\n\n        gCallbackCnt ++;\n\n        // end of the loop\n        if ( user_data-&gt;second == gCallbackCnt.load() ) // check loop count\n        {\n            gResultQueue.push(gCallbackCnt);\n        }\n    }\n\n    // delete argument object\n    delete user_data;\n\n    return 0;\n}\n\n\nint main(int argc, char* argv[])\n{\n    ...\n\n    int total_callback_count = 0;\n\n    try \n    {\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // register call back function\n        ie.RegisterCallback(onInferenceCallbackFunc);\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        // inference loop\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            // user argument\n            std::pair&lt;int, int&gt; *userData = new std::pair&lt;int, int&gt;(i, loop_count);\n\n            // inference asynchronously, use all npu cores\n            ie.RunAsync(inputPtr.data(), userData);\n\n        }\n\n        // wait until all callbacks have been processed\n        total_callback_count = gResultQueue.pop();\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return (total_callback_count == loop_count ? 0 : -1);\n}\n</code></pre></p> <p>The following is an example where multiple threads start input and inference, and a single callback processes the output.  </p> <p>Inference Engine RunAsync, Callback, User Argument, Thread  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p><code>run_async_model_thread.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\nstatic const int THREAD_COUNT = 3;\nstatic std::atomic&lt;int&gt; gResultCount = {0};\nstatic std::atomic&lt;int&gt; gTotalCount = {0};\nstatic ConcurrentQueue&lt;int&gt; gResultQueue(1);\nstatic std::mutex gCBMutex;\n\nstatic int inferenceThreadFunc(dxrt::InferenceEngine&amp; ie, std::vector&lt;uint8_t&gt;&amp; inputPtr, int threadIndex, int loopCount)\n{\n\n    // inference loop\n    for(int i = 0; i &lt; loopCount; ++i) \n    {\n        // user argument\n        UserData *userData = new UserData();\n\n        // thread index \n        userData-&gt;setThreadIndex(threadIndex);\n\n        // total loop count\n        userData-&gt;setLoopCount(loopCount);\n\n        // loop index\n        userData-&gt;setLoopIndex(i);\n\n        try\n        {\n            // inference asynchronously, use all npu cores\n            // if device-load &gt;= max-load-value, this function will block  \n            ie.RunAsync(inputPtr.data(), userData);\n        }\n        catch(const dxrt::Exception&amp; e)\n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n        catch(const std::exception&amp; e)\n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n\n    } // for i\n\n    return 0;\n\n}\n\n// invoke this function asynchronously after the inference is completed\nstatic int onInferenceCallbackFunc(dxrt::TensorPtrs &amp;outputs, void *userArg)\n{\n\n    // the outputs are guaranteed to be valid only within this callback function\n    // processing this callback functions as quickly as possible is beneficial \n    // for improving inference performance\n\n    // user data type casting\n    UserData *user_data = reinterpret_cast&lt;UserData*&gt;(userArg);\n\n    // thread index\n    int thread_index = user_data-&gt;getThreadIndex();\n\n    // loop index\n    int loop_index = user_data-&gt;getLoopIndex();\n\n    // post processing\n    // transfer outputs to the target thread by thread_index\n    // postProcessing(outputs, thread_index);\n    (void)outputs;\n\n\n    // result count \n    {\n        // Mutex locks should be properly adjusted \n        // to ensure that callback functions are thread-safe.\n        std::lock_guard&lt;std::mutex&gt; lock(gCBMutex);\n\n        gResultCount++;\n        if ( gResultCount.load() == gTotalCount.load() ) gResultQueue.push(0);\n    }\n\n    // delete argument object \n    delete user_data;\n\n    return 0;\n}\n\n\nint main(int argc, char* argv[])\n{\n    ...\n\n    bool result = false;\n\n    try\n    {\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // register call back function\n        ie.RegisterCallback(onInferenceCallbackFunc);    \n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        gTotalCount.store(loop_count * THREAD_COUNT);\n\n        // thread vector \n        std::vector&lt;std::thread&gt; thread_array;\n\n        for(int i = 0; i &lt; THREAD_COUNT; ++i)\n        {\n            // create thread\n            thread_array.push_back(std::thread(inferenceThreadFunc, std::ref(ie), std::ref(inputPtr), i, loop_count));\n        }\n\n        for(auto &amp;t : thread_array)\n        {\n            t.join();\n        } // for t\n\n\n        // wait until all callbacks have been processed\n        gResultQueue.pop();\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return result ? 0 : -1;\n}\n</code></pre></p> <p>The following is an example of performing asynchronous inference by creating an inference wait thread. The main thread starts input and inference, and the inference wait thread retrieves the output data corresponding to the input.  </p> <p>Inference Engine RunAsync, Wait  </p> <ul> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p><code>run_async_model_wait.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\n// concurrent queue is a thread-safe queue data structure \n// designed to be used in a multi-threaded environment\nstatic ConcurrentQueue&lt;int&gt; gJobIdQueue;\n\n// user thread to wait for the completion of inference \nstatic int inferenceThreadFunc(dxrt::InferenceEngine&amp; ie, int loopCount)\n{\n    int count = 0;\n\n    while(...)\n    {\n        // pop item from queue \n        int jobId = gJobIdQueue.pop();\n\n        try \n        {\n            // waiting for the inference to complete by jobId \n            auto outputs = ie.Wait(jobId);\n\n            // post processing \n            postProcessing(outputs);\n\n        }\n        catch(const dxrt::Exception&amp; e)  // exception for inference engine \n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n        catch(const std::exception&amp; e)\n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n\n        // something to do\n\n        count++;\n        if ( count &gt;= loopCount ) break;\n\n    } // while\n\n    return 0;\n}\n\nint main()\n{\n    const int LOOP_COUNT = 100;\n    std::string modelPath = \"model-path\";\n\n    try\n    {\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // do not register call back function\n        // inferenceEngine.RegisterCallback(onInferenceCallbackFunc);\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        // create thread\n        auto t1 = std::thread(inferenceThreadFunc, std::ref(ie), LOOP_COUNT);\n\n        // inference loop\n        for(int i = 0; i &lt; LOOP_COUNT; ++i)\n        {\n\n            // no need user argument\n            // UserData *userData = getUserDataInstanceFromDataPool();\n\n            // inference asynchronously, use all npu cores\n            // if device-load &gt;= max-load-value, this function will block\n            auto jobId = ie.RunAsync(inputPtr.data());\n\n            // push jobId in global queue variable\n            gJobIdQueue.push(jobId);\n\n        } // for i\n\n        t1.join();\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#run>_batch","title":"Run (Batch)","text":"<p>The following is an example of batch inference with multiple inputs and multiple outputs.</p> <p><code>run_batch_model.cpp</code></p> <pre><code>int main(int argc, char* argv[])\n{\n    ...\n\n    try\n    {\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputBuffer(ie.GetInputSize(), 0);\n\n        // input buffer vector\n        std::vector&lt;void*&gt; inputBuffers;\n        for(int i = 0; i &lt; batch_count; ++i)\n        {\n            // assigns the same buffer pointer in this example\n            inputBuffers.emplace_back(inputBuffer.data());\n        }\n\n        // output buffer vector\n        std::vector&lt;void*&gt; output_buffers(batch_count, 0);\n\n        // create user output buffers\n        for(auto&amp; ptr : output_buffers)\n        {\n            ptr = new uint8_t[ie.GetOutputSize()];\n        } // for i\n\n        // batch inference loop\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            // inference asynchronously, use all npu core\n            auto outputPtrs = ie.Run(inputBuffers, output_buffers);\n\n            // postProcessing(outputs);\n            (void)outputPtrs;\n        }\n\n        // Deallocated the user's output buffers\n        for(auto&amp; ptr : output_buffers)\n        {\n            delete[] static_cast&lt;uint8_t*&gt;(ptr);\n        } // for i\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#run>_runasync","title":"Run &amp; RunAsync","text":"<p>The method for converting a synchronous inference approach using one NPU core into an asynchronous inference approach using multiple NPU cores is as follows. It requires the use of callbacks or threads, as well as the implementation of multiple input buffers to support concurrent operations effectively.</p> <p>Converting Run(Sync) to RunAsync </p> <ul> <li>Shift from Single NPU Core to Multiple Cores     : Modify the existing Run(Sync) structure, which utilizes a single NPU core, to RunAsync structure capable of leveraging multiple NPU cores simultaneously.  </li> <li>Create Multiple Input/Output Buffers     : Implement multiple input/output buffers to prevent overwriting. Ensure an appropriate number of buffers are created to support concurrent operations effectively.  </li> <li>Introduce Multi-Buffer Concept     : To handle simultaneous inference processes, integrate a multi-buffer mechanism. This is essential for managing concurrent inputs and outputs without data conflicts.  </li> <li>Asynchronous Inference with Threads or Callbacks     : Adjust the code to ensure that inference inputs and outputs operate asynchronously using threads or callbacks for efficient processing.  </li> <li>Thread-Safe Data Exchange     : For data exchange between threads or callbacks, use a thread-safe queue or structured data mechanisms to avoid race conditions and ensure integrity.  </li> </ul> <p>  Figure. Converting Run(Sync) to RunAsync    </p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#inference>_option","title":"Inference Option","text":"<p>The following inference options allow you to specify an NPU core for performing inference.</p> <p>Inference Engine Run, Inference Option  </p> <ul> <li>Select devices     : default devices is <code>{}</code>     : Choose devices to utilize  </li> <li>Select bound option per device     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_ALL</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_0</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_1</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_2</code> </li> <li>Use onnx runtime library (<code>ORT</code>)     : <code>useORT</code> on or off  </li> </ul> <p><code>run_sync_model_bound.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\nint main()\n{\n    std::string modelPath = \"model-path\";\n\n    try\n    {\n\n        // select bound option NPU_0 to NPU_2 per device  \n        dxrt::InferenceOption op;\n\n        // first device only, default null\n        op.devices.push_back(0); // use device 0 \n        op.devices.push_back(3); // use device 3 \n\n        // use BOUND_OPTION::NPU_0 only\n        op.boundOption = dxrt::InferenceOption::BOUND_OPTION::NPU_0; \n\n        // use ORT\n        op.useORT = false;\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath, op);\n\n        // create temporary input buffer for example \n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        // inference loop\n        for(int i = 0; i &lt; 100; ++i)\n        {\n            // input\n            uint8_t* inputPtr = readInputData();\n\n            // inference synchronously with boundOption\n            // use only one npu core\n            // ownership of the outputs is transferred to the user \n            auto outputs = ie.Run(inputPtr.data());\n\n            // post processing\n            postProcessing(outputs);\n\n        } // for i\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#camera>_inference>_display","title":"Camera / Inference / Display","text":"<p>The following is an example of a pattern that performs inference using two models on a single camera input and combines the results from both models for display.</p> <p>  Figure. Multi-model and Multi-output    </p> <p>Multi-model, Async, Wait Thread <code>(CPU_1 \u2192 {NPU_1 + NPU_2} \u2192 CPU_2</code></p> <p><code>display_async_wait.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\n// input processing main thread with 2 InferenceEngine (asynchronous) \n// display thread \n\nstruct FrameJobId {\n    int jobId_A = -1;\n    int jobId_B = -1;\n    void* frameBuffer = nullptr;\n    int loopIndex = -1;\n};\n\nstatic const int BUFFER_POOL_SIZE = 10;\nstatic const int QUEUE_SIZE = 10;\n\nstatic ConcurrentQueue&lt;FrameJobId&gt; gFrameJobIdQueue(QUEUE_SIZE);\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_A;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_B;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gFrameBufferPool;\n\n// total display count\nstatic std::atomic&lt;int&gt; gTotalDisplayCount = {0};\n\nstatic int displayThreadFunc(int loopCount, dxrt::InferenceEngine&amp; ieA, dxrt::InferenceEngine&amp; ieB)\n{\n\n    while(gTotalDisplayCount.load() &lt; loopCount)\n    {\n        // consumer framebuffer &amp; jobIds\n        auto frameJobId = gFrameJobIdQueue.pop();\n\n        // output data of ieA\n        auto outputA = ieA.Wait(frameJobId.jobId_A);\n\n        // output data of ieB\n        auto outputB = ieB.Wait(frameJobId.jobId_B);\n\n        // post-processing w/ outputA &amp; outputB\n        postProcessing(outputA, outputB);\n\n        gTotalDisplayCount++;\n\n        // display (update framebuffer)\n    }\n\n    return 0;\n}\n\n\nint main(int argc, char* argv[])\n{\n    ...\n\n    try\n    {\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieA(modelPath_A);\n\n        gInputBufferPool_A = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieA.GetInputSize());\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieB(modelPath_B);\n\n        gInputBufferPool_B = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieB.GetInputSize());\n\n        const int W = 512, H = 512, CH = 3;\n        gFrameBufferPool = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, W*H*CH);\n\n        // create thread\n        std::thread displayThread(displayThreadFunc, loop_count, std::ref(ieA), std::ref(ieB));\n\n        // input processing\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            uint8_t* frameBuffer = gFrameBufferPool-&gt;pointer(); \n            readFrameBuffer(frameBuffer, W, H, CH);\n\n            uint8_t* inputA = gInputBufferPool_A-&gt;pointer();\n            preProcessing(inputA, frameBuffer);\n\n            uint8_t* inputB = gInputBufferPool_B-&gt;pointer();\n            preProcessing(inputB, frameBuffer);\n\n            // struct to pass to cpu operation thread \n            FrameJobId frameJobId;\n\n            // start inference of A model\n            frameJobId.jobId_A = ieA.RunAsync(inputA);\n\n            // start inference of B model\n            frameJobId.jobId_B = ieB.RunAsync(inputB);\n\n            // framebuffer used for input data\n            frameJobId.frameBuffer = frameBuffer;\n            frameJobId.loopIndex = i;\n\n            // producer frame &amp; jobId\n            gFrameJobIdQueue.push(frameJobId);\n\n        }\n\n        displayThread.join();\n\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p> <p>The following is an example of a pattern that sequentially performs operations using two models and CPU processing. The inference result from Model A is processed through CPU computation and then used as input data for Model B. Finally, the result from Model B is handled for display.</p> <p>  Figure. CPU and NPU Pipeline Operation    </p> <p>Multi-model, Async, Wait Thread <code>(CPU_1 \u2192 NPU_1 \u2192 CPU_2 \u2192 NPU_2 \u2192 CPU_3)</code></p> <p><code>display_async_pipe.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\n// input main thread \n// 1 cpu processing thread  \n// 1 display thread \n\nstruct FrameJobId {\n    int jobId_A = -1;\n    int jobId_B = -1;\n    uint8_t* inputBufferA;\n    uint8_t* inputBufferB;\n    void* frameBuffer = nullptr;\n\n    int loopIndex;\n};\n\nstatic const int BUFFER_POOL_SIZE = 10;\nstatic const int QUEUE_SIZE = 10;\n\nstatic ConcurrentQueue&lt;FrameJobId&gt; gCPUOPQueue(QUEUE_SIZE);\nstatic ConcurrentQueue&lt;FrameJobId&gt; gDisplayQueue(QUEUE_SIZE);\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_A;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_B;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gFrameBufferPool;\n\n// total display count\nstatic std::atomic&lt;int&gt; gTotalDisplayCount = {0};\n\n\nstatic int displayThreadFunc(int loopCount, dxrt::InferenceEngine&amp; ieB)\n{\n\n    while(gTotalDisplayCount.load() &lt; loopCount)\n    {\n        // consumer framebuffer &amp; jobIds\n        auto frameJobId = gDisplayQueue.pop();\n\n        // output data of ieB\n        auto outputB = ieB.Wait(frameJobId.jobId_B);\n\n        // post-processing w/ outputA &amp; outputB\n        postProcessingB(outputB);\n\n        gTotalDisplayCount++;\n\n        // display (update framebuffer)\n        if ( frameJobId.loopIndex == (loopCount - 1)) break;\n    }\n\n    return 0;\n}\n\nstatic int cpuOperationThreadFunc(int loopCount, dxrt::InferenceEngine&amp; ieA, dxrt::InferenceEngine&amp; ieB)\n{\n\n    while(gTotalDisplayCount.load() &lt; loopCount)\n    {\n        // consumer framebuffer &amp; jobIds\n        auto frameJobIdA = gCPUOPQueue.pop();\n\n        // output data of ieA\n        auto outputA = ieA.Wait(frameJobIdA.jobId_A);\n\n        // post-processing w/ outputA\n        postProcessingA(frameJobIdA.inputBufferB, outputA);\n\n        FrameJobId frameJobIdB;\n        frameJobIdB.loopIndex = frameJobIdA.loopIndex;\n        frameJobIdB.jobId_B = ieB.RunAsync(frameJobIdA.inputBufferB);\n\n        gDisplayQueue.push(frameJobIdB);\n\n        // display (update framebuffer)\n        if ( frameJobIdA.loopIndex == (loopCount - 1)) break;\n    }\n\n    return 0;\n}\n\n\nint main(int argc, char* argv[])\n{\n\n    ...\n\n    try\n    {\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieA(modelPath);\n\n        gInputBufferPool_A = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieA.GetInputSize());\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieB(modelPath);\n\n        gInputBufferPool_B = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieB.GetInputSize());\n\n        const int W = 512, H = 512, CH = 3;\n        gFrameBufferPool = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, W*H*CH);\n\n        // create thread\n        std::thread cpuOperationThread(cpuOperationThreadFunc, loop_count, std::ref(ieA), std::ref(ieB));\n        std::thread displayThread(displayThreadFunc, loop_count, std::ref(ieB));\n\n\n        // input processing\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            uint8_t* frameBuffer = gFrameBufferPool-&gt;pointer(); \n            readFrameBuffer(frameBuffer, W, H, CH);\n\n            uint8_t* inputA = gInputBufferPool_A-&gt;pointer();\n            preProcessing(inputA, frameBuffer);\n\n            // struct to pass to a thread \n            FrameJobId frameJobId;\n\n            frameJobId.inputBufferA = inputA;\n            frameJobId.inputBufferB = gInputBufferPool_B-&gt;pointer();\n\n            // start inference of A model\n            frameJobId.jobId_A = ieA.RunAsync(inputA);\n\n            // framebuffer used for input data\n            frameJobId.frameBuffer = frameBuffer;\n            frameJobId.loopIndex = i;\n\n            // producer frame &amp; jobId\n            gCPUOPQueue.push(frameJobId);\n\n        }\n\n        cpuOperationThread.join();\n        displayThread.join();\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#exception","title":"Exception","text":"<p>The error codes and types of exceptions for error handling are as follows.</p> <pre><code>enum ERROR_CODE {\n        DEFAULT = 0x0100,\n        FILE_NOT_FOUND,\n        NULL_POINTER,\n        FILE_IO,\n        INVALID_ARGUMENT,\n        INVALID_OPERATION,\n        INVALID_MODEL,\n        MODEL_PARSING,\n        SERVICE_IO,\n        DEVICE_IO\n    };\n</code></pre> <ul> <li>FileNotFoundException  </li> <li>NullPointerException  </li> <li>FileIOException  </li> <li>InvalidArgumentException  </li> <li>InvalidOperationException  </li> <li>InvalidModelException  </li> <li>ModelParsingException  </li> <li>ServiceIOException  </li> <li>DeviceIOException  </li> </ul> <pre><code>    // try/catch prototype\n\n    try\n    {\n        // DX-RT APIs ...\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1; // or std::exit(-1);\n    }\n    catch(std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;  // or std::exit(-1);\n    }\n</code></pre>"},{"location":"docs/06_01_C%2B%2B_Tutorials.html#examples","title":"Examples","text":"<p>The examples provided earlier are actual code samples that can be executed. Please refer to them for practical use.  </p> <ul> <li><code>display_async_pipe</code>     : An example using <code>[CPU_1 \u2192 {NPU_1 + NPU_2} \u2192 CPU_2]</code> pattern  </li> <li><code>display_async_wait</code>     : An example using <code>[CPU_1 \u2192 NPU_1 \u2192 CPU_2 \u2192 NPU_2 \u2192 CPU_3]</code> pattern  </li> <li><code>display_async_thread</code>     : An example using single model and multi threads  </li> <li><code>display_async_models_1</code>     : An example using multi models and multi threads (Inference Engine is created within each thread)  </li> <li><code>display_async_models_2</code>     : An example using multi models and multi threads (Inference Engine is created in the main thread)  </li> <li><code>run_async_model</code>     : A performance-optimized example using a callback function  </li> <li><code>run_async_model_thread</code>     : An example using a single inference engine, callback function, and thread     : Usage method when there is a single AI model and multiple inputs  </li> <li><code>run_async_model_wait</code>     : An example using threads and waits  </li> <li><code>run_sync_model</code>     : An example using a single thread  </li> <li><code>run_sync_model_thread</code>     : An example running an inference engine on multiple threads  </li> <li><code>run_sync_model_bound</code>     : An example of specifying an NPU using the bound option  </li> </ul>"},{"location":"docs/06_02_Python_Tutorials.html","title":"Python Tutorials","text":""},{"location":"docs/06_02_Python_Tutorials.html#run>_synchronous","title":"Run (Synchronous)","text":"<p>The synchronous Run method uses a single NPU core to perform inference in a blocking manner. It can be configured to utilize multiple NPU cores simultaneously by employing threads to run each core independently. (Refer to Figure in Section 5.2. Inference Workflow)  </p> <p>Inference Engine Run (Python) </p> <p><code>run_sync_model.py</code></p> <pre><code># DX-RT importes\nfrom dx_engine import InferenceEngine\n...\n\n\nif __name__ == \"__main__\":\n    ...    \n\n    # create inference engine instance with model\n    ie = InferenceEngine(modelPath)\n\n    input = [np.zeros(ie.GetInputSize(), dtype=np.uint8)]\n\n    # inference loop\n    for i in range(loop_count):\n\n        # inference synchronously \n        # use only one npu core \n        outputs = ie.Run(input)\n\n        # post processing \n        postProcessing(outputs)\n\n    exit(0)\n</code></pre>"},{"location":"docs/06_02_Python_Tutorials.html#runasync>_asynchronous","title":"RunAsync (Asynchronous)","text":"<p>The asynchronous Run mode is a method that performs inference asynchronously while utilizing multiple NPU cores simultaneously. It can be implemented to maximize NPU resources through a callback function or a thread wait mechanism. </p> <p>Inference Engine RunAsync, Callback, User Argument  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>The following is an example of asynchronous inference using a callback function. A user argument can be used to synchronize the input with the output of the callback.  </p> <p>Inference Engine RunAsync, Callback, User Argument (Python)</p> <p><code>run_async_model.py</code></p> <pre><code>from dx_engine import InferenceEngine\n...\n\nq = queue.Queue()\ngLoopCount = 0\n\nlock = threading.Lock()\n\ndef onInferenceCallbackFunc(outputs, user_arg):\n\n    # the outputs are guaranteed to be valid only within this callback function\n    # processing this callback functions as quickly as possible is beneficial \n    # for improving inference performance\n\n    global gLoopCount\n\n    # Mutex locks should be properly adjusted \n    # to ensure that callback functions are thread-safe.\n    with lock:\n        # user data type casting\n        index, loop_count = user_arg\n\n\n        # post processing\n        #postProcessing(outputs);\n\n        # something to do\n\n        print(\"Inference output (callback) index=\", index)\n\n        gLoopCount += 1\n        if ( gLoopCount == loop_count ) :\n            print(\"Complete Callback\")\n            q.put(0)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n\n    ...    \n\n    # create inference engine instance with model\n    ie = InferenceEngine(modelPath)\n\n    # register call back function\n    ie.register_callback(onInferenceCallbackFunc)\n\n\n    input = [np.zeros(ie.GetInputSize(), dtype=np.uint8)]\n\n    # inference loop\n    for i in range(loop_count):\n\n        # inference asynchronously, use all npu cores\n        # if device-load &gt;= max-load-value, this function will block  \n        ie.RunAsync(input, user_arg=[i, loop_count])\n\n        print(\"Inference start (async)\", i)\n\n    exit(q.get())\n</code></pre> <p>The following is an example where multiple threads start input and inference, and a single callback processes the output.</p> <p>Inference Engine RunAsync, Callback, User Argument, Thread  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>Inference Engine RunAsync, Callback, User Argument, Thread (Python)</p> <p><code>run_async_model_thread.py</code> <pre><code>from dx_engine import InferenceEngine\n...\n\nTHRAD_COUNT = 3\ntotal_count = 0\nq = queue.Queue()\n\nlock = threading.Lock()\n\n\ndef inferenceThreadFunc(ie, threadIndex, loopCount):\n\n    # input\n    input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]\n\n    # inference loop\n    for i in range(loopCount):\n\n        # inference asynchronously, use all npu cores\n        # if device-load &gt;= max-load-value, this function will block  \n        ie.RunAsync(input,user_arg = [i, loopCount, threadIndex])\n\n    return 0\n\ndef onInferenceCallbackFunc(outputs, user_arg):\n    # the outputs are guaranteed to be valid only within this callback function\n    # processing this callback functions as quickly as possible is beneficial \n    # for improving inference performance\n\n    global total_count\n\n    # Mutex locks should be properly adjusted \n    # to ensure that callback functions are thread-safe.\n    with lock:\n        # user data type casting\n        index = user_arg[0]\n        loop_count = user_arg[1]\n        thread_index = user_arg[2]\n\n        # post processing\n        #postProcessing(outputs);\n\n        # something to do\n\n        total_count += 1\n\n        if ( total_count ==  loop_count * THRAD_COUNT) :\n            q.put(0)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    ...    \n\n    # create inference engine instance with model\n    ie = InferenceEngine(modelPath)\n\n    # register call back function\n    ie.register_callback(onInferenceCallbackFunc)\n\n\n    t1 = threading.Thread(target=inferenceThreadFunc, args=(ie, 0, loop_count))\n    t2 = threading.Thread(target=inferenceThreadFunc, args=(ie, 1, loop_count))\n    t3 = threading.Thread(target=inferenceThreadFunc, args=(ie, 2, loop_count))\n\n    # Start and join\n    t1.start()\n    t2.start()\n    t3.start()\n\n\n    # join\n    t1.join()\n    t2.join()\n    t3.join()\n\n\n    exit(q.get())\n</code></pre></p> <p>The following is an example of performing asynchronous inference by creating an inference wait thread. The main thread starts input and inference, and the inference wait thread retrieves the output data corresponding to the input.</p> <p>Inference Engine RunAsync, Wait  </p> <ul> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>Inference Engine RunAsync, Wait (Python)</p> <p><code>run_async_model_wait.py</code> <pre><code># DX-RT imports\nfrom dx_engine import InferenceEngine\n...\n\nq = queue.Queue()\n\n\ndef inferenceThreadFunc(ie, loopCount):\n\n    count = 0\n\n    while(True):\n\n        # pop item from queue \n        jobId = q.get()\n\n        # waiting for the inference to complete by jobId\n        # ownership of the outputs is transferred to the user \n        outputs = ie.Wait(jobId)\n\n        # post processing\n        # postProcessing(outputs);\n\n        # something to do\n\n        count += 1\n        if ( count &gt;= loopCount ):\n            break\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    ...\n\n    # create inference engine instance with model\n    with InferenceEngine(modelPath) as ie:\n\n        # do not register call back function\n        # ie.register_callback(onInferenceCallbackFunc)\n\n        t1 = threading.Thread(target=inferenceThreadFunc, args=(ie, loop_count))\n\n        t1.start()\n\n        input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]\n\n        # inference loop\n        for i in range(loop_count):\n\n            # inference asynchronously, use all npu cores\n            # if device-load &gt;= max-load-value, this function will block  \n            jobId = ie.run_async(input, user_arg=0)\n\n            q.put(jobId)\n\n        t1.join()\n\n    exit(0)\n</code></pre></p>"},{"location":"docs/06_02_Python_Tutorials.html#run>_batch","title":"Run (Batch)","text":"<p>The following is an example of batch inference with multiple inputs and multiple outputs.</p> <p><code>run_batch_model.py</code></p> <pre><code>import numpy as np\nimport sys\nfrom dx_engine import InferenceEngine\nfrom dx_engine import InferenceOption\n\n\nif __name__ == \"__main__\":\n    ...\n\n    # create inference engine instance with model\n    with InferenceEngine(modelPath) as ie:\n\n        input_buffers = []\n        output_buffers = []\n        index = 0\n        for b in range(batch_count):\n            input_buffers.append([np.array([np.random.randint(0, 255)],  dtype=np.uint8)])\n            output_buffers.append([np.zeros(ie.get_output_size(), dtype=np.uint8)])\n            index = index + 1\n\n        # inference loop\n        for i in range(loop_count):\n\n            # batch inference\n            # It operates asynchronously internally \n            # for the specified number of batches and returns the results\n            results = ie.run_batch(input_buffers, output_buffers)\n\n            # post processing \n\n    exit(0)\n</code></pre>"},{"location":"docs/06_02_Python_Tutorials.html#inference>_option","title":"Inference Option","text":"<p>The following inference options allow you to specify an NPU core for performing inference.</p> <p>Inference Engine Run, Inference Option  </p> <ul> <li> select devices default device is <code>[]</code> Choose the device to utilize  (ex. <code>[0, 2]</code>)   </li> <li> select bound option per device <code>InferenceOption.BOUND_OPTION.NPU_ALL</code> <code>InferenceOption.BOUND_OPTION.NPU_0</code> <code>InferenceOption.BOUND_OPTION.NPU_1</code> <code>InferenceOption.BOUND_OPTION.NPU_2</code> </li> <li> use onnx runtime library (<code>ORT</code>) <code>set_use_ort / get_use_ort</code> </li> </ul> <p>NPU_ALL / NPU_0 / NPU_1 / NPU_2 <pre><code># DX-RT imports\nfrom dx_engine import InferenceEngine, InferenceOption\n...\n\nif __name__ == \"__main__\":\n    ...\n\n    # inference option\n    option = InferenceOption()\n\n    print(\"Inference Options:\")\n\n    # select devices\n    option.devices = [0]\n\n    # NPU bound opion (NPU_ALL or NPU_0 or NPU_1 or NPU_2)\n    option.bound_option = InferenceOption.BOUND_OPTION.NPU_ALL\n\n    # use ONNX Runtime (True or False)\n    option.use_ort = False\n\n    # create inference engine instance with model\n    with InferenceEngine(modelPath, option) as ie:\n\n        input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]\n\n        # inference loop\n        for i in range(loop_count):\n\n            # inference synchronously \n            # use only one npu core \n            # ownership of the outputs is transferred to the user \n            outputs = ie.run(input)\n\n            # post processing \n            #postProcessing(outputs)\n            print(\"Inference outputs \", i)\n\n    exit(0)\n</code></pre></p>"},{"location":"docs/06_02_Python_Tutorials.html#examples","title":"Examples","text":"<p>The examples provided earlier are actual code samples that can be executed. Please refer to them for practical use. (<code>examples/python</code>)  </p> <ul> <li> <code>run_async_model.py</code> A performance-optimized example using a callback function   </li> <li> <code>run_async_model_thread.py</code> An example using a single inference engine, callback function, and thread   Usage method when there is a single AI model and multiple inputs   </li> <li> <code>run_async_model_wait.py</code> An example using threads and waits   </li> <li> <code>run_sync_model.py</code> An example using a single thread   </li> <li> <code>run_sync_model_thread.py</code> An example running an inference engine on multiple threads   </li> <li> <code>run_sync_model_bound.py</code> An example of specifying an NPU using the bound option   </li> </ul>"},{"location":"docs/07_01_C%2B%2B_API_Reference.html","title":"C++ API Reference","text":""},{"location":"docs/07_01_C%2B%2B_API_Reference.html#c>_api>_reference","title":"C++ API Reference","text":""},{"location":"docs/07_01_C%2B%2B_API_Reference.html#inference>_engine","title":"Inference Engine","text":"<pre><code>class InferenceEngine\nThis class provides an abstraction for the runtime inference executor of the user's compiled model. Once the user loads a compiled model into the InferenceEngine, real-time device tasks are managed and scheduled by internal runtime libraries. It supports both synchronous and asynchronous inference modes, depending on the user's request.\n\n\nInferenceEngine(const std::string&amp; modelPath, InferenceOption&amp; option = DefaultInferenceOption)\n   Constructs an InferenceEngine instance.\n   parameters:\n      [in] modelPath    : Path to the model file.\n      [in] option   : Inference options, including device and NPU core settings. \n                        (default: DefaultInferenceOption).\n\n\nstd::vector&lt;TensorPtrs&gt; GetAllTaskOutputs()\n   Retrieves the output tensors for all tasks.\n   returns:\n      A vector containing the output tensors of all tasks.\n\n\nstd::vector&lt;TensorPtrs&gt; get_outputs()\n   Retrieves the output tensors for all tasks.\n   returns:\n      A vector containing the output tensors of all tasks.\n\n\nstd::string GetCompileType()\n   Retrieves the compile type of the model.\n   returns:\n      The compile type of the model as a std::string.\n\n\nstd::string get_compile_type()\n   [[deprecated(\"Use GetCompileType() instead\")]]\n   Retrieves the compile type of the model.\n   returns:\n      The compile type of the model as a std::string.\n\n\nTensors GetInputs(void* ptr = nullptr, uint64_t phyAddr = 0)\n   Retrieves the input tensor.\n   This function provides access to the input tensor based on the provided \n   virtual and physical address pointers.\n   parameters:\n      [in] ptr      : pointer to virtual the address (optional)\n      [in] phyAddr  : pointer to physical the address (optional)\n   returns:\n     If `ptr` is `nullptr`, returns the input memory area in the engine.\n     If both `ptr` and `phyAddr` are provided, returns input tensors containing \n     output addresses.\n\n\nTensors inputs(void* ptr = nullptr, uint64_t phyAddr = 0)\n   [[deprecated(\"Use GetInputs() instead\")]]\n   Retrieves the input tensor.\n   This function provides access to the input tensor based on the provided \n   virtual and physical address pointers.\n   parameters:\n      [in] ptr      : pointer to virtual the address (optional)\n      [in] phyAddr  : pointer to physical the address (optional)\n   returns:\n     If `ptr` is `nullptr`, returns the input memory area in the engine.\n     If both `ptr` and `phyAddr` are provided, returns input tensors containing \n     output addresses.\n\n\nstd::vector&lt;Tensors&gt; GetInputs(int devId)\n   Retrieves the input tensors.\n   parameters:\n      [in] devId        : Device Id.\n   returns:\n      A vector of the input tensors.\n\n\nstd::vector&lt;Tensors&gt; inputs(int devId)\n   [[deprecated(\"Use GetInputs() instead\")]]\n   Retrieves the input tensors.\n   parameters:\n      [in] devId        : Device Id.\n   returns:\n      A vector of the input tensors.\n\n\nuint64_t GetInputSize()\n   Retrieves the total size of input tensors.\n   This function returns the total memory size required for the input tensors \n   of a single inference operation.\n   returns:\n      Input size in bytes for one inference.\n\n\nuint64_t input_size()\n   [[deprecated(\"Use GetInputSize() instead\")]]\n   Retrieves the total size of input tensors.\n   This function returns the total memory size required for the input tensors \n   of a single inference operation.\n   returns:\n      Input size in bytes for one inference.\n\n\nint GetLatency()\n   Retrieves the most recent inference latency.\n   This function returns the latency time taken for the most recent inference operation.\n   returns:\n      Latency in microseconds\n\n\nint latency()\n   [deprecated(\"Use GetLatency() instead\")]]\n   Retrieves the most recent inference latency.\n   This function returns the latency time taken for the most recent inference operation.\n   returns:\n      Latency in microseconds\n\n\nint GetLatencyCnt()\n   Retrieves the number of latency measurements.  \n   This function returns the total count of latency measurements. \n   returns:\n      The count of latency measurements.\n\n\nstd::vector&lt;int&gt; GetLatencyVector()\n   Retrieves a vector of inference latency values.\n   returns:\n      A vector of inference latency values in microseconds.\n\n\ndouble GetLatencyMean()\n   Retrieves the average inference latency.  \n   This function returns the mean latency of inference execution.\n   returns:\n      The average inference latency in microseconds.\n\n\ndouble GetLatencyStdDev()\n   Retrieves the standard deviation of inference latency.  \n   This function calculates the variation in inference latency time.\n   returns:\n      The standard deviation of inference latency in microseconds.\n\n\nstd::string GetModelName()\n   Retrieves the name of the model.\n   This function returns the model's name.\n   returns:\n      The name of the model as a string.\n\n\nstd::string name()\n   [[deprecated(\"Use GetModelName() instead\")]]\n   Retrieves the name of the model.\n   This function returns the model's name.\n   returns:\n      The name of the model as a string.\n\n\nuint32_t GetNpuInferenceTime()\n   Retrieves the most recent inference time.\n   This function returns the duration of the most recent inference operation.\n   returns:\n      Inference time in microseconds.\n\n\nuint32_t inference_time()\n   [[deprecated(\"Use GetNpuInferenceTime() instead\")]]\n   Retrieves the most recent inference time.\n   This function returns the duration of the most recent inference operation.\n   returns:\n      Inference time in microseconds.\n\n\nint GetNpuInferenceTimeCnt()\n   Retrieves the number of NPU inference time measurements.  \n   This function returns the total count of NPU inference time measurements.\n   returns:\n      The count of NPU inference time measurements.\n\n\nstd::vector&lt;uint32_t&gt; GetNpuInferenceTimeVector()\n   Retrieves a vector of NPU inference time values.\n   returns:\n      A vector of inference time values in microseconds.\n\n\ndouble GetNpuInferenceTimeMean()\n   Retrieves the average NPU inference time.  \n   This function returns the mean execution time for NPU inference. \n   returns:\n      The average NPU inference time in microseconds.\n\n\ndouble GetNpuInferenceTimeStdDev()\n   Retrieves the standard deviation of NPU inference time.  \n   This function returns the standard deviation of the inference time.\n   returns:\n      The standard deviation of inference time in microseconds.\n\n\nint GetNumTailTasks()\n   Retrieves the number of tail tasks in the model.\n   Tail tasks are tasks that do not have any subsequent tasks in the model's task chain.\n   This function returns the count of such tail tasks. \n   returns:\n      The number of tail tasks.\n\n\nint get_num_tails()\n   [[deprecated(\"Use GetNumTailTasks() instead\")]]\n   Retrieves the number of tail tasks in the model.\n   Tail tasks are tasks that do not have any subsequent tasks in the model's task chain.\n   This function returns the count of such tail tasks. \n   returns:\n      The number of tail tasks.\n\n\nTensors GetOutputs(void *ptr=nullptr, uint64_t phyAddr=0)\n   Retrieves the output tensor.\n   parameters:\n      [in] ptr      : Pointer to the virtual address.\n      [in] phyAddr  : Pointer to the physical address.\n   returns:\n      If `ptr` is `nullptr`, the output memory area in the engine is returned\n      If both `ptr` and `phyAddr` are provided, returns output tensors containing \n      the output addresses.\n\n\nTensors outputs(void *ptr=nullptr, uint64_t phyAddr=0)\n   [[deprecated(\"Use GetOutputs() instead\")]]\n   Retrieves the output tensor.\n   parameters:\n      [in] ptr      : Pointer to the virtual address.\n      [in] phyAddr  : Pointer to the physical address.\n   returns:\n      If ptr is null, the output memory area in the engine is returned\n      If both ptr and phyAddr are provided, returns output tensors containing \n      the output addresses.\n\n\nuint64_t GetOutputSize()\n   Retrieves the total size of output tensors.\n   This function returns the total memory size required for the output tensors \n   of a single inference operation.\n   returns:\n      Output size in bytes for one inference.\n\n\nuint64_t output_size()\n   [[deprecated(\"Use GetOutputSize() instead\")]]\n   Retrieves the total size of output tensors.\n   This function returns the total memory size required for the output tensors \n   of a single inference operation.\n   returns:\n      Output size in bytes for one inference.\n\n\nstd::vector&lt;string&gt; GetTaskOrder()\n   Retrieves the execution order of tasks in the model.\n   This function returns the sequence of tasks as they are executed within the model.\n   returns:\n      A vector of strings representing the task execution order.\n\n\nstd::vector&lt;string&gt; task_order()\n   [[deprecated(\"Use GetTaskOrder() instead\")]]\n   Retrieves the execution order of tasks in the model.\n   This function returns the sequence of tasks as they are executed within the model.\n   returns:\n      A vector of strings representing the task execution order.\n\n\nbool IsPPU()\n   Checks whether PPU is utilized.\n   returns:\n     true if PPU is being utilized, otherwise false.\n\n\nbool is_PPU()\n   [[deprecated(\"Use IsPPU() instead\")]]\n   Checks whether PPU is utilized.\n   returns:\n     true if PPU is being utilized, otherwise false.\n\n\nvoid RegisterCallback(std::function&lt;int(TensorPtrs &amp;outputs, void *userArg)&gt;    callbackFunc)   \n   Register user callback function to be called by inference completion.\n   parameters:\n      [in] callbackFunc : A function that is called when inference is complete.  \n                          It receives two arguments: outputs and userArg.\n                          outputs : output tensors data\n                          userArg : userArg given by RunAsync();\n\n\n\nvoid RegisterCallBack(std::function&lt;int(TensorPtrs &amp;outputs, void *userArg)&gt;    callbackFunc)   \n   [[deprecated(\"Use RegisterCallback() instead\")]]\n   Register user callback function to be called by inference completion.\n   parameters:\n      [in] callbackFunc : A function that is called when inference is complete.  \n                          It receives two arguments: outputs and userArg.\n                          outputs : output tensors data\n                          userArg : userArg given by RunAsync();\n\n\nTensorPtrs Run(void* inputPtr, void * userArg = nullptr, void* outputPtr = nullptr)\n   Runs the inference engine synchronously using the specified input pointer.\n   This function executes inference with the given input data and returns the output tensors.\n   parameters:\n      [in] inputPtr : Pointer to the input data for inference.\n      [in] userArg  : User-defined arguments as a void pointer (optional).\n                        (e.g. original frame data, metadata about input, etc.)\n      [out] outputPtr   : Pointer to store the output data. If `nullptr`, the output data is \n                         stored in an internal buffer.\n   returns:\n      Output tensors as a vector of smart pointer instances.\n\n\nint RunAsync(void* inputPtr, void * userArg = nullptr, void* outputPtr = nullptr)\n   Initiates an asynchronous inference request and returns a job ID.\n   This function performs a non-blocking inference operation using the specified input\n   pointer. It returns a job ID that can be used with the `wait()` function to retrieve \n   the results.\n   parameters:\n      [in] inputPtr : Pointer to the input data for inference.\n      [in] userArg  : User-defined arguments as a void pointer (optional).\n                        (e.g. original frame data, metadata about input, etc.)\n      [out] outputPtr   : Pointer to store the output data. If `nullptr`, the output data is \n                         stored in an internal buffer.\n   returns:\n      Job ID that can be used with the `wait()` function to retrieve the inference result.\n\n\nfloat RunBenchmark(int num, void* inputPtr = nullptr)\n   Runs a benchmark test by executing inference multiple times.\n   This function performs inference in a loop for the specified number of times \n   and calculates the average frames per second (FPS).\n   parameters:\n      [in] num      : Number of inference iterations.\n      [in] inputPtr : Pointer to the input data for inference (optional). \n                         If `nullptr`, default input data is used.\n   returns:\n      Average FPS (frames per second) over the benchmark runs.\n\n\nfloat RunBenchMark(int num, void* inputPtr = nullptr)\n   [[deprecated(\"Use RunBenchmark() instead\")]]\n   Runs a benchmark test by executing inference multiple times.\n   This function performs inference in a loop for the specified number of times \n   and calculates the average frames per second (FPS).\n   parameters:\n      [in] num      : Number of inference iterations.\n      [in] inputPtr : Pointer to the input data for inference (optional). \n                         If `nullptr`, default input data is used.\n   returns:\n      Average FPS (frames per second) over the benchmark runs.\n\n\nTensorPtrs Wait(int jobId)\n   Waits for the completion of an asynchronous inference request.\n   This function blocks execution until the specified inference job, identified by `jobId`, \n   is complete and then returns the output tensors.   \n   parameters:\n      [in] jobId    : Job ID returned by `RunAsync()`, used to track the inference request.\n   returns:\n      Output tensors as a vector of smart pointer instances.\n</code></pre>"},{"location":"docs/07_01_C%2B%2B_API_Reference.html#inference>_option","title":"Inference Option","text":"<pre><code>class InferenceOption\nThis struct specifies inference options applied to dxrt::InferenceEngine.\nUser can configure which npu device is used to inference.\n\nenum BOUND_OPTION {\n        NPU_ALL = 0,\n        NPU_0,\n        NPU_1, \n        NPU_2\n    };\n\nuint32_t boundOption = BOUND_OPTION::NPU_ALL\n   variables:\n   Select the NPU core inside the device.\n   NPU_ALL is an option that uses all NPU cores simultaneously. NPU_0, NPU_1, and NPU_2 are   \n   options that allow using only a single NPU core.\n\n\nstd::vector&lt; int&gt; devices = {}\n   variables:\n   device ID list to use\n   make a list which contains a list of device ID to use. if it is empty\n   (or use default value), \n   then all devices are used. list of device ID to use (it is empty by default, then all \n   devices are used.)\n</code></pre>"},{"location":"docs/07_01_C%2B%2B_API_Reference.html#profiler","title":"Profiler","text":"<pre><code>class Profiler\nProvides a time measurement API based on timestamps.\nThe `Profiler` class is used to measure execution time using timestamps,\nenabling performance analysis of various operations.\n\nvoid Add(const std::string&amp; event)\n   Registers an event in the profiler.\n   This function records an event with the specified name. If the profiler is used \n   in a multi-threaded environment, this function should be called first to ensure \n   proper event tracking.\n   parameters:\n      [in] event    : Name of the event to be registered.\n\n\nvoid AddTimePoint(const std::string&amp; event, TimePointPtr tp)\n   Adds a timing data point for the specified event.\n   This function records a time point associated with a given event name, \n   allowing precise measurement of execution timing.\n   parameters:\n      [in] event    : Name of the event to associate with the time point.\n      [in] tp       : Pointer to the timing data.\n\n\nvoid End(const std::string&amp; event)\n   Records the end point of a specified event.\n   This function marks the completion of an event, allowing for \n   measurement of the event's duration when used with corresponding start points.\n   parameters:\n      [in] event    : Name of the event to mark as completed.\n\n\nvoid Erase(const std::string&amp; event)\n   Clears the timing data of a specified event.\n   parameters:\n      [in] event    : Name of the event whose timing data should be cleared.\n\n\nuint64_t Get(const std::string&amp; event)\n   Retrieves the most recent elapsed time of a specified event.\n   This function returns the elapsed time (in microseconds) for the given event, \n   based on the most recent recorded start and end points.\n   parameters:\n      [in] event    : Name of the event for which the elapsed time is requested.\n   returns:\n      Elapsed time in microseconds.\n\n\ndouble GetAverage(const std::string&amp; event)\n   Retrieves the average elapsed time of a specified event.\n   This function returns the average elapsed time (in microseconds) for the given event, \n   calculated from all recorded start and end points.\n   parameters:\n      [in] event    : Name of the event for which the average elapsed time is requested.\n   returns:\n      Average elapsed time in microseconds.\n\n\nstatic Profiler&amp; GetInstance()\n   Retrieves the singleton instance of the Profiler.\n   This function returns a reference to the pre-created singleton instance of \n   the `Profiler`. Users should not create their own instance.\n   returns:\n      A reference to the singleton instance of `dxrt::Profiler`.\n\n\nvoid Save(const std::string&amp; file)\n   Saves the timing data of all events to a specified file.\n   This function exports all recorded timing data for each event and saves it to the \n   given file. The data can be used for further analysis or reporting.\n   parameters:\n      [in] file     : Name of the file where the timing data will be saved.\n\n\nvoid Show(bool  showDurations = false)\n   Displays the elapsed times for all events.\n   This function prints the elapsed times for each event. If `showDurations` is set to true, \n   the durations of each event will also be shown.\n   parameters:\n      [in] showDurations : If true, displays the durations of each event.\n                           If false, only the elapsed times are shown.\n\n\nvoid Start(const std::string&amp; event)\n   Records the start point of a specified event.\n   This function marks the beginning of an event, allowing for the calculation of \n   its duration when paired with an end point.\n   parameters:\n      [in] event    : Name of the event to mark as started.\n</code></pre>"},{"location":"docs/07_01_C%2B%2B_API_Reference.html#tensor","title":"Tensor","text":"<pre><code>class Tensor\nRepresents a DX-RT tensor object, which defines a data array composed of uniform elements.\nThe `Tensor` class abstracts a tensor object that holds a multi-dimensional array of data \nelements, typically used in machine learning models. This tensor is generally connected to \ninference engine objects for computations.\n\nvoid* data(int  height, int width, int channel)\n   Retrieves a pointer to a specific element in the tensor by its indices.\n   This function returns the address of the element at the specified indices \n   (height, width, channel) for a tensor in NHWC (height, width, channel) data format.\n   parameters:\n      [in] height   : The height index of the desired element.\n      [in] width    : The width index of the desired element.\n      [in] channel  : The channel index of the desired element.\n   returns:\n      A pointer to the specified tensor element at the given indices (height, width, channel).\n</code></pre>"},{"location":"docs/07_02_Python_API_Reference.html","title":"Python API Reference","text":""},{"location":"docs/07_02_Python_API_Reference.html#python>_api>_reference","title":"Python API Reference","text":""},{"location":"docs/07_02_Python_API_Reference.html#inference>_engine","title":"Inference Engine","text":"<pre><code>class InferenceEngine\nThis class provides an abstraction for the runtime inference executor of the user's compiled model. Once the user loads a compiled model into the InferenceEngine, real-time device tasks are managed and scheduled by internal runtime libraries. It supports both synchronous and asynchronous inference modes, depending on the user's request.\n\nInferenceEngine(model_path: str, inference_option=None)\n   Constructs an InferenceEngine instance.\n   parameters:\n      [in] model_path       : Path to the model file.\n      [in] inference_option     : Inference options, including device and NPU core settings. \n                                (default: DefaultInferenceOption).\n\n\nget_all_task_outputs()\n   Retrieves the outputs from all tasks in sequence.\n   returns:\n      The outputs from all tasks in order.\n\n\nget_outputs()\n   @deprecated: Use `get_all_task_outputs()` instead.\n   Retrieves the outputs from all tasks in sequence.\n   returns:\n       The outputs from all tasks in order.\n\n\nget_compile_type()\n   Retrieves the compile type of the model.\n   returns:\n      The compile type of the model.\n\n\nget_input_data_type()\n   Retrieves the required output data types as a list of strings.\n   returns:\n      A list of strings representing the input data types.\n\n\ninput_dtype()\n   @deprecated: Use `get_input_data_type()` instead.\n   Retrieves the required output data types as a list of strings.\n   returns:\n      A list of strings representing the input data types.\n\n\nget_input_size()\n   Retrieves the total size of input tensors.\n   This function returns the total memory size required for the input tensors \n   of a single inference operation.\n   returns:\n      Input size in bytes for one inference.\n\n\ninput_size()\n   @deprecated: Use `get_input_size()` instead.\n   Retrieves the total size of input tensors.\n   This function returns the total memory size required for the input tensors \n   of a single inference operation.\n   returns:\n      Input size in bytes for one inference.\n\n\nget_latency()\n   Retrieves the most recent inference latency.\n   This function returns the latency taken for the most recent inference operation.\n   returns:\n      Latency in microseconds.\n\n\nlatency()\n   @deprecated: Use `get_latency()` instead.\n   Retrieves the most recent inference latency.\n   This function returns the latency taken for the most recent inference operation.\n   returns:\n      Latency in microseconds.\n\n\nget_latency_count()\n   Retrieves the number of latency measurements.  \n   This function returns the total count of latency measurements. \n   returns:\n      The count of latency measurements.\n\n\nget_latency_list()\n   Retrieves a list of inference latency values.\n   returns:\n      A list of inference latency values in microseconds.\n\n\nget_latency_mean()\n   Retrieves the average inference latency.  \n   This function returns the mean latency of inference execution.\n   returns:\n      The average inference latency in microseconds.\n\n\nget_latency_std()\n   Retrieves the standard deviation of inference latency.  \n   This function calculates the variation in inference latency time.\n   returns:\n      The standard deviation of inference latency in microseconds.\n\n\nget_npu_inference_time()\n   Retrieves the most recent inference time.\n   This function returns the duration of the most recent inference execution.\n   returns:\n      Inference time in microseconds.\n\n\ninference_time()\n   @deprecated: Use `get_npu_inference_time()` instead.\n   Retrieves the most recent inference time.\n   This function returns the duration of the most recent inference execution.\n   returns:\n      Inference time in microseconds.\n\n\nget_npu_inference_time_count()\n   Retrieves the number of NPU inference time measurements.  \n   This function returns the total count of NPU inference time measurements.\n   returns:\n      The count of NPU inference time measurements.\n\n\nget_npu_inference_time_list()\n   Retrieves a list of NPU inference time values.\n   returns:\n      A list of inference time values in microseconds.\n\n\nget_npu_inference_time_mean()\n   Retrieves the average NPU inference time.  \n   This function returns the mean execution time for NPU inference. \n   returns:\n      The average NPU inference time in microseconds.\n\n\nget_npu_inference_time_std()\n   Retrieves the standard deviation of NPU inference time.  \n   This function returns the standard deviation of the inference time.\n   returns:\n      The standard deviation of inference time in microseconds.\n\n\nget_num_tail_tasks()\n   Retrieves the number of tail tasks in the model.\n   Tail tasks are tasks that do not have any subsequent tasks in the model's task chain.\n   This function returns the count of such tail tasks. \n   returns:\n      The number of tail tasks.\n\n\nget_num_tails()\n   @deprecated: Use `get_num_tail_tasks()` instead.\n   Retrieves the number of tail tasks in the model.\n   Tail tasks are tasks that do not have any subsequent tasks in the model's task chain.\n   This function returns the count of such tail tasks. \n   returns:\n      The number of tail tasks.\n\n\nget_output_data_type()\n   @deprecated: Use `get_output_data_type()` instead.\n   Retrieves the required output data types as a list of strings.\n   returns:\n      A list of strings representing the output data types.\n\n\noutput_dtype()\n   Retrieves the required output data types as a list of strings.\n   returns:\n      A list of strings representing the output data types.\n\n\nget_output_size()\n   Retrieves the total size of output tensors.\n   This function returns the total memory size required for the output tensors \n   of a single inference operation.\n   returns:\n     Output size in bytes for one inference.\n\n\noutput_size()\n   @deprecated: Use `get_output_size()` instead.\n   Retrieves the total size of output tensors.\n   This function returns the total memory size required for the output tensors \n   of a single inference operation.\n   returns:\n     Output size in bytes for one inference.\n\n\nget_task_order()\n   Retrieves the execution order of tasks in the model.\n   This function returns the sequence of tasks as they are executed within the model.\n   returns:\n      A vector of strings representing the task execution order.\n\n\ntask_order()\n   @deprecated: Use `get_task_order()` instead.\n   Retrieves the execution order of tasks in the model.\n   This function returns the sequence of tasks as they are executed within the model.\n   returns:\n      A vector of strings representing the task execution order.\n\n\nis_ppu()\n   Checks whether PPU is utilized.\n   returns:\n     True if PPU is being utilized, otherwise False.\n\n\nis_PPU()\n   @deprecated: Use `is_ppu()` instead.\n   Checks whether PPU is utilized.\n   returns:\n     True if PPU is being utilized, otherwise False.\n\n\nregister_callBack(callback)\n   Register user callback function to be called by inference completion.\n   parameters:\n      [in] callback : A function that is called when inference is complete.  \n                      It receives two arguments: outputs and user_arg.\n                      - outputs: The data from the output tensors.  \n                      - user_arg: A user-defined argument passed via RunAsync().\n\n\nRegisterCallBack(callback)\n   @deprecated: Use `register_callback()` instead.\n   Register user callback function to be called by inference completion.\n   parameters:\n      [in] callback : A function that is called when inference is complete.  \n                      It receives two arguments: outputs and user_arg.\n                      - outputs: The data from the output tensors.  \n                      - user_arg: A user-defined argument passed via RunAsync().\n\n\nrun(input_feed_list: List[np.ndarray])\n   Runs the inference engine synchronously using the specified input data.  \n   This function executes inference with the provided input data and returns\n   the output tensors.\n   parameters:\n      [in] input_feed_list  : A list of Numpy arrays representing the input data.\n   returns:\n      A list of Numpy arrays representing the output tensors.\n\n\nRun(input_feed_list: List[np.ndarray])\n   @deprecated: Use run() instead.\n   Runs the inference engine synchronously using the specified input data.  \n   This function executes inference with the provided input data and returns\n   the output tensors.\n   parameters:\n      [in] input_feed_list  : A list of Numpy arrays representing the input data.\n   returns:\n      A list of Numpy arrays representing the output tensors.\n\n\nrun_async(input_feed_list: List[np.ndarray], user_arg)\n   Initiates an asynchronous inference request and returns a job ID.\n   This function performs a non-blocking inference operation using the specified input\n   data. It returns a job ID that can be used with the `wait()` function to retrieve \n   the results.\n   parameters:\n      [in] input_feed_list  : input data for inference.\n      [in] user_arg     : user-defined arguments.\n                            (e.g. original frame data, metadata about input, ... )\n   returns:\n      Job ID that can be used with the `wait()` function to retrieve the inference result.\n\n\nRunAsync(input_feed_list: List[np.ndarray], user_arg)\n   @deprecated: Use `run_async()` instead.\n   Initiates an asynchronous inference request and returns a job ID.\n   This function performs a non-blocking inference operation using the specified input\n   data. It returns a job ID that can be used with the `wait()` function to retrieve \n   the results.\n   parameters:\n      [in] input_feed_list  : input data for inference.\n      [in] user_arg     : user-defined arguments.\n                            (e.g. original frame data, metadata about input, ... )\n   returns:\n      Job ID that can be used with the `wait()` function to retrieve the inference result.\n\n\nrun_benchmark(loop_cnt,input_feed_list)\n   Runs a benchmark test by executing inference multiple times.\n   This function performs inference in a loop for the specified number of times \n   and calculates the average frames per second (FPS).\n   parameters:\n      [in] loop_cnt     : Number of inference iterations.\n      [in] input_feed_list  : the input data for inference\n   returns:\n      Average FPS (frames per second) over the benchmark runs.\n\n\nRunBenchMark(loop_cnt,input_feed_list)\n   @deprecated: Use `run_benchmark()` instead.\n   Runs a benchmark test by executing inference multiple times.\n   This function performs inference in a loop for the specified number of times \n   and calculates the average frames per second (FPS).\n   parameters:\n      [in] loop_cnt     : Number of inference iterations.\n      [in] input_feed_list  : the input data for inference\n   returns:\n      Average FPS (frames per second) over the benchmark runs.\n\n\nwait(int jobId)\n   Wait until a request is complete and returns output.\n   parameters:\n      [in] jobId    : job Id returned by run_async()\n   returns:\n      output tensors as vector\n\n\nwait(int jobId)\n   @deprecated: Use `wait()` instead.\n   Wait until a request is complete and returns output.\n   parameters:\n      [in] jobId    : job Id returned by RunAsync()\n   returns:\n      output tensors as vector\n</code></pre>"},{"location":"docs/Appendix_Change_Log.html","title":"Change Log","text":""},{"location":"docs/Appendix_Change_Log.html#v295>_may>_2025","title":"v2.9.5 (May 2025)","text":"<ul> <li>Added full support for Python run_model.  </li> <li>Updated the run_model option and its description  </li> <li>Improve the Python API  <ul> <li>InferenceOption is now supported identically to the C++ API.  </li> <li>set_devices(...) \u2192 devices = [0]  </li> <li>set_bound_option(...) \u2192 bound_option = InferenceOption.BOUND_OPTION.NPU_ALL</li> <li>set_use_ort(...) \u2192 use_ort = True</li> <li>Callback functions registered via register_callback now accept user_arg of custom types. (removed .value)</li> <li>user_arg.value \u2192 user_arg</li> <li>run() now supports both single-input and batch-input modes, depending on the input format.</li> </ul> </li> <li>Modify the build.sh script according to cmake options.  <ul> <li>CMake option USE_ORT=ON, running build.sh --clean installs ONNX Runtime.  </li> <li>CMake option USE_PYTHON=ON, running build.sh installs the Python package.  </li> <li>CMake option USE_SERVICE=ON, running build.sh starts or restarts the service.  </li> </ul> </li> <li>Add dxrt-cli -v to display minimum driver &amp; compiler versions  </li> <li>Addressed multithreading issues by implementing additional locks, improving stability under heavy load.  </li> <li>Fix crash on multi-device environments with more than 2 H1 cards. (&gt;=8 devices)  </li> <li>Resolved data corruption errors that could occur in different scenarios, ensuring data integrity.  </li> <li>Fix profiler bugs.  </li> <li>Addressed issues identified by static analysis and other tools, enhancing code quality and reliability.  </li> <li>Add --use_ort flag to the run_model.py example for ONNX Runtime.  </li> <li>Add run batch function. (Python &amp; C++)  <ul> <li>batch inference with multiple inputs and multiple outputs.  </li> </ul> </li> <li>Minimum model file versions  <ul> <li>.dxnn file format version &gt;= v6  </li> <li>compiler version &gt;= v1.15.2  </li> </ul> </li> <li>Minimum Driver and Firmware versions  <ul> <li>RT Driver Version &gt;= v1.5.0  </li> <li>PCIe Driver Version &gt;= v1.4.0  </li> <li>Firmware Version &gt;= v2.0.5  </li> </ul> </li> </ul>"},{"location":"docs/Appendix_Change_Log.html#v282>_april>_2025","title":"v2.8.2 (April 2025)","text":"<ul> <li>Modify Inference Engine to be used with 'with' statements, and update relevant examples.  </li> <li>Add Python inference option interface with the following configurations  </li> <li>NPU Device Selection / NPU Bound Option / ORT Usage Flag  </li> <li>Display dxnn versions in parse_model (.dxnn file format version &amp; compiler version)  </li> <li>Added instructions on how to retrieve device status information  </li> <li>Driver and Firmware versions  <ul> <li>RT Driver &gt;= v1.3.3  </li> <li>Firmware &gt;= v1.6.3  </li> </ul> </li> </ul>"}]}