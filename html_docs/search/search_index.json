{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"docs/01_DXNN_Runtime_Overview.html","title":"DXNN Runtime Overview","text":"<p>This chapter provides an overview of the DEEPX SDK architecture and explains each core component and its role in the AI development workflow.  </p>"},{"location":"docs/01_DXNN_Runtime_Overview.html#deepx>_sdk>_architecture","title":"DEEPX SDK Architecture","text":"<p>  Figure. DEEPX SDK Architecture    </p> <p>DEEPX SDK is an all-in-one software development platform that streamlines the process of compiling, optimizing, simulating, and deploying AI inference applications on DEEPX NPUs (Neural Processing Units). It provides a complete toolchain, from AI model creation to runtime deployment, optimized for edge and embedded systems, enabling developers to build high-performance AI applications with minimal effort.  </p> <p>DX-COM is the compiler in the DEEPX SDK that converts a pre-trained ONNX model and its associated configuration JSON file into a hardware-optimized .dxnn binary for DEEPX NPUs. The ONNX file contains the model structure and weights, while the JSON file defines pre/post-processing settings and compilation parameters. DX-COM provides a fully compiled .dxnn file, optimized for low-latency and high-efficient inference on DEEPX NPU.  </p> <p>DX-RT is the runtime software responsible for executing ,dxnn models on DEEPX NPU hardware. DX-RT directly interacts with the DEEPX NPU through firmware and device drivers, using PCIe interface for high-speed data transfer between the host and the NPU, and provides C/C++ and Python APIs for application-level inference control. DX-RT offers a complete runtime environment, including model loading, I/O buffer management, inference execution, and real-time hardware monitoring.  </p> <p>DX ModelZoo is a curated collection of pre-trained neural network models optimized for DEEPX NPU, designed to simplify AI development for DEEPX users. It includes pre-trained ONNX models, configuration JSON files, and pre-compiled DXNN binaries, allowing developers to rapidly test and deploy applications. DX ModelZoo also provides benchmark tools for comparing the performance of quantized INT8 models on DEEPX NPU with full-precision FP32 models on CPU or GPU.  </p> <p>DX-STREAM is a custom GStreamer plugin that enables real-time streaming data integration into AI inference applications on DEEPX NPU. It provides a modular pipeline framework with configurable elements for preprocessing, inference, and postprocessing, tailored to vision AI work. DX-Stream allows developers to build flexible, high-performance applications for use cases such as video analytics, smart cameras, and edge AI systems.  </p> <p>DX-APP is a sample application that demonstrates how to run compiled models on actual DEEPX NPU using DX-RT. It includes ready-to-use code for common vision tasks such as object detection, face recognition, and image classification. DX-APP helps developers quickly set up the runtime environment and serves as a template for building and customizing their own AI applications.  </p>"},{"location":"docs/01_DXNN_Runtime_Overview.html#inference>_flow>_of>_dx-rt","title":"Inference Flow of DX-RT","text":"<p>Here is the inference flow of DX-RT.</p> <p>  Figure. Inference Flow of DXNN Runtime    </p> <p>This figure illustrates the inference workflow of the DXNN Runtime SDK, which integrates OpenCV-based input/output handling with efficient NPU-accelerated model execution.</p> <p>Input &amp; Pre-Processing Input data\u2014such as images, camera feeds, or video\u2014is captured using OpenCV. The data is then passed through a Pre-Processing module, which transforms it into \\input tensors suitable for the model.  </p> <p>Feeding Input to the Inference Engine The pre-processed input tensors are fed into the InferenceEngine along with the compiled model (.dxnn). Before execution, you must configure the InferenceOption, which specifies the target device and available resources.  </p> <p>Model Execution The InferenceEngine is the core component of the DXNN Runtime SDK. It: - Initializes and controls the NPU device - Manages memory for input/output tensors - Schedules inference tasks across NPU and CPU, optimizing their interaction for real-time performance</p> <p>Post-Processing &amp; Display The output tensors are processed to a Post-Processing stage, typically involving OpenCV for decoding, formatting, or visualization. Finally, the results are displayed or forwarded to the next processing step. </p>"},{"location":"docs/02_Installation_on_Linux.html","title":"Installation on Linux","text":"<p>This chapter describes the system requirements, source file structure, and the installation instructions for setting up DX-RT on a Linux-based host system.  </p> <p>After you check the system requirements, follow these instructions.  </p> <ul> <li>System Requirement Check  </li> <li>Build Environment Setup  </li> <li>Source File Structure Check  </li> <li>Framework Build  </li> <li>Linux Device Driver Installation  </li> <li>Python Package Installation  </li> <li>Service Registration  </li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#system>_requirements","title":"System Requirements","text":"<p>This section describes the hardware and software requirements for running DX-RT on Linux.</p> <p>Hardware and Software Requirements </p> <ul> <li>CPU: x86_64, aarch64</li> <li>RAM: 8GB RAM (16GB RAM or higher is recommended)</li> <li>Storage: 4GB or higher available disk space</li> <li>OS: Ubuntu 20.04 / 22.04 / 24.04 (x86_64 / aarch64)</li> <li>Hardware: The system must support connection to an M1 M.2 module with the M.2 interface on the host PC. </li> </ul> <p>  Figure. DX-M1 M.2 Module    </p>"},{"location":"docs/02_Installation_on_Linux.html#build>_environment>_setup","title":"Build Environment Setup","text":"<p>DEEPX provides an installation shell script to set up the DX-RT build environment. You can install the entire toolchain installation or perform a partial installation as necessary.</p> <p>DX-RT supports the Target OS of Ubuntu 18.04, Ubuntu 20.04, Ubuntu 22.04, and Ubuntu 24.04. </p> <p>Installation of DX-RT  To install the full DX-RT toolchain, use the following commands.  </p> <pre><code>$ cd dx_rt\n$ ./install.sh --all\n</code></pre> <p>Here are the available <code>install.sh</code> options.  <pre><code>  ./install.sh [ options ]\n    --help            Shows help message\n    --arch [x86_64, aarch64]\n                      Sets target CPU architecture\n    --dep             Installs build dependencies : cmake, gcc, ninja, etc..\n    --onnxruntime     (Optional) Installs onnxruntime library\n    --all             Installs architecture + dependency + onnxruntime library\n</code></pre></p> <p>Installation with ONNX Runtime Use the ONNX Runtime option if you need to offload certain neural network (NN) operations to the CPU that are not supported by the NPU.  </p> <p>We recommend using ONNX Runtime linux x64 version more than v1.20.1. <pre><code>https://github.com/microsoft/onnxruntime/releases/download/v1.20.1/onnxruntime-linux-x64-1.20.1.tgz\n$ sudo tar -xvzf onnxruntime-linux-x64-1.20.1.tgz -C /usr/local --strip-components=1\n$ sudo ldconfig\n</code></pre></p> <p>To install the ONNX Runtime library, run the following command. <pre><code>./install.sh --onnxruntime\n</code></pre></p> <p>Installation for a Specific CPU Architecture The DX-RT targets the x86_64 architecture. If you\u2019re compiling for another architecture (e.g., aarch64), specify it using the <code>--arch</code> option.  <pre><code>./install.sh --arch aarch64 --onnxruntime\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#source>_file>_structure","title":"Source File Structure","text":"<p>The DX-RT source directory is organized as follows.  You can install the full toolchain using the <code>install.sh</code>, and the build and library using <code>build.sh</code>.  </p> <pre><code>\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 cli\n\u251c\u2500\u2500 build.sh\n\u251c\u2500\u2500 build_x86_64\n\u251c\u2500\u2500 build_aarch64\n\u251c\u2500\u2500 cmake\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 examples\n\u251c\u2500\u2500 extern\n\u251c\u2500\u2500 install.sh\n\u251c\u2500\u2500 lib\n\u251c\u2500\u2500 python_package\n\u251c\u2500\u2500 sample\n\u251c\u2500\u2500 service\n\u2514\u2500\u2500 tool\n</code></pre> <ul> <li><code>assets</code>: Images for documentation</li> <li><code>bin</code>: Compiled binary executables</li> <li><code>cli</code>: Command-line application source code</li> <li><code>build.sh</code>: Shell script for building the framework</li> <li><code>build_arch</code>: Build outputs for aarch64 architecture</li> <li><code>cmake</code>: CMake scripts for build configuration</li> <li><code>docs</code>: Markdown documents </li> <li><code>examples</code>: Inference example files</li> <li><code>extern</code>: Third-party libraries</li> <li><code>install.sh</code>: Shell script for toolchain installation</li> <li><code>lib</code>: DX-RT library sources</li> <li><code>python_package</code>: Python modules for DX-RT</li> <li><code>sample</code>: Sample input files for demo apps</li> <li><code>service</code>: Service unit files for runtime management</li> <li><code>tool</code>: Profiler result visualization tools</li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#framework>_build>_on>_linux","title":"Framework Build on Linux","text":"<p>After compiling the DX-RT environment setup, you can build the framework using the provided <code>build.sh</code> shell script.</p>"},{"location":"docs/02_Installation_on_Linux.html#framework>_source>_build","title":"Framework Source Build","text":"<p>DEEPX supports the default target CPU architecture as x86_64, aarch64.  </p> <p>The build script also supports options for build cleaning, specifying build type, and installing libraries to the system paths.  </p> <p>Build Instructions To build the DX-RT framework, run the following command. <pre><code>$ cd dx_rt\n$ ./build.sh\n</code></pre></p> <p>Here are the <code>build.sh</code> options and their descriptions. <pre><code>./build.sh [ options ]\n    --help       Shows help message\n    --clean      Cleans previous build artifacts\n    --verbose    Shows full build commands during execution\n    --type [Release, Debug, RelWithDebInfo]\n                 Specifies the cmake build type\n    --arch [x86_64, aarch64]\n                 Sets target CPU architecture\n    --install &lt;path&gt;\n                 Sets the installation path for built libraries\n    --uninstall  Removes installed DX-RT files\n    --clang      Compiles using clang\n</code></pre></p> <p>Example. Build with <code>clean</code> Option To clean existing build files before rebuilding. <pre><code>$ ./build.sh --clean\n</code></pre></p> <p>Example. Build with <code>library</code> Option To install build library files to <code>/usr/local</code>.  <pre><code># default path is /usr/local\n$ ./build.sh --install /usr/local\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#options>_for>_build>_target","title":"Options for Build Target","text":"<p>DX-RT supports configuration  build targets, allowing you to enable or disable option features such as  ONNX Runtime, Python API, multi-process service support, and shared library builds.  </p> <p>You can configure these options by editing the following file: <code>cmake/dxrt.cfg.cmake</code> </p> <p>Here are the available options for building targets. <pre><code>option(USE_ORT \"Use ONNX Runtime\" OFF)\noption(USE_PYTHON \"Use Python\" OFF)\noption(USE_SERVICE \"Use Service\" OFF)\noption(USE_SHARED_DXRT_LIB \"Build for DX-RT Shared Library\" ON)\n</code></pre></p> <ul> <li><code>USE_ORT</code>: Enables ONNX Runtime for NN (neural network) operations that NPU does not support</li> <li><code>USE_PYTHON</code>: Enables Python API support</li> <li><code>USE_SERVICE</code>: Enables service for multi-process support </li> <li><code>USE_SHARED_DXRT_LIB</code>: Builds DX-RT as shared library (default: ON)</li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#build>_guide>_for>_cross-compile","title":"Build Guide for Cross-compile","text":"<p>Setup Files for Cross-compile  DEEPX supports cross-compilation for the following default target CPU Architecture: x86_64, aarch64. DEEPX supports  the default target CPU architecture as x86_64.  </p> <p>Toolchain Configuration To cross-compile for a specific target, configure the toolchain file. <code>cmake/toolchain.&lt;CMAKE_SYSTEM_PROCESSOR&gt;.cmake</code></p> <p>Example To cross-compile files for aarch64. <pre><code>SET(CMAKE_C_COMPILER /usr/bin/aarch64-linux-gnu-gcc )\nSET(CMAKE_CXX_COMPILER /usr/bin/aarch64-linux-gnu-g++ )\nSET(CMAKE_LINKER /usr/bin/aarch64-linux-gnu-ld )\nSET(CMAKE_NM /usr/bin/aarch64-linux-gnu-nm )\nSET(CMAKE_OBJCOPY /usr/bin/aarch64-linux-gnu-objcopy )\nSET(CMAKE_OBJDUMP /usr/bin/aarch64-linux-gnu-objdump )\nSET(CMAKE_RANLIB /usr/bin/aarch64-linux-gnu-ranlib )\n</code></pre></p> <p>Non Cross-compile Case (Build on Host) To build and install DX-RT on the host system, run the following command. <pre><code>./build.sh --install /usr/local\n</code></pre></p> <p>Recommended install path: <code>/usr/local</code> (commonly included in OS search paths)  </p> <p>Cross-compile Case (Build for Target Architecture) Cross-compile for a specific architecture, run the following command. <pre><code>./build.sh --arch &lt;target_cpu&gt;\n</code></pre></p> <p>Here are the <code>build.sh</code> options and their descriptions. <pre><code>./build.sh [ options ]\n   --help   Shows help message\n   --clean  Cleans previous build artifacts\n   --verbose    Shows full build commands during execution\n   --type   Specifies the cmake build type : [ Release, Debug, RelWithDebInfo ]\n   --arch   Sets target CPU architecture : [ x86_64, aarch64 ]\n   --install    Installs build libraries \n   --uninstall  Removes installed DX-RT files\n</code></pre></p> <p>Here are the examples of cross-compile cases. <pre><code>./build.sh --arch aarch64\n./build.sh --arch x86_64\n</code></pre></p> <p>Output Directory After a successful build, output binaries is located under <code>&lt;build directory&gt; /bin/</code> <pre><code>&lt;build directory&gt;/bin/\n \u251c\u2500\u2500 dxrtd\n \u251c\u2500\u2500 dxrt-cli\n \u251c\u2500\u2500 parse_model\n \u251c\u2500\u2500 run_model\n \u2514\u2500\u2500 examples\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#linux>_device>_driver>_installation","title":"Linux Device Driver Installation","text":"<p>After building the DX-RT framework, you can install the Linux device driver for M1 AI Accelerator (NPU).  </p>"},{"location":"docs/02_Installation_on_Linux.html#prerequisites","title":"Prerequisites","text":"<p>Before installing the Linux device driver, you should check that the accelerator device is properly recognized by the system.  </p> <p>To check PCIe device recognition, run the following command. <pre><code>$ lspci -vn | grep 1ff4\n0b:00.0 1200: 1ff4:0000\n</code></pre></p> <p>Note. If there is no output, the PCIe link is not properly connected. Please check the physical connection and system BIOS settings.  </p> <p>Optional. Display the DEEPX name in <code>lspci</code>. If you want to display the DEEPX name in <code>lspci</code>, you can modify the PCI DB. (Only for Ubuntu)  </p> <p>To display the DeepX device name, run the following command. <pre><code>$ sudo update-pciids\n$ lspci\n...\n0b:00.0 Processing accelerators: DEEPX Co., Ltd. DX_M1\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#linux>_device>_driver>_structure","title":"Linux Device Driver Structure","text":"<p>The DX-RT Linux device driver source is structured to support flexible builds across devices, architectures, and modules. The directory layout is as follows. <pre><code>- .gitmodules\n\n- [modules]\n    |\n    - device.mk     \n    - kbuild\n    - Makefile\n    - build.sh\n    - [rt]\n        - Kbuild\n    - [pci_deepx] : submodule\n        - Kbuild\n\n- [utils] : submodule\n</code></pre></p> <ul> <li><code>device.mk</code>: Device configuration file</li> <li><code>kbuild</code>: Top-level build rules</li> <li><code>Makefile</code>: Build entry point</li> <li><code>build.sh</code>: Build automation script</li> <li><code>rt</code>: Runtime driver source (<code>dxrt_driver.ko</code>)</li> <li><code>pci_deepx</code>: PCIe DMA driver (<code>submodule, dx_dma.ko</code>)</li> <li><code>utils</code>: Supporting utilities (<code>submodule</code>)</li> </ul> <p>Here are the descriptions of the key components.  </p> <p><code>device.mk</code> Defines supported device configuration.  </p> <p>To build for a specific device, run the following command. <pre><code>$ make DEVICE=[device]\n</code></pre></p> <p>For example, in the case of a device like M1, you should select a submodule, such as PCIe, that has a dependency on M1. <pre><code>$ make DEVICE=m1 PCIE=[deepx]\n</code></pre></p> <p><code>kbuild</code> Linux kernel build configuration file for each module directory. It instructs the kernel build system on how to compile driver modules.  </p> <p><code>build.sh</code> Shell script to streamline the build process. It runs the Makefile with common options. </p> <p>Here are the options for <code>build.sh</code>. <pre><code>Usage:\nUsage:\n   build.sh &lt;options&gt;\n\noptions:\n   -d, --device   [device]      select target device: m1\n   -m, --module   [module]      select PCIe module: deepx\n   -k, --kernel   [kernel dir]  'KERNEL_DIR=[kernel dir]', The directory where the \n                                kernel source is located \n                                default: /lib/modules/6.5.0-18-generic/build)\n   -a, --arch     [arch]        set 'ARCH=[arch]' Target CPU architecture for \n                                cross-compilation, default: x86_64\n   -t, --compiler [cross tool]  'CROSS_COMPILE=[cross tool]' cross compiler binary, \n                                e.g aarch64-linux-gnu-\n   -i, --install  [install dir] 'INSTALL_MOD_PATH=[install dir]', module install \n                                directory install to: \n                                [install dir]/lib/modules/[KERNELRELEASE]/extra/\n   -c, --command  [command]     clean | install | uninstall\n                                - uninstall: Remove the module files installed \n                                on the host PC.\n   -j, --jops     [jobs]        set build jobs\n   -f, --debug    [debug]       set debug feature [debugfs | log | all]\n   -v, --verbose                build verbose (V=1)\n   -h, --help                   show this help\n</code></pre></p> <p>The build process generates the following kernel modules.  </p> <ul> <li> <p><code>modules/rt</code> -&gt; <code>dxrt_driver.ko</code>     : a core runtime driver for M1 NPU devices. This is responsible for system-level communication, memory control, and device command execution. </p> </li> <li> <p><code>modules/pci_deepx</code> -&gt; <code>dx_dma.ko</code>     : PCIe DMA (Direct Memory Access) kernel module for high-speed data transfer between host and the M1 device. This enables efficient data movement with minimal CPU overhead, ideal for real-time and data intensive AI workloads. </p> </li> </ul>"},{"location":"docs/02_Installation_on_Linux.html#linux>_device>_driver>_build","title":"Linux Device Driver Build","text":"<p>After completing the environment setup of the DXNN Linux Device Driver, you can build the kernel modules using either the make(Makefile) or <code>build.sh</code> script. Both methods are supported by DEEPX.  </p> <p>Option 1. Build Using <code>Makefile</code> </p> <p><code>build</code> <pre><code>e.g $ cd modules\ne.g $ make DEVICE=m1 PCIE=deepx\n</code></pre></p> <p><code>clean</code> <pre><code>e.g $ cd modules\ne.g $ make DEVICE=m1 PCIE=deepx clean\n</code></pre></p> <p><code>install</code> Installs the driver to: <code>/lib/modules/$(KERNELRELEASE)/extra/</code> <pre><code>e.g $ cd modules\ne.g $ make DEVICE=m1 PCIE=deepx install\n</code></pre></p> <p>Option 2. Build Using <code>build.sh</code> </p> <p>Use this method if your system supports self-compiling kernel modules (<code>.ko</code> files).</p> <p><code>build</code> <pre><code>e.g $ ./build.sh -d m1 -m deepx\n(Default device: m1, PCI3 module: deepx)\n</code></pre></p> <p><code>clean</code> <pre><code>e.g $ ./build.sh -c clean\n</code></pre></p> <p><code>install</code> Installs the driver to: <code>/lib/modules/$(KERNELRELEASE)/extra/</code> <pre><code>e.g $ sudo ./build.sh -c install\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#auto-loading>_modules>_at>_boot>_time","title":"Auto-Loading Modules at Boot Time","text":"<p>DEEPX allows kernel modules to be automatically loaded at system boot, either through manual setup or using the <code>build.sh</code> script.  </p> <p>Manual Installation Method </p> <p>Step 1. Install Kernel Modules Installs modules to: <code>/lib/modules/$(KERNELRELEASE)/extra/</code> <pre><code>make DEVICE=m1 PCIE=deepx install\n</code></pre></p> <p>Step 2. Update Module Dependencies Updates: <code>/lib/modules/$(KERNELRELEASE)/modules.dep</code> <pre><code>$ sudo depmod -A\n</code></pre></p> <p>Step 3. Add Module Confiduration Copy the preconfigured module config file. <pre><code>$ sudo cp modules/dx_dma.conf /etc/modprobe.d/\n</code></pre></p> <p>This ensures the modules (<code>dx_dma</code>) are auto-loaded on boot.  </p> <p>Step 4. Test with modprobe To verify the correct installation. <pre><code>$ sudo modprobe dx_dma\n$ lsmod\n  dxrt_driver            40960  0\n  dx_dma                176128  1 dxrt_driver\n</code></pre></p> <p>Automated Installation Using <code>build.sh</code> The <code>build.sh</code> script automates installation and setup, including dependency updates and modprobe configuration.  </p> <p>Run the following command <pre><code>$ sudo ./build.sh -d m1 -m deepx -c install\n- DEVICE        : m1\n- PCIE          : deepx\n- MODULE CONF   : /.../rt_npu_linux_driver/modules/dx_dma.conf\n- ARCH (HOST)   : x86_64\n- KERNEL        : /lib/modules/5.15.0-102-generic/build\n- INSTALL       : /lib/modules/5.15.0-102-generic/extra/\n\n *** Build : install ***\n$ make DEVICE=m1 PCIE=deepx install\n\nmake -C /lib/modules/5.15.0-102-generic/build M=/home/jhk/deepx/dxrt/module/rt_npu_linux_driver/modules  modules_install\n ....\n - SUCCESS\n\n *** Update : /lib/modules/5.15.0-102-generic/modules.dep ***\n $ depmod -A\n $ cp /home/jhk/deepx/dxrt/module/rt_npu_linux_driver/modules/dx_dma.conf /etc/modprobe.d/\n</code></pre></p> <p>Uninstalling Modules To completely remove the installed modules and configs. <pre><code>$ ./build.sh -d m1 -m deepx -c uninstall\n- DEVICE        : m1\n- PCIE          : deepx\n- MODULE CONF   : /.../rt_npu_linux_driver/modules/dx_dma.conf\n- ARCH (HOST)   : x86_64\n- KERNEL        : /lib/modules/5.15.0-102-generic/build\n- INSTALL       : /lib/modules/5.15.0-102-generic/extra/\n\n *** Remove : /lib/modules/5.15.0-102-generic/extra ***\n $ rm -rf /lib/modules/5.15.0-102-generic/extra/pci_deepx\n $ rm -rf /lib/modules/5.15.0-102-generic/extra/rt\n\n *** Remove : /etc/modprobe.d ***\n $ rm /etc/modprobe.d/dx_dma.conf\n\n *** Update : /lib/modules/5.15.0-102-generic/modules.dep ***\n $ depmod\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#python>_package>_installation","title":"Python Package Installation","text":"<p>DEEPX provides a Python package for DX-RT, available under the module name dx-engine. It supports Python 3.8 or later and allows you to interface with DX-RT in Python-based applications. </p> <p>Installation Steps 1. Navigate to the python_package directory.  <pre><code>$ cd python_package\n</code></pre></p> <p>2. Install the package <pre><code>$ pip install .\n</code></pre></p> <p>3. Verify the installation <pre><code>$ pip list | grep dx\ndx-engine          1.0.0\n</code></pre></p> <p>For details on using DX-RT with Python, refer to Section 6.2 Python in 6. Programming Guide.</p>"},{"location":"docs/02_Installation_on_Linux.html#service>_registration","title":"Service Registration","text":"<p>DX-RT supports multi-process operation through the background service (<code>dxrtd daemon</code>). To enable the multi-process feature, you must build the Runtime with Service support and the service must be registered in the system below.  </p> <p>Note.   - DX-RT must be built with <code>USE_SERVICE=ON</code>. (default setting)   - DX-RT must be registered and managed as a system service using <code>systemd</code>.  </p> <p>Registering and Running the DX-RT Service 1. Modify the service unit file.   Ensure the ExecStart path is correctly configured. <pre><code>$ vi ./service/dxrt.service\n</code></pre></p> <p>2. Copy the service file to the system folder. <pre><code>$ sudo cp ./service/dxrt.service /etc/systemd/system\n</code></pre></p> <p>3. Start the service. <pre><code>$ sudo systemctl start dxrt.service\n</code></pre></p> <p>Service Management Commands <pre><code>$ sudo systemctl stop dxrt.service          # Stop the service\n$ sudo systemctl status dxrt.service        # Check service status\n$ sudo systemctl restart dxrt.service       # Restart the service\n$ sudo systemctl enable dxrt.service        # Enable on boot\n$ sudo systemctl disable dxrt.service       # Disable on boot\n$ sudo journalctl -u dxrt.service           # View service logs\n</code></pre></p>"},{"location":"docs/02_Installation_on_Linux.html#sanity>_check","title":"Sanity Check","text":"<p>The Sanity Check is a script used to quickly verify that a driver has been installed correctly and that the device is recognized properly.</p> <pre><code>$ sudo ./SanityCheck.sh\n============================================================================\n==== Sanity Check Date : DATE ====\nLog file location : .../dx_rt/dx_report/sanity/result/sanity_check_result_[date]_[hh/mm/ss].log\n\n==== PCI Link-up Check ====\n[OK] Vendor ID 1ff4 is present in the PCI devices.(num=2)\n==== Device File Check ====\n[OK] /dev/dxrt0 exists.\n[OK] /dev/dxrt0 is a character device.\n[OK] /dev/dxrt0 has correct permissions (0666).\n[OK] /dev/dxrt1 exists.\n[OK] /dev/dxrt1 is a character device.\n[OK] /dev/dxrt1 has correct permissions (0666).\n==== Kernel Module Check ====\n[OK] dxrt_driver module is loaded.\n[OK] dx_dma module is loaded.\n[OK] PCIe 02:00.0 driver probe is success.\n[OK] PCIe 07:00.0 driver probe is success.\n\n============================================================================\n** Sanity check PASSED!\n============================================================================\n</code></pre>"},{"location":"docs/03_Installation_on_Windows.html","title":"Installation on Windows","text":"<p>This chapter describes the instructions for installing and using DX-RT on a Windows system.  </p>"},{"location":"docs/03_Installation_on_Windows.html#system>_requirements","title":"System Requirements","text":"<p>This section describes the hardware and software requirements for running DX-RT on Windows.  </p> <ul> <li>RAM: 8GB RAM (16GB RAM or higher is recommended)</li> <li>Storage: 4GB or higher available disk space</li> <li>OS: Windows 10 / 11</li> <li>Python: Version 3.11 (for Python module support)</li> <li>Compiler: Visual Studio 2022 (required for building C++ examples)</li> <li>Hardware: The system must support connection to an M1 M.2 module with the M.2 interface on the host PC.  </li> </ul> <p>The current version only supports Single-process and does not support Multi-process.  </p> <p>  Figure. DX-M1 M.2 Module    </p>"},{"location":"docs/03_Installation_on_Windows.html#execute>_installer","title":"Execute Installer","text":"<p>DEEPX provides the Windows installer executable file for DX-RT.  </p> <ul> <li><code>DXNN_Runtime_v[version]_windows_[architecture].exe</code> </li> </ul> <p>Here is an example of the execution file.  </p> <ul> <li><code>DXNN_Runtime_vX.X.X_windows_amd64.exe</code> </li> </ul> <p>Default Directory Path </p> <ul> <li>'C:/DevTools/DXNN/dxrt_v[version]'  </li> </ul> <p>Once you install the exe file, the driver will be installed automatically. So you can verify the installation via Device Manager under DEEPX_DEVICE.  </p> <p>Note. If Visual Studio 2022 is not installed, you may be prompted to install the Microsoft Visual C++ Redistributable (<code>VC_redist.x64.exe</code>) using administrator permissions.  </p>"},{"location":"docs/03_Installation_on_Windows.html#file>_structure","title":"File Structure","text":"<pre><code>\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 drivers\n\u251c\u2500\u2500 examples\n\u251c\u2500\u2500 firmware\n\u251c\u2500\u2500 include\n\u251c\u2500\u2500 lib\n\u2514\u2500\u2500 python_package\n</code></pre> <ul> <li><code>bin</code>: Compiled binary executables</li> <li><code>docs</code>: Markdown documents </li> <li><code>examples</code>: Inference example files</li> <li><code>include</code>: Header files for DX-RT libraries</li> <li><code>lib</code>: Pre-built DX-RT libraries</li> <li><code>python_package</code>: Python modules for DX-RT</li> <li><code>sample</code>: Sample input files for demo apps</li> <li><code>service</code>: Service unit files for runtime management</li> <li><code>tool</code>: Profiler result visualization tools</li> </ul>"},{"location":"docs/03_Installation_on_Windows.html#running>_examples","title":"Running Examples","text":"<p>DX-RT includes sample programs in both C++ and Python.  </p>"},{"location":"docs/03_Installation_on_Windows.html#building>_c>_examples","title":"Building C++ Examples","text":"<p>Visual Studio 2022 should be installed on your PC.  </p> <p>1. Open the solution file in the following location. <code>examples\\&lt;example-name&gt;\\msvc\\&lt;example-name&gt;.sln</code></p> <p>2. In Visual Studio, Click Rebuild Solution.  </p> <p>Once the build is complete, an <code>x64</code> directory is generated in the same location as the solution file. The executable file of the sample includes the Debug or Release sub-folder.  </p>"},{"location":"docs/03_Installation_on_Windows.html#running>_c>_examples","title":"Running C++ Examples","text":"<p>1. Run the executable file of the sample at the following location. <code>examples\\&lt;example-name&gt;\\msvc\\x64\\[Debug|Release]\\&lt;example-name&gt;.exe</code></p> <p>Example <pre><code>C:\\...&gt; cd .\\examples\\run_async_model\\msvc\\x64\\Release\nC:\\...\\examples\\run_async_model\\msvc\\x64\\Release&gt; .\\run_async_model.exe model.dxnn 100\n</code></pre></p>"},{"location":"docs/03_Installation_on_Windows.html#python>_package>_installation","title":"Python Package Installation","text":"<p>DEEPX provides a Python module named <code>dx_engine</code> for Python 3.11.  </p> <p>1. Build and Install the Package Navigate to the Python package directory and install the module.  </p> <pre><code>C:\\...\\dxrt_vX.X.X&gt; cd python_package/\nC:\\...\\dxrt_vX.X.X\\python_package&gt; pip install .\n</code></pre> <p>2. Verify the Installation Open a Python shell and check the installed version.  </p> <pre><code>C:\\...&gt; python\n... \n&gt;&gt;&gt; from dx_engine import version\n&gt;&gt;&gt; print(version.__version__)\n1.0.1\n</code></pre> <p>Examples <pre><code>cd examples/python\nC:\\...\\examples\\python&gt; python run_async_model.py ...model.dxnn 10\n</code></pre></p>"},{"location":"docs/04_Model_Inference.html","title":"Model Inference","text":""},{"location":"docs/04_Model_Inference.html#model>_file>_format","title":"Model File Format","text":"<p>The original ONNX model is converted by DX-COM into the following structure.</p> <pre><code>Model dir.\n    \u2514\u2500\u2500 graph.dxnn\n</code></pre> <ul> <li> <code>graph.dxnn</code> A unified DEEPX artifact that contains  NPU command data, model metadata, model parameters.   </li> </ul> <p>This file is used directly for inference on DEEPX hardware</p>"},{"location":"docs/04_Model_Inference.html#inference>_workflow","title":"Inference Workflow","text":"<p>Here the inference workflow using the DXNN Runtime as follows.  </p> <p>  Figure. Inference Workflow    </p> <ul> <li>Compiled Model and optional InferenceOption are provided to initialize the InferenceEngine.  </li> <li>Pre-processed Input Tensors are passed to the InferenceEngine for inference.  </li> <li>The InferenceEngine produces Output Tensors as a result of the inference.  </li> <li>These outputs are then passed to the Post-Processing stage for interpretation or further action.  </li> </ul>"},{"location":"docs/04_Model_Inference.html#prepare>_the>_model","title":"Prepare the Model","text":"<p>Choose one of the following options.  </p> <ul> <li>Use a pre-built model from DX ModelZoo </li> <li>Compile an ONNX model into the DX-RT format using DX-COM (Refer to the DX-COM User Guide for details.)  </li> </ul>"},{"location":"docs/04_Model_Inference.html#configure>_inference>_options","title":"Configure Inference Options","text":"<p>Create a <code>dxrt::InferenceOption</code> object to configure runtime settings for the inference engine.  </p> <p>Note. This option is temporarily unsupported in the current version, and will be available in the next release.</p>"},{"location":"docs/04_Model_Inference.html#load>_the>_model>_into>_the>_inference>_engine","title":"Load the Model into the Inference Engine","text":"<p>Create a <code>dxrt::InferenceEngine</code> instance using the path to the compiled model directory. Hardware resources are automatically initialized during this step.  </p> <p>If <code>dxrt::InferenceEngine</code> is not provided, a default option is applied.  </p> <pre><code>auto ie = dxrt::InferenceEngine(\"yolov5s.dxnn\");\nauto ie = dxrt::InferenceEngine(\"yolov5s.dxnn\", &amp;option);\n</code></pre>"},{"location":"docs/04_Model_Inference.html#connect>_input>_tensors","title":"Connect Input Tensors","text":"<p>Prepare input buffers for inference.  </p> <p>The following example shows how to initialize the buffer with the appropriate size.  </p> <pre><code>std::vector&lt;uint8_t&gt; inputBuf(ie.GetInputSize(), 0);  \n</code></pre> <p>Refer to DX-APP User Guide for practical examples on connecting inference engines to image sources such as cameras or video, along with the preprocessing routines. </p>"},{"location":"docs/04_Model_Inference.html#inference","title":"Inference","text":"<p>DX-RT provides both synchronous and asynchronous execution modes for flexible inference handling.  </p>"},{"location":"docs/04_Model_Inference.html#run>_->_synchronous>_execution","title":"Run - Synchronous Execution","text":"<p>Use the <code>dxrt::InferenceEngine::Run()</code> method for blocking, single-core inference.  </p> <pre><code>auto outputs = ie.Run(inputBuf.data());\n</code></pre> <ul> <li>This method processes input and output on the same thread.  </li> <li>This method is suitable for simple and sequential workloads.  </li> </ul>"},{"location":"docs/04_Model_Inference.html#run>_->_asynchronous>_execution","title":"Run - Asynchronous Execution","text":""},{"location":"docs/04_Model_Inference.html#with>_wait","title":"With <code>Wait()</code>","text":"<p>Use <code>RunAsync()</code> to perform the inference in non-blocking mode, and retrieve results later with <code>Wait()</code>.  </p> <pre><code>auto jobId = ie.RunAsync(inputBuf.data());\nauto outputs = ie.Wait(jobId);\n</code></pre> <ul> <li>This method is ideal for parallel workloads where inference can run in the background.  </li> <li>This method is continuously executed while waiting for the result.  </li> </ul>"},{"location":"docs/04_Model_Inference.html#with>_callback>_function","title":"With <code>Callback function</code>","text":"<p>Use a callback function to handle output as soon as inference completes. </p> <pre><code>std::function&lt;int(vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt;, void*)&gt; postProcCallBack = \\\n    [&amp;](vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt; outputs, void *arg)\n    {\n        /* Process output tensors here */\n        ... ...\n        return 0;\n    };\nie.RegisterCallback(postProcCallBack)\n</code></pre> <ul> <li>The callback is triggered by a background thread after inference.  </li> <li>You can pass a custom argument to track input/output pairs.</li> </ul> <p>Note. Output data is only valid within the callback scope.  </p>"},{"location":"docs/04_Model_Inference.html#process>_output>_tensors","title":"Process Output Tensors","text":"<p>Once inference is complete, the output tensors are processed using Tensor APIs and custom post-processing logic. You can find the templates and example code in DX-APP to help you implement post-process smoothly. As noted earlier, using callbacks allows for more efficient and real-time post-processing.  </p>"},{"location":"docs/04_Model_Inference.html#multiple>_device>_inference","title":"Multiple Device Inference","text":"<p>This feature is not applicable to single-NPU devices. Basically, the inference engine schedules and manages multiple devices in real time. If the inference option is explicitly set, the inference engine may only use specific devices during real-time inference for the model.  </p>"},{"location":"docs/04_Model_Inference.html#data>_format>_of>_device>_tensor","title":"Data Format of Device Tensor","text":"<p>Compiled models use the NHWC format by default.  </p> <p>However, the input tensor formats on the device side may vary depending on the hardware\u2019s processing type.  </p> <p>Input Tensor Formats </p> Type Compiled Model Format Device Format Data Size <code>Formatter</code> <code>[N, H, W, C]</code> <code>[N, H, W, C]</code> 8-bit <code>IM2COL</code> <code>[N, H, W, C]</code> <code>[N, H, align64(W*C)]</code> 8-bit <ul> <li>Formatter Type Example: <code>[1, 3, 224, 224] (NCHW) -&gt; [1, 224, 224, 3] (NHWC)</code> </li> <li>IM2COL Type Example: <code>[1, 3, 224, 224] (NCHW) -&gt; [1, 224, 224*3+32] (NH, aligned width x channel)</code> </li> </ul> <p>Output Tensor Formats </p> <p>The output tensor format is also aligned with the NHWC format, but with padding applied for alignment.</p> Type Compiled Model Format Device Format <code>Aligned NHWC</code> <code>[N, H, W, C]</code> <code>[N, H, W, align64(C)]</code> <ul> <li>Output Example: <code>[1, 40, 52, 36] (NCHW) -&gt; [1, 52, 36, 40+24]</code>    (Channel size is aligned for optimal memory access.)  </li> </ul> <p>Post-processing can be performed directly without converting formats. API to convert from device format to NCHW/NHWC format will be supported in the next release.  </p>"},{"location":"docs/04_Model_Inference.html#profile>_application","title":"Profile Application","text":""},{"location":"docs/04_Model_Inference.html#gather>_timing>_data>_per>_event","title":"Gather Timing Data per Event","text":"<p>You can profile events within your application using the Profiler APIs. Please refer to Section 8. API reference.  </p> <p>Here is a basic usage example. </p> <pre><code>// Built-in core profiling event\n\n// Enable the profiler\ndxrt::Configuration::GetInstance().SetEnable(dxrt::Configuration::ITEM::PROFILER, true);\n\n// Set attributes to show data in console and save to a file\ndxrt::Configuration::GetInstance().SetAttribute(dxrt::Configuration::ITEM::PROFILER, \n                                                    dxrt::Configuration::ATTRIBUTE::PROFILER_SHOW_DATA, \"ON\");\ndxrt::Configuration::GetInstance().SetAttribute(dxrt::Configuration::ITEM::PROFILER, \n                                                    dxrt::Configuration::ATTRIBUTE::PROFILER_SAVE_DATA, \"ON\");\n\n// User's profiling event\nauto&amp; profiler = dxrt::Profiler::GetInstance();\nprofiler.Start(\"1sec\");\nsleep(1);\nprofiler.End(\"1sec\");\n</code></pre> <p>After the application is finished, <code>profiler.json</code> is created in the working directory.</p>"},{"location":"docs/04_Model_Inference.html#visualize>_profiler>_data","title":"Visualize Profiler Data","text":"<p>You can visualize the profiling results using the following Python script.  </p> <pre><code>python3 tool/profiler/plot.py --input profiler.json\n</code></pre> <p>This generates an image file named <code>profiler.png</code>, providing a detailed view of runtime event timing for performance analysis. </p> <p>  Figure. DX-RT Profiling Report    </p> <p>Script Usage: <code>tool/profiler/plot.py</code> </p> <p>Use this script to draw a timing chart from profiling data generated by DX-RT.</p> <pre><code>usage: plot.py [-h] [-i INPUT] [-o OUTPUT] [-s START] [-e END] [-g]\n</code></pre> <p>Optional Arguments  </p> <ul> <li><code>-h, --help</code>: Show help message and exit  </li> <li><code>-i INPUT, --input INPUT</code>: Input <code>.json</code> file to visualize (e.g., <code>profiler.json</code>)  </li> <li><code>-o OUTPUT, --output OUTPUT</code>: Output image file name to save (e.g., profiler.png)  </li> <li><code>-s START, --start START</code>: Starting position (normalized, &gt; 0.0) within the time interval [0.0-1.0]  </li> <li><code>-e END, --end END</code>: End position (normalized, &lt; 1.0) within the time interval [0.0-1.0]  </li> <li><code>-g, --show_gap</code>: Show time gaps between the start point of each event  </li> </ul> <p>Please refer to usage of <code>tool/profiler/plot.py</code>.  </p> <pre><code>usage: plot.py [-h] [-i INPUT] [-o OUTPUT] [-s START] [-e END] [-g]\n</code></pre>"},{"location":"docs/04_Model_Inference.html#how>_to>_create>_an>_application>_using>_dx-rt","title":"How To Create an Application Using DX-RT","text":"<p>This guide provides step-by-step instructions for creating a new CMake project using the DX-RT library.</p> <p>1. Build the DX-RT Library  Before starting, make sure the DX-RT library is already built.  </p> <p>Refer to Chapter 2. Installation on Linus and Chapter 3. Installation on Windows for detailed build instructions. </p> <p>2. Create a New CMake Project  Create a project directory and an initial <code>CMakeLists.txt</code> file. <pre><code>mkdir MyProject\ncd MyProject\ntouch CMakeLists.txt\n</code></pre></p> <p>3. \u201cHello World\u201d with DX-RT API Create a simple source file (<code>main.cpp</code>) that uses a DX-RT API.  </p> <pre><code>#include \"dxrt/dxrt_api.h\"\nusing namespace std;\n\nint main(int argc, char *argv[])\n{\n auto&amp; devices = dxrt::CheckDevices();\n cout &lt;&lt; \"hello, world\" &lt;&lt; endl;\n return 0;\n}\n</code></pre> <p>4. Modify CMakeLists.txt Edit the <code>CMakeLists.txt</code> file as follows.  </p> <pre><code>cmake_minimum_required(VERSION 3.14)\nproject(app_template)\n\nset(CMAKE_CXX_STANDARD_REQUIRED \"ON\")\nset(CMAKE_CXX_STANDARD \"14\")\n\n# Set the DX-RT library installation path (adjust as needed)\nset(DXRT_LIB_PATH \"/usr/local/lib\") \n\n# Locate the DX-RT library\nfind_library(DXRT_LIBRARY REQUIRED NAMES dxrt_${CMAKE_SYSTEM_PROCESSOR} PATHS $\n{DXRT_LIB_PATH})\n\n# Add executable and link libraries\nadd_executable(HelloWorld main.cpp)\ntarget_link_libraries(HelloWorld PRIVATE ${DXRT_LIBRARY} protobuf)\n</code></pre> <p>Replace <code>/usr/local/lib</code> with the actual path where the DX-RT library is installed.</p> <p>5. Build the Project Compile your project using the following commands. <pre><code>mkdir build\ncd build\ncmake ..\nmake\n</code></pre></p> <p>6. Run the Executable After a successful build, run the generated executable.   </p> <pre><code>./HelloWorld\n</code></pre> <p>You now successfully create and build a CMake project using the DX-RT library. </p>"},{"location":"docs/04_Model_Inference.html#optional>_improving>_cpu>_task>_throughput>_with>_dxrt>_dynamic>_cpu>_thread","title":"(Optional) Improving CPU Task Throughput with DXRT_DYNAMIC_CPU_THREAD","text":"<p>The USE_ORT option allows for enabling ONNX Runtime to handle operations that are not supported by the NPU.  When this option is active, the model's CPU tasks are executed via ONNX Runtime. </p> <p>To mitigate potential bottlenecks in these CPU tasks, especially under varying Host CPU conditions, an optional dynamic multi-threading feature is provided. This feature monitors the input queue load to identify CPU task congestion. If a high load is detected, it dynamically increases the number of threads allocated to CPU tasks, thereby improving their throughput. This dynamic CPU threading can be enabled by setting the DXRT_DYNAMIC_CPU_THREAD=ON environment variable (e.g., export DXRT_DYNAMIC_CPU_THREAD=ON). </p> <p>Additionally, if the system observes that CPU tasks are experiencing significant load, it will display a message: \"To improve FPS, set: 'export DXRT_DYNAMIC_CPU_THREAD=ON'\", recommending the activation of this feature for better performance.</p> <p>Warning: Enabling the DXRT_DYNAMIC_CPU_THREAD=ON option does not always guarantee an FPS increase; its effectiveness can vary depending on the specific workload and system conditions.</p>"},{"location":"docs/05_Command_Line_Interface.html","title":"Command Line Interface","text":"<p>This chapter introduces DX-RT command-line tools for model inspection, execution, and device management.</p>"},{"location":"docs/05_Command_Line_Interface.html#parse>_model","title":"Parse Model","text":"<p>This tool is used to parse and inspect a compiled model file (<code>.dxnn</code>), printing model structure and metadata.  </p> <p>Source: <code>bin/parse_model.cpp</code></p> <p>Usage <pre><code>parse_model -m &lt;model_dir&gt;\n</code></pre></p> <p>Option </p> <ul> <li><code>-m, --model</code>: Path to the compiled model file  (<code>.dxnn</code>)  </li> <li><code>-h, --help</code>: Show help message  </li> </ul> <p>Example <pre><code>$./parse_model -m model.dxnn\n</code></pre></p>"},{"location":"docs/05_Command_Line_Interface.html#run>_model","title":"Run Model","text":"<p>This tool runs a compiled model and performs a basic inference test. It measures inference time, validates output data against a reference, and optionally runs in a loop for stress testing.  </p> <p>Source: <code>bin/run_model.cpp</code></p> <p>Usage <pre><code>run_model -m &lt;model_dir&gt; -i &lt;input_bin&gt; -o &lt;output_bin&gt; -r &lt;reference output_bin&gt; -l &lt;number of loops&gt;\n</code></pre></p> <p>Option </p> <ul> <li><code>-c, --config</code>: Path to a JSON configuration file  </li> <li><code>-m, --model</code>: Path to the compiled model file (<code>.dxnn</code>)  </li> <li><code>-i, --input</code>: Input binary file  </li> <li><code>-o, --output</code>: Output file to save inference results  </li> <li><code>-r, --ref</code>: Reference output file to compare results  </li> <li><code>-l, --loop</code>: Number of inference iteration to run (loop test)</li> <li><code>--use-ort</code>: use ONNX Runtime</li> <li><code>-h, --help</code>: Show help message  </li> </ul> <p>Example <pre><code>$ run_model -m /.../model.dxnn -i /.../input.bin -l 100\n</code></pre></p>"},{"location":"docs/05_Command_Line_Interface.html#dx-rt>_cli>_tool>_firmware>_interface","title":"DX-RT CLI Tool (Firmware Interface)","text":"<p>This tool provides a command-line interface to interact with DX-RT accelerator devices. It supports querying device status, resetting hardware, updating firmware, and more.  </p> <p>Note. This tool is applicable only for accelerator devices.  </p> <p>Usage <pre><code>dxrt-cli &lt;option&gt; &lt;argument&gt;\n</code></pre></p> <p>Option </p> <ul> <li><code>-s, --status</code>: Get current device status  </li> <li><code>-i, --info</code>: Display basic device information  </li> <li><code>-m, --monitor</code>: Monitoring device status every [arg] seconds (arg &gt; 0)  </li> <li><code>-r, --reset</code>: Reset device (0: NPU only, 1: full device) (default: 0)  </li> <li><code>-d, --device</code>: Specify device ID (default: -1 for all device)  </li> <li><code>-u, --fwupdate</code>: Update firmware with a Deepx firmware file (options: force:, unreset)  </li> <li><code>-w, --fwupload</code>: Update firmware file (2nd_boot or rtos)  </li> <li><code>-g, --fwversion</code>: Check firmware version from a firmware file  </li> <li><code>-p, --dump</code>: Dump initial device state to a file  </li> <li><code>-l, --fwlog</code>: Extract firmware logs to a file  </li> <li><code>-h, --help</code>: Show help message  </li> </ul> <p>Example <pre><code>$ dxrt-cli --status\n\n$ dxrt-cli --reset 0\n\n$ dxrt-cli --fwupdate fw.bin\n\n$ dxrt-cli -m 1\n</code></pre></p>"},{"location":"docs/06_Inference_API.html","title":"Inference API","text":""},{"location":"docs/06_Inference_API.html#c>_inference>_engine>_api","title":"C++ Inference Engine API","text":""},{"location":"docs/06_Inference_API.html#synchronous>_inference>_api","title":"Synchronous Inference API","text":""},{"location":"docs/06_Inference_API.html#run>_single>_inputoutput","title":"Run (Single Input/Output)","text":"<pre><code>TensorPtrs Run(void *inputPtr, void *userArg = nullptr, void *outputPtr = nullptr)\n</code></pre> Input Format Description Model Type Output Format Notes <code>void* inputPtr</code> Single input pointer Single-Input <code>TensorPtrs</code> (Vector) Traditional method <code>void* inputPtr</code> Concatenated buffer pointer Multi-Input <code>TensorPtrs</code> (Vector) Auto-split applied <p>Example:</p> <pre><code>// Single input model\nauto outputs = ie.Run(inputData);\n\n// Multi-input model (auto-split)\nauto outputs = ie.Run(concatenatedInput);\n</code></pre>"},{"location":"docs/06_Inference_API.html#run>_batch","title":"Run (Batch)","text":"<pre><code>std::vector&lt;TensorPtrs&gt; Run(\n    const std::vector&lt;void*&gt;&amp; inputBuffers,\n    const std::vector&lt;void*&gt;&amp; outputBuffers,\n    const std::vector&lt;void*&gt;&amp; userArgs\n)\n</code></pre> Input Format Condition Interpretation Output Format Notes <code>vector&lt;void*&gt;</code> (size=1) Single-Input Single Inference <code>vector&lt;TensorPtrs&gt;</code> (size=1) Special case <code>vector&lt;void*&gt;</code> (size=N) Single-Input Batch Inference <code>vector&lt;TensorPtrs&gt;</code> (size=N) N samples <code>vector&lt;void*&gt;</code> (size=M) Multi-Input, M==input_count Single Inference <code>vector&lt;TensorPtrs&gt;</code> (size=1) Multi-input single <code>vector&lt;void*&gt;</code> (size=N*M) Multi-Input, N*M==multiple Batch Inference <code>vector&lt;TensorPtrs&gt;</code> (size=N) N samples, M inputs <p>Example:</p> <pre><code>// Single input batch\nstd::vector&lt;void*&gt; batchInputs = {sample1, sample2, sample3};\nauto batchOutputs = ie.Run(batchInputs, outputBuffers, userArgs);\n\n// Multi-input single\nstd::vector&lt;void*&gt; multiInputs = {input1, input2}; // M=2\nauto singleOutput = ie.Run(multiInputs, {outputBuffer}, {userArg});\n\n// Multi-input batch\nstd::vector&lt;void*&gt; multiBatch = {s1_i1, s1_i2, s2_i1, s2_i2}; // N=2, M=2\nauto batchOutputs = ie.Run(multiBatch, outputBuffers, userArgs);\n</code></pre>"},{"location":"docs/06_Inference_API.html#runmultiinput>_dictionary","title":"RunMultiInput (Dictionary)","text":"<pre><code>TensorPtrs RunMultiInput(\n    const std::map&lt;std::string, void*&gt;&amp; inputTensors,\n    void *userArg = nullptr,\n    void *outputPtr = nullptr\n)\n</code></pre> Input Format Constraints Output Format Notes <code>map&lt;string, void*&gt;</code> Must include all input tensor names <code>TensorPtrs</code> For multi-input models only <p>Example:</p> <pre><code>std::map&lt;std::string, void*&gt; inputs = {\n    {\"input1\", data1},\n    {\"input2\", data2}\n};\nauto outputs = ie.RunMultiInput(inputs);\n</code></pre>"},{"location":"docs/06_Inference_API.html#runmultiinput>_vector","title":"RunMultiInput (Vector)","text":"<pre><code>TensorPtrs RunMultiInput(\n    const std::vector&lt;void*&gt;&amp; inputPtrs,\n    void *userArg = nullptr,\n    void *outputPtr = nullptr\n)\n</code></pre> Input Format Constraints Output Format Notes <code>vector&lt;void*&gt;</code> size == input_tensor_count <code>TensorPtrs</code> Order matches GetInputTensorNames()"},{"location":"docs/06_Inference_API.html#asynchronous>_inference>_api","title":"Asynchronous Inference API","text":""},{"location":"docs/06_Inference_API.html#runasync>_single","title":"RunAsync (Single)","text":"<pre><code>int RunAsync(void *inputPtr, void *userArg = nullptr, void *outputPtr = nullptr)\n</code></pre> Input Format Model Type Output Format Notes <code>void* inputPtr</code> Single-Input <code>int</code> (jobId) Result received via Wait(jobId) <code>void* inputPtr</code> Multi-Input <code>int</code> (jobId) Auto-split applied"},{"location":"docs/06_Inference_API.html#runasync>_vector","title":"RunAsync (Vector)","text":"<pre><code>int RunAsync(const std::vector&lt;void*&gt;&amp; inputPtrs, void *userArg = nullptr, void *outputPtr = nullptr)\n</code></pre> Input Format Condition Interpretation Output Format Notes <code>vector&lt;void*&gt;</code> (size==input_count) Multi-Input Multi-input single <code>int</code> (jobId) Recommended method <code>vector&lt;void*&gt;</code> (size!=input_count) Any Uses only the first element <code>int</code> (jobId) Fallback"},{"location":"docs/06_Inference_API.html#runasyncmultiinput>_dictionary","title":"RunAsyncMultiInput (Dictionary)","text":"<pre><code>int RunAsyncMultiInput(\n    const std::map&lt;std::string, void*&gt;&amp; inputTensors,\n    void *userArg = nullptr,\n    void *outputPtr = nullptr\n)\n</code></pre> Input Format Constraints Output Format Notes <code>map&lt;string, void*&gt;</code> For multi-input models only <code>int</code> (jobId) Most explicit method"},{"location":"docs/06_Inference_API.html#runasyncmultiinput>_vector","title":"RunAsyncMultiInput (Vector)","text":"<pre><code>int RunAsyncMultiInput(\n    const std::vector&lt;void*&gt;&amp; inputPtrs,\n    void *userArg = nullptr,\n    void *outputPtr = nullptr\n)\n</code></pre> Input Format Constraints Output Format Notes <code>vector&lt;void*&gt;</code> size == input_tensor_count <code>int</code> (jobId) Converted to a dictionary internally"},{"location":"docs/06_Inference_API.html#wait","title":"Wait","text":"<pre><code>TensorPtrs Wait(\n    int jobId\n)\n</code></pre>"},{"location":"docs/06_Inference_API.html#python>_inference>_engine>_api","title":"Python Inference Engine API","text":""},{"location":"docs/06_Inference_API.html#synchronous>_inference>_api_1","title":"Synchronous Inference API","text":""},{"location":"docs/06_Inference_API.html#run>_unified>_api","title":"run (Unified API)","text":"<pre><code>def run(\n    input_data: Union[np.ndarray, List[np.ndarray], List[List[np.ndarray]]],\n    output_buffers: Optional[Union[List[np.ndarray], List[List[np.ndarray]]]] = None,\n    user_args: Optional[Union[Any, List[Any]]] = None\n) -&gt; Union[List[np.ndarray], List[List[np.ndarray]]]\n</code></pre> <p>Detailed Input/Output Matrix:</p> Input Type Input Condition Model Type Interpretation Output Type Output Structure <code>np.ndarray</code> size == total_input_size Multi-Input Auto-split single <code>List[np.ndarray]</code> Single sample output <code>np.ndarray</code> size != total_input_size Single-Input Single Inference <code>List[np.ndarray]</code> Single sample output <code>List[np.ndarray]</code> len == 1 Single-Input Single Inference <code>List[np.ndarray]</code> Single sample output <code>List[np.ndarray]</code> len == input_count Multi-Input Single Inference <code>List[np.ndarray]</code> Single sample output <code>List[np.ndarray]</code> len == N*input_count Multi-Input Batch Inference (N samples) <code>List[List[np.ndarray]]</code> N sample outputs <code>List[np.ndarray]</code> len &gt; 1 Single-Input Batch Inference <code>List[List[np.ndarray]]</code> <code>len</code> sample outputs <code>List[List[np.ndarray]]</code> Explicit batch Any Batch Inference <code>List[List[np.ndarray]]</code> Matches outer list size <p>Auto-split Special Cases:</p> Condition Example Input Interpretation Output Multi-input + first element is total_size <code>[concatenated_array]</code> Auto-split single <code>List[np.ndarray]</code> Multi-input + all elements are total_size <code>[concat1, concat2, concat3]</code> Auto-split batch <code>List[List[np.ndarray]]</code> <p>Example:</p> <pre><code># 1. Single array auto-split (multi-input)\nconcatenated = np.zeros(ie.get_input_size(), dtype=np.uint8)\noutputs = ie.run(concatenated)  # List[np.ndarray]\n\n# 2. Multi-input single\ninput_list = [input1_array, input2_array]  # len == 2\noutputs = ie.run(input_list)  # List[np.ndarray]\n\n# 3. Multi-input batch (flattened)\nflattened = [s1_i1, s1_i2, s2_i1, s2_i2]  # 2 samples, 2 inputs each\noutputs = ie.run(flattened)  # List[List[np.ndarray]], len=2\n\n# 4. Multi-input batch (explicit)\nexplicit_batch = [[s1_i1, s1_i2], [s2_i1, s2_i2]]\noutputs = ie.run(explicit_batch)  # List[List[np.ndarray]], len=2\n\n# 5. Single-input batch\nsingle_batch = [sample1, sample2, sample3]\noutputs = ie.run(single_batch)  # List[List[np.ndarray]], len=3\n</code></pre>"},{"location":"docs/06_Inference_API.html#run>_multi>_input>_dictionary","title":"run_multi_input (Dictionary)","text":"<pre><code>def run_multi_input(\n    input_tensors: Dict[str, np.ndarray],\n    output_buffers: Optional[List[np.ndarray]] = None,\n    user_arg: Any = None\n) -&gt; List[np.ndarray]\n</code></pre> Input Type Constraints Output Type Notes <code>Dict[str, np.ndarray]</code> Must include all input tensors <code>List[np.ndarray]</code> For multi-input models only"},{"location":"docs/06_Inference_API.html#asynchronous>_inference>_api_1","title":"Asynchronous Inference API","text":""},{"location":"docs/06_Inference_API.html#run>_async","title":"run_async","text":"<pre><code>def run_async(\n    input_data: Union[np.ndarray, List[np.ndarray]],\n    user_arg: Any = None,\n    output_buffer: Optional[Union[np.ndarray, List[np.ndarray]]] = None\n) -&gt; int\n</code></pre> Input Type Condition Interpretation Output Type Constraints <code>np.ndarray</code> Any Single Inference <code>int</code> (jobId) Batch not supported <code>List[np.ndarray]</code> len == input_count Multi-input single <code>int</code> (jobId) Batch not supported <code>List[np.ndarray]</code> len == 1 Single-input single <code>int</code> (jobId) Batch not supported"},{"location":"docs/06_Inference_API.html#run>_async>_multi>_input","title":"run_async_multi_input","text":"<pre><code>def run_async_multi_input(\n    input_tensors: Dict[str, np.ndarray],\n    user_arg: Any = None,\n    output_buffer: Optional[List[np.ndarray]] = None\n) -&gt; int\n</code></pre> Input Type Constraints Output Type Notes <code>Dict[str, np.ndarray]</code> For multi-input models only <code>int</code> (jobId) Single inference only"},{"location":"docs/06_Inference_API.html#wait_1","title":"wait","text":"<pre><code>def wait(job_id: int) -&gt; List[np.ndarray]\n</code></pre>"},{"location":"docs/06_Inference_API.html#input>_format>_analysis>_logic","title":"Input Format Analysis Logic","text":""},{"location":"docs/06_Inference_API.html#python>_input>_analysis>_flow","title":"Python Input Analysis Flow","text":"<pre><code>def _analyze_input_format(input_data):\n    # 1. Check for np.ndarray\n    if isinstance(input_data, np.ndarray):\n        if should_auto_split_input(input_data):\n            return auto_split_single_inference()\n        else:\n            return single_inference()\n\n    # 2. Check for List\n    if isinstance(input_data, list):\n        if isinstance(input_data[0], list):\n            # List[List[np.ndarray]] - Explicit batch\n            return explicit_batch_inference()\n        else:\n            # List[np.ndarray] - Requires complex analysis\n            return analyze_list_ndarray(input_data)\n</code></pre>"},{"location":"docs/06_Inference_API.html#listnpndarray>_analysis>_details","title":"<code>List[np.ndarray]</code> Analysis Details","text":"<pre><code>def analyze_list_ndarray(input_data):\n    input_count = len(input_data)\n\n    if is_multi_input_model():\n        expected_count = get_input_tensor_count()\n\n        if input_count == expected_count:\n            return single_inference()\n        elif input_count % expected_count == 0:\n            batch_size = input_count // expected_count\n            return batch_inference(batch_size)\n        elif all(should_auto_split_input(arr) for arr in input_data):\n            return auto_split_batch_inference()\n        else:\n            raise ValueError(\"Invalid input count\")\n    else:  # Single-input model\n        if input_count == 1:\n            return single_inference()\n        else:\n            return batch_inference(input_count)\n</code></pre>"},{"location":"docs/06_Inference_API.html#output>_format>_rules","title":"Output Format Rules","text":""},{"location":"docs/06_Inference_API.html#single>_inference>_output","title":"Single Inference Output","text":"API Output Format Structure C++ Run <code>TensorPtrs</code> <code>vector&lt;shared_ptr&lt;Tensor&gt;&gt;</code> Python run <code>List[np.ndarray]</code> <code>[output1, output2, ...]</code>"},{"location":"docs/06_Inference_API.html#batch>_inference>_output","title":"Batch Inference Output","text":"API Output Format Structure C++ Run (batch) <code>vector&lt;TensorPtrs&gt;</code> <code>[sample1_outputs, sample2_outputs, ...]</code> Python run (batch) <code>List[List[np.ndarray]]</code> <code>[[s1_o1, s1_o2], [s2_o1, s2_o2], ...]</code>"},{"location":"docs/06_Inference_API.html#asynchronous>_output","title":"Asynchronous Output","text":"API Immediate Return After <code>wait</code> C++ RunAsync <code>int</code> (jobId) <code>TensorPtrs</code> Python run_async <code>int</code> (jobId) <code>List[np.ndarray]</code>"},{"location":"docs/06_Inference_API.html#special>_cases","title":"Special Cases","text":""},{"location":"docs/06_Inference_API.html#auto-split>_condition","title":"Auto-Split Condition","text":"<p>C++:</p> <pre><code>bool shouldAutoSplitInput() const {\n    return _isMultiInput &amp;&amp; _inputTasks.size() == 1;\n}\n</code></pre> <p>Python:</p> <pre><code>def _should_auto_split_input(input_data: np.ndarray) -&gt; bool:\n    if not self.is_multi_input_model():\n        return False\n\n    expected_total_size = self.get_input_size()\n    actual_size = input_data.nbytes\n\n    return actual_size == expected_total_size\n</code></pre>"},{"location":"docs/06_Inference_API.html#batch>_size>_determination","title":"Batch Size Determination","text":"Condition Batch Size Calculation Single-input + List[np.ndarray] <code>len(input_data)</code> Multi-input + List[np.ndarray] <code>len(input_data) // input_tensor_count</code> List[List[np.ndarray]] <code>len(input_data)</code>"},{"location":"docs/06_Inference_API.html#error>_conditions","title":"Error Conditions","text":"Condition Error Type Message Multi-input + invalid size <code>ValueError</code> \"Invalid input count for multi-input model\" Async + batch <code>ValueError</code> \"Batch inference not supported in async\" Empty input <code>ValueError</code> \"Input data cannot be empty\" Type mismatch <code>TypeError</code> \"Expected np.ndarray or List[np.ndarray]\""},{"location":"docs/06_Inference_API.html#output>_buffer>_handling","title":"Output Buffer Handling","text":""},{"location":"docs/06_Inference_API.html#python>_output>_buffer>_matrix","title":"Python Output Buffer Matrix","text":"Input Format Output Buffer Format Handling Single Inference <code>None</code> Auto-allocated Single Inference <code>List[np.ndarray]</code> User-provided Single Inference <code>np.ndarray</code> (total_size) Used after auto-split Batch Inference <code>List[List[np.ndarray]]</code> Explicit batch buffer Batch Inference <code>List[np.ndarray]</code> Flattened batch buffer"},{"location":"docs/06_Inference_API.html#performance>_considerations","title":"Performance Considerations","text":""},{"location":"docs/06_Inference_API.html#memory>_allocation","title":"Memory Allocation","text":"Method Pros Cons Auto-allocation (No Buffer) Ease of use Memory allocated on every call User-provided (With Buffer) Performance optimization Complex memory management"},{"location":"docs/06_Inference_API.html#inference>_method","title":"Inference Method","text":"Method Use Case Characteristics Synchronous Simple processing Sequential execution Asynchronous High throughput Requires callback management Batch Bulk processing Increased memory usage"},{"location":"docs/07_Multi_Input_Inference.html","title":"Multi-Input Inference API","text":"<p>DXRT supports various inference methods for multi-input models, which have multiple input tensors. This section explains how to use the inference APIs for these models.</p>"},{"location":"docs/07_Multi_Input_Inference.html#identifying>_a>_multi-input>_model","title":"Identifying a Multi-Input Model","text":"<p>C++</p> <pre><code>dxrt::InferenceEngine ie(modelPath);\n\n// Check if the model is multi-input\nbool isMultiInput = ie.IsMultiInputModel();\n\n// Get the number of input tensors\nint inputCount = ie.GetInputTensorCount();\n\n// Get the input tensor names\nstd::vector&lt;std::string&gt; inputNames = ie.GetInputTensorNames();\n\n// Get the input tensor to task mapping\nstd::map&lt;std::string, std::string&gt; mapping = ie.GetInputTensorToTaskMapping();\n</code></pre> <p>Python</p> <pre><code>from dx_engine import InferenceEngine\n\nie = InferenceEngine(model_path)\n\n# Check if the model is multi-input\nis_multi_input = ie.is_multi_input_model()\n\n# Get the number of input tensors\ninput_count = ie.get_input_tensor_count()\n\n# Get the input tensor names\ninput_names = ie.get_input_tensor_names()\n\n# Get the input tensor to task mapping\nmapping = ie.get_input_tensor_to_task_mapping()\n</code></pre>"},{"location":"docs/07_Multi_Input_Inference.html#multi-input>_inference>_methods","title":"Multi-Input Inference Methods","text":""},{"location":"docs/07_Multi_Input_Inference.html#inference>_without>_output>_buffers>_auto-allocation","title":"Inference without Output Buffers (Auto-Allocation)","text":"<p>In this approach, you do not provide output buffers; the inference engine allocates the necessary memory automatically.</p>"},{"location":"docs/07_Multi_Input_Inference.html#dictionary>_format>_recommended","title":"Dictionary Format (Recommended)","text":"<p>This method involves providing input tensors mapped by their names. It is the most explicit and least error-prone method.</p> <p>C++</p> <pre><code>// Using Dictionary format (auto-allocation)\nstd::map&lt;std::string, void*&gt; inputTensors;\ninputTensors[\"input1\"] = input1_data;\ninputTensors[\"input2\"] = input2_data;\n\n// Synchronous inference without output buffers (auto-allocation)\nauto outputs = ie.RunMultiInput(inputTensors);\n</code></pre> <p>Python</p> <pre><code># Using Dictionary format (auto-allocation)\ninput_tensors = {\n    \"input1\": input1_array,\n    \"input2\": input2_array\n}\n\n# Synchronous inference without output buffers (auto-allocation)\noutputs = ie.run_multi_input(input_tensors)\n</code></pre>"},{"location":"docs/07_Multi_Input_Inference.html#vector>_format","title":"Vector Format","text":"<p>This method involves providing input tensors in a vector/list. The order must match the order returned by <code>GetInputTensorNames()</code>.</p> <p>C++</p> <pre><code>// Using Vector format (must match the order of GetInputTensorNames())\nstd::vector&lt;void*&gt; inputPtrs = {input1_data, input2_data};\n\n// Synchronous inference without output buffers (auto-allocation)\nauto outputs = ie.RunMultiInput(inputPtrs);\n</code></pre> <p>Python</p> <pre><code># Using Vector format (must match the order of get_input_tensor_names())\ninput_list = [input1_array, input2_array]\n\n# Synchronous inference without output buffers (auto-allocation)\noutputs = ie.run(input_list)\n</code></pre>"},{"location":"docs/07_Multi_Input_Inference.html#auto-split>_format","title":"Auto-Split Format","text":"<p>This method automatically splits a single concatenated buffer into multiple inputs. It is applied automatically when the total size of the provided buffer matches the model's total input size.</p> <p>C++</p> <pre><code>// A single buffer with all inputs concatenated\nstd::vector&lt;uint8_t&gt; concatenatedInput(ie.GetInputSize());\n// ... fill data ...\n\n// Processed via auto-split (output buffers auto-allocated)\nauto outputs = ie.Run(concatenatedInput.data());\n</code></pre> <p>Python</p> <pre><code># A single array with all inputs concatenated\nconcatenated_input = np.zeros(ie.get_input_size(), dtype=np.uint8)\n# ... fill data ...\n\n# Processed via auto-split (output buffers auto-allocated)\noutputs = ie.run(concatenated_input)\n</code></pre>"},{"location":"docs/07_Multi_Input_Inference.html#inference>_with>_user-provided>_output>_buffers","title":"Inference with User-Provided Output Buffers","text":"<p>In this approach, the user pre-allocates and provides the output buffers. This is advantageous for memory management and performance optimization.</p>"},{"location":"docs/07_Multi_Input_Inference.html#dictionary>_format","title":"Dictionary Format","text":"<p>C++</p> <pre><code>// Using Dictionary format\nstd::map&lt;std::string, void*&gt; inputTensors;\ninputTensors[\"input1\"] = input1_data;\ninputTensors[\"input2\"] = input2_data;\n\n// Create output buffer\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// Synchronous inference (with user-provided output buffer)\nauto outputs = ie.RunMultiInput(inputTensors, userArg, outputBuffer.data());\n</code></pre> <p>Python</p> <pre><code># Using Dictionary format\ninput_tensors = {\n    \"input1\": input1_array,\n    \"input2\": input2_array\n}\n\n# Create output buffers\noutput_buffers = [np.zeros(size, dtype=np.uint8) for size in ie.get_output_tensor_sizes()]\n\n# Synchronous inference (with user-provided output buffers)\noutputs = ie.run_multi_input(input_tensors, output_buffers=output_buffers)\n</code></pre>"},{"location":"docs/07_Multi_Input_Inference.html#vector>_format_1","title":"Vector Format","text":"<p>C++</p> <pre><code>// Using Vector format (must match the order of GetInputTensorNames())\nstd::vector&lt;void*&gt; inputPtrs = {input1_data, input2_data};\n\n// Create output buffer\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// Synchronous inference (with user-provided output buffer)\nauto outputs = ie.RunMultiInput(inputPtrs, userArg, outputBuffer.data());\n</code></pre> <p>Python</p> <pre><code># Using Vector format (must match the order of get_input_tensor_names())\ninput_list = [input1_array, input2_array]\n\n# Create output buffers\noutput_buffers = [np.zeros(size, dtype=np.uint8) for size in ie.get_output_tensor_sizes()]\n\n# Synchronous inference (with user-provided output buffers)\noutputs = ie.run(input_list, output_buffers=output_buffers)\n</code></pre>"},{"location":"docs/07_Multi_Input_Inference.html#auto-split>_format_1","title":"Auto-Split Format","text":"<p>C++</p> <pre><code>// A single buffer with all inputs concatenated\nstd::vector&lt;uint8_t&gt; concatenatedInput(ie.GetInputSize());\n// ... fill data ...\n\n// Create output buffer\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// Processed via auto-split (with user-provided output buffer)\nauto outputs = ie.Run(concatenatedInput.data(), userArg, outputBuffer.data());\n</code></pre> <p>Python</p> <pre><code># A single array with all inputs concatenated\nconcatenated_input = np.zeros(ie.get_input_size(), dtype=np.uint8)\n# ... fill data ...\n\n# Create output buffers\noutput_buffers = [np.zeros(size, dtype=np.uint8) for size in ie.get_output_tensor_sizes()]\n\n# Processed via auto-split (with user-provided output buffers)\noutputs = ie.run(concatenated_input, output_buffers=output_buffers)\n</code></pre>"},{"location":"docs/07_Multi_Input_Inference.html#multi-input>_batch>_inference","title":"Multi-Input Batch Inference","text":""},{"location":"docs/07_Multi_Input_Inference.html#explicit>_batch>_format","title":"Explicit Batch Format","text":"<p>This method involves explicitly providing input tensors for each item in the batch.</p> <p>C++</p> <pre><code>// Batch input buffers (concatenated format)\nstd::vector&lt;void*&gt; batchInputs = {sample1_ptr, sample2_ptr, sample3_ptr};\nstd::vector&lt;void*&gt; batchOutputs = {output1_ptr, output2_ptr, output3_ptr};\nstd::vector&lt;void*&gt; userArgs = {userArg1, userArg2, userArg3};\n\n// Batch inference\nauto results = ie.Run(batchInputs, batchOutputs, userArgs);\n</code></pre> <p>Python</p> <pre><code># Format: List[List[np.ndarray]]\nbatch_inputs = [\n    [sample1_input1, sample1_input2],  # First sample\n    [sample2_input1, sample2_input2],  # Second sample\n    [sample3_input1, sample3_input2]   # Third sample\n]\n\nbatch_outputs = [\n    [sample1_output1, sample1_output2],  # Output buffers for the first sample\n    [sample2_output1, sample2_output2],  # Output buffers for the second sample\n    [sample3_output1, sample3_output2]   # Output buffers for the third sample\n]\n\n# Batch inference\nresults = ie.run(batch_inputs, output_buffers=batch_outputs)\n</code></pre>"},{"location":"docs/07_Multi_Input_Inference.html#flattened>_batch>_format","title":"Flattened Batch Format","text":"<p>This method involves providing all inputs in a flattened format.</p> <p>Python</p> <pre><code># Flattened format: [sample1_input1, sample1_input2, sample2_input1, sample2_input2, ...]\nflattened_inputs = [\n    sample1_input1, sample1_input2,  # First sample\n    sample2_input1, sample2_input2,  # Second sample\n    sample_input1, sample3_input2   # Third sample\n]\n\n# Automatically recognized as a batch (input count is a multiple of the model's input count)\nresults = ie.run(flattened_inputs, output_buffers=batch_outputs)\n</code></pre>"},{"location":"docs/07_Multi_Input_Inference.html#asynchronous>_inference","title":"Asynchronous Inference","text":""},{"location":"docs/07_Multi_Input_Inference.html#callback-based>_asynchronous>_inference","title":"Callback-Based Asynchronous Inference","text":"<p>C++</p> <pre><code>// Register callback function\nie.RegisterCallback([](dxrt::TensorPtrs&amp; outputs, void* userArg) -&gt; int {\n    // Process outputs\n    return 0;\n});\n\n// Dictionary format asynchronous inference\nint jobId = ie.RunAsyncMultiInput(inputTensors, userArg);\n\n// Vector format asynchronous inference\nint jobId = ie.RunAsyncMultiInput(inputPtrs, userArg);\n</code></pre> <p>Python</p> <pre><code># Define callback function\ndef callback_handler(outputs, user_arg):\n    # Process and validate outputs\n    return 0\n\n# Register callback\nie.register_callback(callback_handler)\n\n# Dictionary format asynchronous inference\njob_id = ie.run_async_multi_input(input_tensors, user_arg=user_arg)\n\n# Vector format asynchronous inference\njob_id = ie.run_async(input_list, user_arg=user_arg)\n</code></pre>"},{"location":"docs/07_Multi_Input_Inference.html#simple>_asynchronous>_inference","title":"Simple Asynchronous Inference","text":"<p>C++</p> <pre><code>// Single buffer asynchronous inference\nint jobId = ie.RunAsync(inputPtr, userArg);\n\n// Wait for the result\nauto outputs = ie.Wait(jobId);\n</code></pre> <p>Python</p> <pre><code># Single buffer asynchronous inference\njob_id = ie.run_async(input_buffer, user_arg=user_arg)\n\n# Wait for the result\noutputs = ie.wait(job_id)\n</code></pre>"},{"location":"docs/08_Global_Instance.html","title":"Global Instance","text":""},{"location":"docs/08_Global_Instance.html#configuration","title":"Configuration","text":"<p>The <code>Configuration</code> class is a central component for managing global application settings for the DXRT library. It provides a consistent and thread-safe point of access for querying and modifying configuration parameters.</p> <p>This guide covers usage for both C++ and Python. The class is designed as a singleton, meaning only one instance of the configuration manager exists. The Python class acts as a wrapper around the core C++ singleton, so all instances in C++ and Python share the same state.</p> <p>Key Features:</p> <ul> <li>Singleton Pattern: Guarantees a single, globally accessible configuration instance.</li> <li>Dynamic Configuration: Allows enabling/disabling features and setting attributes at runtime.</li> <li>Version Reporting: Provides methods to retrieve library and driver versions.</li> <li>Language Support: Available in both C++ and Python.</li> </ul>"},{"location":"docs/08_Global_Instance.html#getting>_an>_instance","title":"Getting an Instance","text":"<p>How you get the configuration object differs slightly between C++ and Python.</p> <p>C++</p> <p>In C++, you must access the single instance through the static <code>GetInstance()</code> method. The constructor is private to enforce the singleton pattern.</p> <pre><code>#include \"dxrt/common.h\"\n\n// Correct: Get the single, global instance\ndxrt::Configuration&amp; config = dxrt::Configuration::GetInstance();\n\n// Incorrect: The following line will cause a compile error\n// dxrt::Configuration myConfig; // Error: constructor is private\n</code></pre> <p>Python</p> <p>In Python, you create an instance using the standard constructor. Internally, this constructor retrieves the single, underlying C++ instance. All Python <code>Configuration</code> objects will therefore refer to the same settings.</p> <pre><code>from dx_engine.configuration import Configuration\n\n# Create a Configuration object.\n# This holds a reference to the global settings instance.\nconfig = Configuration()\n</code></pre>"},{"location":"docs/08_Global_Instance.html#configuration>_scopes>_item>_and>_attribute","title":"Configuration Scopes: ITEM and ATTRIBUTE","text":"<p>Configuration is organized around two enumerations, <code>ITEM</code> and <code>ATTRIBUTE</code>, which are used in both C++ and Python.</p>"},{"location":"docs/08_Global_Instance.html#item","title":"ITEM","text":"<p>An <code>ITEM</code> represents a major feature or module that can be enabled or disabled.</p> Item Description <code>DEBUG</code> Enables general debug mode. <code>PROFILER</code> Enables profiler functionality. <code>SERVICE</code> Configures service-related operations. <code>DYNAMIC_CPU_THREAD</code> Manages dynamic CPU thread settings. <code>TASK_FLOW</code> Controls task flow management features. <code>SHOW_THROTTLING</code> Enables the display of throttling information. <code>SHOW_PROFILE</code> Enables the display of profile results. <code>SHOW_MODEL_INFO</code> Enables the display of detailed model information."},{"location":"docs/08_Global_Instance.html#attribute","title":"ATTRIBUTE","text":"<p>An <code>ATTRIBUTE</code> represents a specific property of an <code>ITEM</code>, usually set with a string value like a file path.</p> Attribute Associated <code>ITEM</code> Description <code>PROFILER_SHOW_DATA</code> <code>PROFILER</code> Attribute for showing profiler data. <code>PROFILER_SAVE_DATA</code> <code>PROFILER</code> Attribute for saving profiler data to a file."},{"location":"docs/08_Global_Instance.html#key>_operations>_and>_usage","title":"Key Operations and Usage","text":"<p>This section details the main operations with examples for both languages.</p>"},{"location":"docs/08_Global_Instance.html#enabling>_and>_disabling>_features","title":"Enabling and Disabling Features","text":"<p>Use these methods to turn features on or off and check their current status.</p> <p>C++</p> <pre><code>// Enable the profiler\nconfig.SetEnable(dxrt::Configuration::ITEM::PROFILER, true);\n\n// Check if the profiler is enabled\nif (config.GetEnable(dxrt::Configuration::ITEM::PROFILER)) {\n    std::cout &lt;&lt; \"Profiler is enabled.\" &lt;&lt; std::endl;\n}\n</code></pre> <p>Python</p> <pre><code># Enable showing model information\nconfig.set_enable(Configuration.ITEM.SHOW_MODEL_INFO, True)\n\n# Check if showing model info is enabled\nis_enabled = config.get_enable(Configuration.ITEM.SHOW_MODEL_INFO)\nprint(f\"SHOW_MODEL_INFO is enabled: {is_enabled}\")\n</code></pre>"},{"location":"docs/08_Global_Instance.html#working>_with>_attributes","title":"Working with Attributes","text":"<p>For more fine-grained control, use attributes to set and get string-based values.</p> <p>C++</p> <pre><code>// First, ensure the parent item is enabled\nconfig.SetEnable(dxrt::Configuration::ITEM::PROFILER, true);\n\n// Set the path where profiler data should be saved\nstd::string profile_path = \"/var/log/my_app_profile.json\";\nconfig.SetAttribute(dxrt::Configuration::ITEM::PROFILER,\n                      dxrt::Configuration::ATTRIBUTE::PROFILER_SAVE_DATA,\n                      profile_path);\n\n// Retrieve the attribute value later\nstd::string saved_path = config.GetAttribute(dxrt::Configuration::ITEM::PROFILER,\n                                              dxrt::Configuration::ATTRIBUTE::PROFILER_SAVE_DATA);\n</code></pre> <p>Python</p> <pre><code># First, ensure the parent item is enabled\nconfig.set_enable(Configuration.ITEM.PROFILER, True)\n\n# Set the path for saving profiler data\nprofile_log_path = \"/var/log/dx_profile.json\"\nconfig.set_attribute(Configuration.ITEM.PROFILER,\n                     Configuration.ATTRIBUTE.PROFILER_SAVE_DATA,\n                     profile_log_path)\n\n# Retrieve the path later\nsaved_path = config.get_attribute(Configuration.ITEM.PROFILER,\n                                  Configuration.ATTRIBUTE.PROFILER_SAVE_DATA)\nprint(f\"Profiler data will be saved to: {saved_path}\")\n</code></pre>"},{"location":"docs/08_Global_Instance.html#retrieving>_version>_information","title":"Retrieving Version Information","text":"<p>These methods are critical for debugging, logging, and ensuring system compatibility.</p> <p>C++</p> <pre><code>#include &lt;vector&gt;\n#include &lt;utility&gt;\n#include &lt;string&gt;\n\ntry {\n    std::cout &lt;&lt; \"DXRT Library Version: \" &lt;&lt; config.GetVersion() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Driver Version: \" &lt;&lt; config.GetDriverVersion() &lt;&lt; std::endl;\n\n    // Get firmware versions for all detected devices\n    std::vector&lt;std::pair&lt;int, std::string&gt;&gt; fw_versions = config.GetFirmwareVersions();\n    for (const auto&amp; fw : fw_versions) {\n        std::cout &lt;&lt; \"Device \" &lt;&lt; fw.first &lt;&lt; \" Firmware Version: \" &lt;&lt; fw.second &lt;&lt; std::endl;\n    }\n} catch (const std::runtime_error&amp; e) {\n    std::cerr &lt;&lt; \"Error retrieving version information: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre> <p>Python</p> <pre><code>print(f\"Library Version: {config.get_version()}\")\nprint(f\"Driver Version: {config.get_driver_version()}\")\nprint(f\"PCIe Driver Version: {config.get_pcie_driver_version()}\")\n</code></pre>"},{"location":"docs/08_Global_Instance.html#devicestatus","title":"DeviceStatus","text":"<p>The <code>DeviceStatus</code> class is designed to provide a snapshot of a device's state. When you obtain a <code>DeviceStatus</code> object, it captures the device's properties (like model and memory) and real-time metrics (like temperature and clock speed) at that specific moment.</p> <p>The general workflow is:</p> <ul> <li>Use a static/class method to find the number of available devices.</li> <li>Use another static/class method to get a status object for a specific device ID.</li> <li>Use instance methods on that object to retrieve the data you need.</li> </ul>"},{"location":"docs/08_Global_Instance.html#getting>_started>_accessing>_devices","title":"Getting Started: Accessing Devices","text":"<p>The first step is always to find out how many devices are available and then create a status object for the one you want to inspect.</p>"},{"location":"docs/08_Global_Instance.html#step>_1>_get>_the>_device>_count","title":"Step 1: Get the Device Count","text":"<p>Use the static methods below to determine how many NPU devices are recognized by the system.</p> <p>C++</p> <pre><code>#include \"dxrt/dxrt_api.h\" // Main C++ header\n\nint deviceCount = dxrt::DeviceStatus::GetDeviceCount();\nstd::cout &lt;&lt; \"Found \" &lt;&lt; deviceCount &lt;&lt; \" devices.\" &lt;&lt; std::endl;\n</code></pre> <p>Python</p> <pre><code>from dx_engine.dev_status import DeviceStatus # Main Python class\n\ndevice_count = DeviceStatus.get_device_count()\nprint(f\"Found {device_count} devices.\")\n</code></pre>"},{"location":"docs/08_Global_Instance.html#step>_2>_get>_the>_status>_object","title":"Step 2: Get the Status Object","text":"<p>Once you have the count, you can get a status object for any valid device ID (from <code>0</code> to <code>device_count - 1</code>).</p> <p>C++ It's crucial to use a <code>try...catch</code> block, as requesting an invalid ID will throw an exception.</p> <pre><code>try {\n    // Get a status snapshot for device with ID 0\n    dxrt::DeviceStatus status = dxrt::DeviceStatus::GetCurrentStatus(0);\n    std::cout &lt;&lt; \"Successfully created status object for device \" &lt;&lt; status.GetId() &lt;&lt; std::endl;\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre> <p>Python The factory method <code>get_current_status()</code> returns a <code>DeviceStatus</code> object.</p> <pre><code>if device_count &gt; 0:\n    # Get the status object for the first device (ID 0)\n    status_obj = DeviceStatus.get_current_status(0)\n    print(f\"Successfully created status object for device ID: {status_obj.get_id()}\")\n</code></pre>"},{"location":"docs/08_Global_Instance.html#querying>_device>_information","title":"Querying Device Information","text":"<p>With a <code>DeviceStatus</code> object, you can access a wealth of information.</p>"},{"location":"docs/08_Global_Instance.html#formatted>_summary>_strings>_c>_only","title":"Formatted Summary Strings (C++ Only)","text":"<p>For quick logging or command-line display, the C++ class offers powerful helper methods that return a pre-formatted, human-readable string summary. These are equivalent to the <code>dxrt-cli</code> tool's output.</p> <ul> <li><code>GetInfoString()</code>: Returns static hardware info (model, memory, board, firmware).</li> <li><code>GetStatusString()</code>: Returns dynamic real-time status (NPU voltage, clock, temp, DVFS state).</li> </ul> <pre><code>// Print static hardware information\nstd::cout &lt;&lt; \"--- Device Info ---\\n\" &lt;&lt; status.GetInfoString() &lt;&lt; std::endl;\n\n// Print dynamic, real-time status\nstd::cout &lt;&lt; \"--- Real-time Status ---\\n\" &lt;&lt; status.GetStatusString() &lt;&lt; std::endl;\n</code></pre>"},{"location":"docs/08_Global_Instance.html#accessing>_specific>_attributes>_c>_and>_python","title":"Accessing Specific Attributes (C++ and Python)","text":"<p>For programmatic access, use the instance methods to get individual data points.</p> Metric C++ Method Python Method Return Value Device ID <code>GetId()</code> <code>get_id()</code> <code>int</code> Temperature <code>GetTemperature(ch)</code> <code>get_temperature(ch)</code> <code>int</code> (Celsius) NPU Voltage <code>GetNpuVoltage(ch)</code> <code>get_npu_voltage(ch)</code> <code>uint32_t</code> / <code>int</code> (mV) NPU Clock <code>GetNpuClock(ch)</code> <code>get_npu_clock(ch)</code> <code>uint32_t</code> / <code>int</code> (MHz) <p>Note: The C++ API provides a richer set of methods for querying static hardware details like memory, board type, and device variants.</p>"},{"location":"docs/08_Global_Instance.html#complete>_usage>_examples","title":"Complete Usage Examples \ud83d\udccb","text":"<p>Here is a complete example for each language, showing how to iterate through all devices and print their status.</p>"},{"location":"docs/08_Global_Instance.html#c>_example","title":"C++ Example","text":"<p>This example uses the formatted string helpers for a concise report.</p> <pre><code>#include &lt;iostream&gt;\n#include \"dxrt/dxrt_api.h\" // DXRT API header file\n\n/**\n * @brief Prints the detailed status for each NPU core of a specific device.\n * @param device_id The ID of the device to query.\n */\nvoid print_detailed_device_status(int device_id) {\n    try {\n        // Get a snapshot of the current status for the specified device.\n        dxrt::DeviceStatus status = dxrt::DeviceStatus::GetCurrentStatus(device_id);\n\n        std::cout &lt;&lt; \"--- Device ID: \" &lt;&lt; device_id &lt;&lt; \" ---\" &lt;&lt; std::endl;\n\n        // Assuming 2 NPU cores per device, like in the Python example.\n        // In a real application, it's better to get the core count dynamically from the API.\n        for (int core_ch = 0; core_ch &lt; 2; ++core_ch) {\n            // Individually query the temperature, voltage, and clock speed for each core.\n            int temp = status.GetTemperature(core_ch);\n            uint32_t voltage = status.GetNpuVoltage(core_ch);\n            uint32_t clock = status.GetNpuClock(core_ch);\n\n            // Print in the same format as the Python example.\n            std::cout &lt;&lt; \"  Core \" &lt;&lt; core_ch\n                      &lt;&lt; \": Temp=\" &lt;&lt; temp &lt;&lt; \"'C\"\n                      &lt;&lt; \", Voltage=\" &lt;&lt; voltage &lt;&lt; \"mV\"\n                      &lt;&lt; \", Clock=\" &lt;&lt; clock &lt;&lt; \"MHz\" &lt;&lt; std::endl;\n        }\n        std::cout &lt;&lt; std::endl; // Add a newline for readability\n\n    } catch (const dxrt::Exception&amp; e) {\n        std::cerr &lt;&lt; \"Error getting report for device \" &lt;&lt; device_id &lt;&lt; \": \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    }\n}\n\nint main() {\n    int deviceCount = dxrt::DeviceStatus::GetDeviceCount();\n    if (deviceCount == 0) {\n        std::cout &lt;&lt; \"No DEEPX devices found.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    std::cout &lt;&lt; \"Querying status for \" &lt;&lt; deviceCount &lt;&lt; \" device(s)...\\n\" &lt;&lt; std::endl;\n\n    // Iterate through all devices and print their detailed status.\n    for (int i = 0; i &lt; deviceCount; ++i) {\n        print_detailed_device_status(i);\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"docs/08_Global_Instance.html#python>_example","title":"Python Example","text":"<p>This example iterates through each device and NPU core to print specific metrics.</p> <pre><code>from dx_engine.dev_status import DeviceStatus\n\ndef main():\n    \"\"\"Checks for all available devices and prints their real-time status.\"\"\"\n    try:\n        device_count = DeviceStatus.get_device_count()\n        if device_count == 0:\n            print(\"No devices found.\")\n            return\n\n        print(f\"Querying status for {device_count} device(s)...\\n\")\n        # Iterate through each device by its ID\n        for i in range(device_count):\n            print(f\"--- Device ID: {i} ---\")\n            status = DeviceStatus.get_current_status(i)\n\n            # Assuming 2 NPU cores per device for this example\n            for core_ch in range(2):\n                temp = status.get_temperature(core_ch)\n                voltage = status.get_npu_voltage(core_ch)\n                clock = status.get_npu_clock(core_ch)\n                print(f\"  Core {core_ch}: Temp={temp}\u00b0C, Voltage={voltage}mV, Clock={clock}MHz\")\n            print(\"\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html","title":"C++ Tutorials","text":""},{"location":"docs/09_01_C%2B%2B_Tutorials.html#c>_tutorials","title":"C++ Tutorials","text":""},{"location":"docs/09_01_C%2B%2B_Tutorials.html#run>_synchronous","title":"Run (Synchronous)","text":"<p>The synchronous Run method uses a single NPU core to perform inference in a blocking manner. It can be configured to utilize multiple NPU cores simultaneously by employing threads to run each core independently.</p> <p>  Figure. Synchronous Inference Operation    </p> <p>Inference Engine Run synchronous  </p> <ul> <li>Inference synchronously  </li> <li>Use only one npu core  </li> </ul> <p>The following is the simplest example of synchronous inference.  </p> <p><code>run_sync_model.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\nint main()\n{\n    std::string modelPath = \"model-path\";\n\n    try \n    {\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        // inference loop\n        for(int i = 0; i &lt; 100; ++i)\n        {\n            // inference synchronously\n            // use only one npu core\n            auto outputs = ie.Run(inputPtr.data());\n\n            // post processing\n            postProcessing(outputs);\n\n        } // for i\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine  \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#runasync>_asynchronous","title":"RunAsync (Asynchronous)","text":"<p>The asynchronous Run mode is a method that performs inference asynchronously while utilizing multiple NPU cores simultaneously. It can be implemented to maximize NPU resources through a callback function or a thread wait mechanism.</p> <p>  Figure. Asynchronous Inference Operation    </p> <p>Inference Engine RunAsync, Callback, User Argument  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>The following is an example of asynchronous inference using a callback function. A user argument can be used to synchronize the input with the output of the callback.  </p> <p><code>run_async_model.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\nint main(int argc, char* argv[])\n{\n    ...\n\n    int callback_count = 0;\n\n    try \n    {\n\n        std::mutex cv_mutex;\n        std::condition_variable cv;\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(model_path);\n\n        // register call back function\n        ie.RegisterCallback([&amp;callback_count, &amp;loop_count, &amp;cv_mutex, &amp;cv] \n            (dxrt::TensorPtrs &amp;outputs, void *userArg) {\n\n            std::ignore = outputs;\n            std::ignore = userArg;\n\n            std::unique_lock&lt;std::mutex&gt; lock(cv_mutex);\n            callback_count++;\n            if ( callback_count == loop_count ) cv.notify_one();\n\n            return 0;\n        });\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        auto start = std::chrono::high_resolution_clock::now();\n\n        // inference loop\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            // user argument\n            std::pair&lt;int, int&gt; *userData = new std::pair&lt;int, int&gt;(i, loop_count);\n\n            // inference asynchronously, use all npu cores\n            ie.RunAsync(inputPtr.data(), userData);\n\n            log.Debug(\"Inference request submitted with user_arg(\" + std::to_string(i) + \")\");\n        }\n\n        // wait until all callbacks have been processed\n        std::unique_lock&lt;std::mutex&gt; lock(cv_mutex);\n        cv.wait(lock, [&amp;callback_count, &amp;loop_count] { \n            return callback_count == loop_count;\n        });\n\n        ...\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        ...\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        ...\n        return -1;\n    }\n    catch(...)\n    {\n        ...\n        return -1;\n    }\n\n    return (callback_count == loop_count ? 0 : -1);\n}\n</code></pre></p> <p>The following is an example where multiple threads start input and inference, and a single callback processes the output.  </p> <p>Inference Engine RunAsync, Callback, User Argument, Thread  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p><code>run_async_model_thread.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\nstatic const int THREAD_COUNT = 3;\nstatic std::atomic&lt;int&gt; gResultCount = {0};\nstatic std::atomic&lt;int&gt; gTotalCount = {0};\nstatic ConcurrentQueue&lt;int&gt; gResultQueue(1);\nstatic std::mutex gCBMutex;\n\nstatic int inferenceThreadFunc(dxrt::InferenceEngine&amp; ie, std::vector&lt;uint8_t&gt;&amp; inputPtr, int threadIndex, int loopCount)\n{\n\n    // inference loop\n    for(int i = 0; i &lt; loopCount; ++i) \n    {\n        // user argument\n        UserData *userData = new UserData();\n\n        // thread index \n        userData-&gt;setThreadIndex(threadIndex);\n\n        // total loop count\n        userData-&gt;setLoopCount(loopCount);\n\n        // loop index\n        userData-&gt;setLoopIndex(i);\n\n        try\n        {\n            // inference asynchronously, use all npu cores\n            // if device-load &gt;= max-load-value, this function will block  \n            ie.RunAsync(inputPtr.data(), userData);\n        }\n        catch(const dxrt::Exception&amp; e)\n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n        catch(const std::exception&amp; e)\n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n\n    } // for i\n\n    return 0;\n\n}\n\n// invoke this function asynchronously after the inference is completed\nstatic int onInferenceCallbackFunc(dxrt::TensorPtrs &amp;outputs, void *userArg)\n{\n\n    // the outputs are guaranteed to be valid only within this callback function\n    // processing this callback functions as quickly as possible is beneficial \n    // for improving inference performance\n\n    // user data type casting\n    UserData *user_data = reinterpret_cast&lt;UserData*&gt;(userArg);\n\n    // thread index\n    int thread_index = user_data-&gt;getThreadIndex();\n\n    // loop index\n    int loop_index = user_data-&gt;getLoopIndex();\n\n    // post processing\n    // transfer outputs to the target thread by thread_index\n    // postProcessing(outputs, thread_index);\n    (void)outputs;\n\n\n    // result count \n    {\n        // Mutex locks should be properly adjusted \n        // to ensure that callback functions are thread-safe.\n        std::lock_guard&lt;std::mutex&gt; lock(gCBMutex);\n\n        gResultCount++;\n        if ( gResultCount.load() == gTotalCount.load() ) gResultQueue.push(0);\n    }\n\n    // delete argument object \n    delete user_data;\n\n    return 0;\n}\n\n\nint main(int argc, char* argv[])\n{\n    ...\n\n    bool result = false;\n\n    try\n    {\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // register call back function\n        ie.RegisterCallback(onInferenceCallbackFunc);    \n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        gTotalCount.store(loop_count * THREAD_COUNT);\n\n        // thread vector \n        std::vector&lt;std::thread&gt; thread_array;\n\n        for(int i = 0; i &lt; THREAD_COUNT; ++i)\n        {\n            // create thread\n            thread_array.push_back(std::thread(inferenceThreadFunc, std::ref(ie), std::ref(inputPtr), i, loop_count));\n        }\n\n        for(auto &amp;t : thread_array)\n        {\n            t.join();\n        } // for t\n\n\n        // wait until all callbacks have been processed\n        gResultQueue.pop();\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return result ? 0 : -1;\n}\n</code></pre></p> <p>The following is an example of performing asynchronous inference by creating an inference wait thread. The main thread starts input and inference, and the inference wait thread retrieves the output data corresponding to the input.  </p> <p>Inference Engine RunAsync, Wait  </p> <ul> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p><code>run_async_model_wait.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\n// concurrent queue is a thread-safe queue data structure \n// designed to be used in a multi-threaded environment\nstatic ConcurrentQueue&lt;int&gt; gJobIdQueue;\n\n// user thread to wait for the completion of inference \nstatic int inferenceThreadFunc(dxrt::InferenceEngine&amp; ie, int loopCount)\n{\n    int count = 0;\n\n    while(...)\n    {\n        // pop item from queue \n        int jobId = gJobIdQueue.pop();\n\n        try \n        {\n            // waiting for the inference to complete by jobId \n            auto outputs = ie.Wait(jobId);\n\n            // post processing \n            postProcessing(outputs);\n\n        }\n        catch(const dxrt::Exception&amp; e)  // exception for inference engine \n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n        catch(const std::exception&amp; e)\n        {\n            std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n            std::exit(-1);\n        }\n\n        // something to do\n\n        count++;\n        if ( count &gt;= loopCount ) break;\n\n    } // while\n\n    return 0;\n}\n\nint main()\n{\n    const int LOOP_COUNT = 100;\n    std::string modelPath = \"model-path\";\n\n    try\n    {\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // do not register call back function\n        // inferenceEngine.RegisterCallback(onInferenceCallbackFunc);\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        // create thread\n        auto t1 = std::thread(inferenceThreadFunc, std::ref(ie), LOOP_COUNT);\n\n        // inference loop\n        for(int i = 0; i &lt; LOOP_COUNT; ++i)\n        {\n\n            // no need user argument\n            // UserData *userData = getUserDataInstanceFromDataPool();\n\n            // inference asynchronously, use all npu cores\n            // if device-load &gt;= max-load-value, this function will block\n            auto jobId = ie.RunAsync(inputPtr.data());\n\n            // push jobId in global queue variable\n            gJobIdQueue.push(jobId);\n\n        } // for i\n\n        t1.join();\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#run>_batch","title":"Run (Batch)","text":"<p>The following is an example of batch inference with multiple inputs and multiple outputs.</p> <p><code>run_batch_model.cpp</code></p> <pre><code>int main(int argc, char* argv[])\n{\n    ...\n\n    try\n    {\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath);\n\n        // create temporary input buffer for example\n        std::vector&lt;uint8_t&gt; inputBuffer(ie.GetInputSize(), 0);\n\n        // input buffer vector\n        std::vector&lt;void*&gt; inputBuffers;\n        for(int i = 0; i &lt; batch_count; ++i)\n        {\n            // assigns the same buffer pointer in this example\n            inputBuffers.emplace_back(inputBuffer.data());\n        }\n\n        // output buffer vector\n        std::vector&lt;void*&gt; output_buffers(batch_count, 0);\n\n        // create user output buffers\n        for(auto&amp; ptr : output_buffers)\n        {\n            ptr = new uint8_t[ie.GetOutputSize()];\n        } // for i\n\n        // batch inference loop\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            // inference asynchronously, use all npu core\n            auto outputPtrs = ie.Run(inputBuffers, output_buffers);\n\n            // postProcessing(outputs);\n            (void)outputPtrs;\n        }\n\n        // Deallocated the user's output buffers\n        for(auto&amp; ptr : output_buffers)\n        {\n            delete[] static_cast&lt;uint8_t*&gt;(ptr);\n        } // for i\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#run>_runasync","title":"Run &amp; RunAsync","text":"<p>The method for converting a synchronous inference approach using one NPU core into an asynchronous inference approach using multiple NPU cores is as follows. It requires the use of callbacks or threads, as well as the implementation of multiple input buffers to support concurrent operations effectively.</p> <p>Converting Run(Sync) to RunAsync </p> <ul> <li>Shift from Single NPU Core to Multiple Cores     : Modify the existing Run(Sync) structure, which utilizes a single NPU core, to RunAsync structure capable of leveraging multiple NPU cores simultaneously.  </li> <li>Create Multiple Input/Output Buffers     : Implement multiple input/output buffers to prevent overwriting. Ensure an appropriate number of buffers are created to support concurrent operations effectively.  </li> <li>Introduce Multi-Buffer Concept     : To handle simultaneous inference processes, integrate a multi-buffer mechanism. This is essential for managing concurrent inputs and outputs without data conflicts.  </li> <li>Asynchronous Inference with Threads or Callbacks     : Adjust the code to ensure that inference inputs and outputs operate asynchronously using threads or callbacks for efficient processing.  </li> <li>Thread-Safe Data Exchange     : For data exchange between threads or callbacks, use a thread-safe queue or structured data mechanisms to avoid race conditions and ensure integrity.  </li> </ul> <p>  Figure. Converting Run(Sync) to RunAsync    </p>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#inference>_option","title":"Inference Option","text":"<p>The following inference options allow you to specify an NPU core for performing inference.</p> <p>Inference Engine Run, Inference Option  </p> <ul> <li>Select devices     : default devices is <code>{}</code>     : Choose devices to utilize  </li> <li>Select bound option per device     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_ALL</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_0</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_1</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_2</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_01</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_12</code>     : <code>dxrt::InferenceOption::BOUND_OPTION::NPU_02</code> </li> <li>Use onnx runtime library (<code>ORT</code>)     : <code>useORT</code> on or off  </li> </ul> <p><code>run_sync_model_bound.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\nint main()\n{\n    std::string modelPath = \"model-path\";\n\n    try\n    {\n\n        // select bound option NPU_0 to NPU_2 per device  \n        dxrt::InferenceOption op;\n\n        // first device only, default null\n        op.devices.push_back(0); // use device 0 \n        op.devices.push_back(3); // use device 3 \n\n        // use BOUND_OPTION::NPU_0 only\n        op.boundOption = dxrt::InferenceOption::BOUND_OPTION::NPU_0; \n\n        // use ORT\n        op.useORT = false;\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ie(modelPath, op);\n\n        // create temporary input buffer for example \n        std::vector&lt;uint8_t&gt; inputPtr(ie.GetInputSize(), 0);\n\n        // inference loop\n        for(int i = 0; i &lt; 100; ++i)\n        {\n            // input\n            uint8_t* inputPtr = readInputData();\n\n            // inference synchronously with boundOption\n            // use only one npu core\n            // ownership of the outputs is transferred to the user \n            auto outputs = ie.Run(inputPtr.data());\n\n            // post processing\n            postProcessing(outputs);\n\n        } // for i\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#configuration>_and>_device>_status","title":"Configuration and Device Status","text":"<p>This guide explains how to use the <code>Configuration</code> class to set up the inference engine and the <code>DeviceStatus</code> class to monitor hardware status in C++.</p>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#engine>_configuration","title":"Engine Configuration \u2699\ufe0f","text":"<p>The <code>Configuration</code> class, implemented as a Singleton, allows you to set global parameters for the inference engine before it runs.</p> <pre><code>// Get the singleton instance and set engine parameters\ndxrt::Configuration::GetInstance().SetEnable(dxrt::Configuration::ITEM::SHOW_MODEL_INFO, true);\ndxrt::Configuration::GetInstance().SetEnable(dxrt::Configuration::ITEM::SHOW_PROFILE, true);\n</code></pre> <ul> <li><code>Configuration::GetInstance()</code>: Accesses the single, global instance of the configuration manager.</li> <li><code>.SetEnable(...)</code>: Enables engine features. Here, it's configured to print detailed model information and performance profiling data when the <code>InferenceEngine</code> is initialized.</li> </ul>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#querying>_device>_status","title":"Querying Device Status \ud83d\udda5\ufe0f","text":"<p>The <code>DeviceStatus</code> class is used to get real-time operational information from the NPU hardware. This is often done after a workload to check the device's state.</p> <pre><code>// Get the number of available devices\nauto device_count = dxrt::DeviceStatus::GetDeviceCount();\n\n// Loop through each device\nfor(int i = 0; i &lt; device_count; ++i)\n{\n    // Get a status snapshot for the current device\n    auto device_status = dxrt::DeviceStatus::GetCurrentStatus(i);\n\n    // Query and print specific metrics like temperature, voltage, and clock speed\n    log.Info(\"Device: \" + std::to_string(device_status.GetId()));\n    log.Info(\"   Temperature: \" + std::to_string(device_status.GetTemperature(0)));\n    log.Info(\"   Voltage: \" + std::to_string(device_status.GetNpuVoltage(0)));\n    log.Info(\"   Clock: \" + std::to_string(device_status.GetNpuClock(0)));\n}\n</code></pre> <ul> <li><code>DeviceStatus::GetDeviceCount()</code>: A static method that returns the number of connected DEEPX devices.</li> <li><code>DeviceStatus::GetCurrentStatus(i)</code>: Returns a status object containing a snapshot of the hardware metrics for device <code>i</code> at that specific moment.</li> <li><code>device_status.Get...()</code>: Instance methods used to retrieve individual metrics from the status object, such as <code>GetTemperature()</code>, <code>GetNpuVoltage()</code>, and <code>GetNpuClock()</code> for a specific NPU core (e.g., core 0).</li> </ul>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#profiler>_configuration","title":"Profiler Configuration","text":"<p>This guide provides a simple, code-focused manual on how to configure the profiler using the DXRT SDK. The profiler is a powerful tool for analyzing the performance of each layer within your model.</p> <p>Configuration is managed through the <code>dxrt::Configuration</code> singleton instance.</p>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#enabling>_the>_profiler","title":"Enabling the Profiler","text":"<p>Before you can use any profiler features, you must first enable it. This is the essential first step for any profiling activity.</p> <pre><code>// Enable the profiler feature\ndxrt::Configuration::GetInstance().SetEnable(dxrt::Configuration::ITEM::PROFILER, true);\n</code></pre> <ul> <li><code>SetEnable</code>: This function activates or deactivates a specific DXRT feature.</li> <li><code>dxrt::Configuration::ITEM::PROFILER</code>: Specifies that the target feature is the profiler.</li> <li><code>true</code>: Enables the profiler. Set to <code>false</code> to disable it.</li> </ul>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#configuration>_options","title":"Configuration Options","text":"<p>Once enabled, you can set specific attributes for the profiler's behavior.</p>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#displaying>_profiler>_data>_in>_the>_console","title":"Displaying Profiler Data in the Console","text":"<p>To see the profiling results printed directly to your console after the inference runs, use the <code>PROFILER_SHOW_DATA</code> attribute.</p> <pre><code>// Configure the profiler to print its report to the console\ndxrt::Configuration::GetInstance().SetAttribute(dxrt::Configuration::ITEM::PROFILER,\n                                                 dxrt::Configuration::ATTRIBUTE::PROFILER_SHOW_DATA, \"ON\");\n</code></pre> <ul> <li><code>SetAttribute</code>: Sets a specific property for a DXRT feature.</li> <li><code>PROFILER_SHOW_DATA</code>: The attribute to control console output.</li> <li><code>\"ON\"</code>: A string value to enable this attribute. Use <code>\"OFF\"</code> to disable it.</li> </ul>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#saving>_profiler>_data>_to>_a>_file","title":"Saving Profiler Data to a File","text":"<p>To save the profiling report to a file for later analysis, use the <code>PROFILER_SAVE_DATA</code> attribute. The resulting report is generated in the same folder with the name <code>profiler.json</code>. \ud83d\udcc4</p> <pre><code>// Configure the profiler to save its report to a file\ndxrt::Configuration::GetInstance().SetAttribute(dxrt::Configuration::ITEM::PROFILER,\n                                                 dxrt::Configuration::ATTRIBUTE::PROFILER_SAVE_DATA, \"ON\");\n</code></pre> <ul> <li><code>PROFILER_SAVE_DATA</code>: The attribute to control file output.</li> <li><code>\"ON\"</code>: A string value to enable file saving. Use <code>\"OFF\"</code> to disable it.</li> </ul>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#complete>_code>_example","title":"Complete Code Example","text":"<p>Here is a complete example showing how to apply all the configurations within a <code>try-catch</code> block before creating the <code>InferenceEngine</code>.</p> <pre><code>try \n{\n    // Step 1: Enable the profiler\n    dxrt::Configuration::GetInstance().SetEnable(dxrt::Configuration::ITEM::PROFILER, true);\n\n    // Step 2: Set attributes to show data in console and save to a file\n    dxrt::Configuration::GetInstance().SetAttribute(dxrt::Configuration::ITEM::PROFILER, \n                                                     dxrt::Configuration::ATTRIBUTE::PROFILER_SHOW_DATA, \"ON\");\n    dxrt::Configuration::GetInstance().SetAttribute(dxrt::Configuration::ITEM::PROFILER, \n                                                     dxrt::Configuration::ATTRIBUTE::PROFILER_SAVE_DATA, \"ON\");\n\n    // Step 3: Create the InferenceEngine instance and run inference\n    // The profiler will automatically work on the models run by this engine.\n    dxrt::InferenceEngine ie(model_path);\n\n    // ... run inference loop ...\n}\ncatch (const dxrt::Exception&amp; e)\n{\n    // ... handle exceptions ...\n}\n</code></pre>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#camera>_inference>_display","title":"Camera / Inference / Display","text":"<p>The following is an example of a pattern that performs inference using two models on a single camera input and combines the results from both models for display.</p> <p>  Figure. Multi-model and Multi-output    </p> <p>Multi-model, Async, Wait Thread <code>(CPU_1 \u2192 {NPU_1 + NPU_2} \u2192 CPU_2</code></p> <p><code>display_async_wait.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\n// input processing main thread with 2 InferenceEngine (asynchronous) \n// display thread \n\nstruct FrameJobId {\n    int jobId_A = -1;\n    int jobId_B = -1;\n    void* frameBuffer = nullptr;\n    int loopIndex = -1;\n};\n\nstatic const int BUFFER_POOL_SIZE = 10;\nstatic const int QUEUE_SIZE = 10;\n\nstatic ConcurrentQueue&lt;FrameJobId&gt; gFrameJobIdQueue(QUEUE_SIZE);\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_A;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_B;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gFrameBufferPool;\n\n// total display count\nstatic std::atomic&lt;int&gt; gTotalDisplayCount = {0};\n\nstatic int displayThreadFunc(int loopCount, dxrt::InferenceEngine&amp; ieA, dxrt::InferenceEngine&amp; ieB)\n{\n\n    while(gTotalDisplayCount.load() &lt; loopCount)\n    {\n        // consumer framebuffer &amp; jobIds\n        auto frameJobId = gFrameJobIdQueue.pop();\n\n        // output data of ieA\n        auto outputA = ieA.Wait(frameJobId.jobId_A);\n\n        // output data of ieB\n        auto outputB = ieB.Wait(frameJobId.jobId_B);\n\n        // post-processing w/ outputA &amp; outputB\n        postProcessing(outputA, outputB);\n\n        gTotalDisplayCount++;\n\n        // display (update framebuffer)\n    }\n\n    return 0;\n}\n\n\nint main(int argc, char* argv[])\n{\n    ...\n\n    try\n    {\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieA(modelPath_A);\n\n        gInputBufferPool_A = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieA.GetInputSize());\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieB(modelPath_B);\n\n        gInputBufferPool_B = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieB.GetInputSize());\n\n        const int W = 512, H = 512, CH = 3;\n        gFrameBufferPool = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, W*H*CH);\n\n        // create thread\n        std::thread displayThread(displayThreadFunc, loop_count, std::ref(ieA), std::ref(ieB));\n\n        // input processing\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            uint8_t* frameBuffer = gFrameBufferPool-&gt;pointer(); \n            readFrameBuffer(frameBuffer, W, H, CH);\n\n            uint8_t* inputA = gInputBufferPool_A-&gt;pointer();\n            preProcessing(inputA, frameBuffer);\n\n            uint8_t* inputB = gInputBufferPool_B-&gt;pointer();\n            preProcessing(inputB, frameBuffer);\n\n            // struct to pass to cpu operation thread \n            FrameJobId frameJobId;\n\n            // start inference of A model\n            frameJobId.jobId_A = ieA.RunAsync(inputA);\n\n            // start inference of B model\n            frameJobId.jobId_B = ieB.RunAsync(inputB);\n\n            // framebuffer used for input data\n            frameJobId.frameBuffer = frameBuffer;\n            frameJobId.loopIndex = i;\n\n            // producer frame &amp; jobId\n            gFrameJobIdQueue.push(frameJobId);\n\n        }\n\n        displayThread.join();\n\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p> <p>The following is an example of a pattern that sequentially performs operations using two models and CPU processing. The inference result from Model A is processed through CPU computation and then used as input data for Model B. Finally, the result from Model B is handled for display.</p> <p>  Figure. CPU and NPU Pipeline Operation    </p> <p>Multi-model, Async, Wait Thread <code>(CPU_1 \u2192 NPU_1 \u2192 CPU_2 \u2192 NPU_2 \u2192 CPU_3)</code></p> <p><code>display_async_pipe.cpp</code> <pre><code>// DX-RT includes\n#include \"dxrt/dxrt_api.h\"\n...\n\n// input main thread \n// 1 cpu processing thread  \n// 1 display thread \n\nstruct FrameJobId {\n    int jobId_A = -1;\n    int jobId_B = -1;\n    uint8_t* inputBufferA;\n    uint8_t* inputBufferB;\n    void* frameBuffer = nullptr;\n\n    int loopIndex;\n};\n\nstatic const int BUFFER_POOL_SIZE = 10;\nstatic const int QUEUE_SIZE = 10;\n\nstatic ConcurrentQueue&lt;FrameJobId&gt; gCPUOPQueue(QUEUE_SIZE);\nstatic ConcurrentQueue&lt;FrameJobId&gt; gDisplayQueue(QUEUE_SIZE);\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_A;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gInputBufferPool_B;\nstatic std::shared_ptr&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt; gFrameBufferPool;\n\n// total display count\nstatic std::atomic&lt;int&gt; gTotalDisplayCount = {0};\n\n\nstatic int displayThreadFunc(int loopCount, dxrt::InferenceEngine&amp; ieB)\n{\n\n    while(gTotalDisplayCount.load() &lt; loopCount)\n    {\n        // consumer framebuffer &amp; jobIds\n        auto frameJobId = gDisplayQueue.pop();\n\n        // output data of ieB\n        auto outputB = ieB.Wait(frameJobId.jobId_B);\n\n        // post-processing w/ outputA &amp; outputB\n        postProcessingB(outputB);\n\n        gTotalDisplayCount++;\n\n        // display (update framebuffer)\n        if ( frameJobId.loopIndex == (loopCount - 1)) break;\n    }\n\n    return 0;\n}\n\nstatic int cpuOperationThreadFunc(int loopCount, dxrt::InferenceEngine&amp; ieA, dxrt::InferenceEngine&amp; ieB)\n{\n\n    while(gTotalDisplayCount.load() &lt; loopCount)\n    {\n        // consumer framebuffer &amp; jobIds\n        auto frameJobIdA = gCPUOPQueue.pop();\n\n        // output data of ieA\n        auto outputA = ieA.Wait(frameJobIdA.jobId_A);\n\n        // post-processing w/ outputA\n        postProcessingA(frameJobIdA.inputBufferB, outputA);\n\n        FrameJobId frameJobIdB;\n        frameJobIdB.loopIndex = frameJobIdA.loopIndex;\n        frameJobIdB.jobId_B = ieB.RunAsync(frameJobIdA.inputBufferB);\n\n        gDisplayQueue.push(frameJobIdB);\n\n        // display (update framebuffer)\n        if ( frameJobIdA.loopIndex == (loopCount - 1)) break;\n    }\n\n    return 0;\n}\n\n\nint main(int argc, char* argv[])\n{\n\n    ...\n\n    try\n    {\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieA(modelPath);\n\n        gInputBufferPool_A = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieA.GetInputSize());\n\n        // create inference engine instance with model\n        dxrt::InferenceEngine ieB(modelPath);\n\n        gInputBufferPool_B = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, ieB.GetInputSize());\n\n        const int W = 512, H = 512, CH = 3;\n        gFrameBufferPool = std::make_shared&lt;SimpleCircularBufferPool&lt;uint8_t&gt;&gt;(BUFFER_POOL_SIZE, W*H*CH);\n\n        // create thread\n        std::thread cpuOperationThread(cpuOperationThreadFunc, loop_count, std::ref(ieA), std::ref(ieB));\n        std::thread displayThread(displayThreadFunc, loop_count, std::ref(ieB));\n\n\n        // input processing\n        for(int i = 0; i &lt; loop_count; ++i)\n        {\n            uint8_t* frameBuffer = gFrameBufferPool-&gt;pointer(); \n            readFrameBuffer(frameBuffer, W, H, CH);\n\n            uint8_t* inputA = gInputBufferPool_A-&gt;pointer();\n            preProcessing(inputA, frameBuffer);\n\n            // struct to pass to a thread \n            FrameJobId frameJobId;\n\n            frameJobId.inputBufferA = inputA;\n            frameJobId.inputBufferB = gInputBufferPool_B-&gt;pointer();\n\n            // start inference of A model\n            frameJobId.jobId_A = ieA.RunAsync(inputA);\n\n            // framebuffer used for input data\n            frameJobId.frameBuffer = frameBuffer;\n            frameJobId.loopIndex = i;\n\n            // producer frame &amp; jobId\n            gCPUOPQueue.push(frameJobId);\n\n        }\n\n        cpuOperationThread.join();\n        displayThread.join();\n\n    }\n    catch (const dxrt::Exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch (const std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n    catch(...)\n    {\n        std::cerr &lt;&lt; \"Exception\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#exception","title":"Exception","text":"<p>The error codes and types of exceptions for error handling are as follows.</p> <pre><code>enum ERROR_CODE {\n        DEFAULT = 0x0100,\n        FILE_NOT_FOUND,\n        NULL_POINTER,\n        FILE_IO,\n        INVALID_ARGUMENT,\n        INVALID_OPERATION,\n        INVALID_MODEL,\n        MODEL_PARSING,\n        SERVICE_IO,\n        DEVICE_IO\n    };\n</code></pre> <ul> <li>FileNotFoundException  </li> <li>NullPointerException  </li> <li>FileIOException  </li> <li>InvalidArgumentException  </li> <li>InvalidOperationException  </li> <li>InvalidModelException  </li> <li>ModelParsingException  </li> <li>ServiceIOException  </li> <li>DeviceIOException  </li> </ul> <pre><code>    // try/catch prototype\n\n    try\n    {\n        // DX-RT APIs ...\n    }\n    catch(const dxrt::Exception&amp; e)  // exception for inference engine \n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; \" error-code=\" &lt;&lt; e.code() &lt;&lt; std::endl;\n        return -1; // or std::exit(-1);\n    }\n    catch(std::exception&amp; e)\n    {\n        std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;  // or std::exit(-1);\n    }\n</code></pre>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#multi-input>_inference","title":"Multi-Input Inference","text":"<p>This guide explains various methods for performing inference on multi-input models using the <code>dxrt::InferenceEngine</code>. The examples cover different input formats, synchronous and asynchronous execution, and batch processing.</p>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#model>_information","title":"Model Information","text":"<p>Before running inference, it's useful to inspect the model's properties. The <code>printModelInfo</code> function shows how to query the inference engine for details about the model's input and output tensors.</p> <ul> <li><code>ie.IsMultiInputModel()</code>: Checks if the loaded model has multiple inputs.</li> <li><code>ie.GetInputTensorCount()</code>: Gets the number of input tensors.</li> <li><code>ie.GetInputTensorNames()</code>: Retrieves the names of all input tensors.</li> <li><code>ie.GetInputTensorSizes()</code>: Gets the size (in bytes) of each input tensor.</li> <li><code>ie.GetOutputTensorNames()</code> / <code>ie.GetOutputTensorSizes()</code>: Provide similar information for output tensors.</li> </ul> <pre><code>void printModelInfo(dxrt::InferenceEngine&amp; ie) {\n    if (ie.IsMultiInputModel()) {\n        std::cout &lt;&lt; \"Input tensor count: \" &lt;&lt; ie.GetInputTensorCount() &lt;&lt; std::endl;\n        auto inputNames = ie.GetInputTensorNames();\n        auto inputSizes = ie.GetInputTensorSizes();\n        for (size_t i = 0; i &lt; inputNames.size(); ++i) {\n            std::cout &lt;&lt; \"  \" &lt;&lt; inputNames[i] &lt;&lt; \": \" &lt;&lt; inputSizes[i] &lt;&lt; \" bytes\" &lt;&lt; std::endl;\n        }\n    }\n}\n</code></pre>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#synchronous>_single>_inference","title":"Synchronous Single Inference","text":"<p>These examples demonstrate different ways to run a single inference request synchronously.</p>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#input>_formats","title":"Input Formats","text":""},{"location":"docs/09_01_C%2B%2B_Tutorials.html#a>_dictionary>_format>_stdmapstdstring>_void","title":"A. Dictionary Format (<code>std::map&lt;std::string, void*&gt;</code>)","text":"<p>This is the most robust method. You provide a map where keys are the tensor names and values are pointers to the input data. This format is not sensitive to the order of tensors.</p> <ul> <li>API: <code>ie.RunMultiInput(inputTensors)</code></li> <li>Use Case: Recommended for clarity and to avoid errors from tensor reordering.</li> </ul> <pre><code>// Create input data\nstd::map&lt;std::string, void*&gt; inputTensors;\ninputTensors[\"input_1\"] = inputData1.data();\ninputTensors[\"input_2\"] = inputData2.data();\n\n// Run inference\nauto outputs = ie.RunMultiInput(inputTensors);\n</code></pre>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#b>_vector>_format>_stdvectorvoid","title":"B. Vector Format (<code>std::vector&lt;void*&gt;</code>)","text":"<p>You provide a vector of pointers to the input data. The order of pointers in the vector must match the order returned by <code>ie.GetInputTensorNames()</code>.</p> <ul> <li>API: <code>ie.RunMultiInput(inputPtrs)</code></li> <li>Use Case: When tensor order is known and fixed. Can be slightly more performant than the map-based approach due to less overhead.</li> </ul> <pre><code>// Create input data in the correct order\nstd::vector&lt;void*&gt; inputPtrs;\ninputPtrs.push_back(inputData1.data()); // Corresponds to first name in GetInputTensorNames()\ninputPtrs.push_back(inputData2.data()); // Corresponds to second name\n\n// Run inference\nauto outputs = ie.RunMultiInput(inputPtrs);\n</code></pre>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#c>_auto-split>_concatenated>_buffer","title":"C. Auto-Split Concatenated Buffer","text":"<p>You provide a single, contiguous buffer containing all input data concatenated together. The engine automatically splits this buffer into the correct tensor inputs based on their sizes. The concatenation order must match the order from <code>ie.GetInputTensorNames()</code>.</p> <ul> <li>API: <code>ie.Run(concatenatedInput.data())</code></li> <li>Use Case: Efficient when input data is already in a single block or when interfacing with systems that provide data this way.</li> </ul> <pre><code>// Create a single buffer with all input data concatenated\nauto concatenatedInput = createDummyInput(ie.GetInputSize());\n\n// Run inference\nauto outputs = ie.Run(concatenatedInput.data());\n</code></pre>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#output>_buffer>_management","title":"Output Buffer Management","text":"<p>For each synchronous method, you can either let the engine allocate output memory automatically or provide a pre-allocated buffer for performance gains.</p> <ul> <li> <p>Auto-Allocated Output (No Buffer Provided): Simpler to use. The engine returns smart pointers to newly allocated memory.</p> <pre><code>// Engine allocates and manages output memory\nauto outputs = ie.RunMultiInput(inputTensors);\n</code></pre> </li> <li> <p>User-Provided Output Buffer: More performant as it avoids repeated memory allocations. The user is responsible for allocating a buffer of size <code>ie.GetOutputSize()</code>.</p> <pre><code>// User allocates the output buffer\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// Run inference, placing results in the provided buffer\nauto outputs = ie.RunMultiInput(inputTensors, nullptr, outputBuffer.data());\n</code></pre> </li> </ul>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#synchronous>_batch>_inference","title":"Synchronous Batch Inference","text":"<p>For processing multiple inputs at once to maximize throughput, you can use the batch inference API. This is more efficient than running single inferences in a loop.</p> <ul> <li>API: <code>ie.Run(batchInputPtrs, batchOutputPtrs, userArgs)</code></li> <li>Input: A vector of pointers, where each pointer is a concatenated buffer for one sample in the batch.</li> <li>Output: A vector of pointers, where each pointer is a pre-allocated buffer for the corresponding sample's output.</li> </ul> <pre><code>int batchSize = 3;\nstd::vector&lt;void*&gt; batchInputPtrs;\nstd::vector&lt;void*&gt; batchOutputPtrs;\n\n// Prepare input and output buffers for each sample in the batch\nfor (int i = 0; i &lt; batchSize; ++i) {\n    // Each input is a full concatenated buffer\n    batchInputData[i] = createDummyInput(ie.GetInputSize());\n    batchInputPtrs.push_back(batchInputData[i].data());\n\n    // Pre-allocate output buffer for each sample\n    batchOutputData[i].resize(ie.GetOutputSize());\n    batchOutputPtrs.push_back(batchOutputData[i].data());\n}\n\n// Run batch inference\nauto batchOutputs = ie.Run(batchInputPtrs, batchOutputPtrs);\n</code></pre>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#asynchronous>_inference","title":"Asynchronous Inference","text":"<p>Asynchronous APIs allow you to submit inference requests without blocking the calling thread. The results are returned later via a callback function. This is ideal for applications that need to remain responsive, such as those with a user interface.</p> <ul> <li>APIs:<ul> <li><code>ie.RunAsyncMultiInput(inputTensors, userArg)</code></li> <li><code>ie.RunAsync(concatenatedInput.data(), userArg)</code></li> </ul> </li> <li>Callback Registration: <code>ie.RegisterCallback(callback_function)</code></li> </ul> <p>The <code>AsyncInferenceHandler</code> class demonstrates how to manage state across multiple asynchronous calls.</p> <ul> <li>Register a Callback: Provide a function that the engine will call upon completion of each async request. The callback receives the output tensors and a <code>userArg</code> pointer for context.</li> <li>Submit Requests: Call an <code>RunAsync</code> variant. This call returns immediately with a job ID.</li> <li>Process in Callback: The callback function is executed in a separate worker thread. Here, you can process the results. It's crucial to ensure thread safety if you modify shared data.</li> </ul> <pre><code>// 1. Create a handler and register its callback method\nAsyncInferenceHandler handler(asyncCount);\nie.RegisterCallback([&amp;handler](dxrt::TensorPtrs&amp; outputs, void* userArg) -&gt; int {\n    return handler.callback(outputs, userArg);\n});\n\n// 2. Submit multiple async requests in a loop\nfor (int i = 0; i &lt; asyncCount; ++i) {\n    void* userArg = reinterpret_cast&lt;void*&gt;(static_cast&lt;uintptr_t&gt;(i));\n    // Each call is non-blocking\n    ie.RunAsyncMultiInput(asyncInputTensors[i], userArg);\n}\n\n// 3. Wait for all callbacks to complete\nhandler.waitForCompletion();\n\n// 4. Clear the callback when done\nie.RegisterCallback(nullptr);\n</code></pre>"},{"location":"docs/09_01_C%2B%2B_Tutorials.html#examples","title":"Examples","text":"<p>The examples provided earlier are actual code samples that can be executed. Please refer to them for practical use.  </p> <ul> <li><code>display_async_pipe</code>     : An example using <code>[CPU_1 \u2192 {NPU_1 + NPU_2} \u2192 CPU_2]</code> pattern  </li> <li><code>display_async_wait</code>     : An example using <code>[CPU_1 \u2192 NPU_1 \u2192 CPU_2 \u2192 NPU_2 \u2192 CPU_3]</code> pattern  </li> <li><code>display_async_thread</code>     : An example using single model and multi threads  </li> <li><code>display_async_models_1</code>     : An example using multi models and multi threads (Inference Engine is created within each thread)  </li> <li><code>display_async_models_2</code>     : An example using multi models and multi threads (Inference Engine is created in the main thread)  </li> <li><code>run_async_model</code>     : A performance-optimized example using a callback function  </li> <li><code>run_async_model_thread</code>     : An example using a single inference engine, callback function, and thread     : Usage method when there is a single AI model and multiple inputs  </li> <li><code>run_async_model_wait</code>     : An example using threads and waits  </li> <li><code>run_async_model_conf</code>     : An example of using configuration  </li> <li><code>run_async_model_profiler</code>     : An example of using profiler  </li> <li><code>run_async_model_conf</code>     : An example of using configuration and device status  </li> <li><code>run_async_model_profiler</code>     : An example of using profiler configuration </li> <li><code>run_sync_model</code>     : An example using a single thread  </li> <li><code>run_sync_model_thread</code>     : An example running an inference engine on multiple threads  </li> <li><code>run_sync_model_bound</code>     : An example of specifying an NPU using the bound option  </li> <li><code>run_batch_model</code>     : An example of using batch inference  </li> <li><code>multi_input_model_inference</code>     : An example of using multi-input model inference  </li> </ul>"},{"location":"docs/09_02_Python_Tutorials.html","title":"Python Tutorials","text":""},{"location":"docs/09_02_Python_Tutorials.html#run>_synchronous","title":"Run (Synchronous)","text":"<p>The synchronous Run method uses a single NPU core to perform inference in a blocking manner. It can be configured to utilize multiple NPU cores simultaneously by employing threads to run each core independently. (Refer to Figure in Section 5.2. Inference Workflow)  </p> <p>Inference Engine Run (Python) </p> <p><code>run_sync_model.py</code></p> <pre><code># DX-RT importes\nfrom dx_engine import InferenceEngine\n...\n\n\nif __name__ == \"__main__\":\n    ...    \n\n    # create inference engine instance with model\n    ie = InferenceEngine(modelPath)\n\n    input = [np.zeros(ie.GetInputSize(), dtype=np.uint8)]\n\n    # inference loop\n    for i in range(loop_count):\n\n        # inference synchronously \n        # use only one npu core \n        outputs = ie.Run(input)\n\n        # post processing \n        postProcessing(outputs)\n\n    exit(0)\n</code></pre>"},{"location":"docs/09_02_Python_Tutorials.html#runasync>_asynchronous","title":"RunAsync (Asynchronous)","text":"<p>The asynchronous Run mode is a method that performs inference asynchronously while utilizing multiple NPU cores simultaneously. It can be implemented to maximize NPU resources through a callback function or a thread wait mechanism. </p> <p>Inference Engine RunAsync, Callback, User Argument  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>The following is an example of asynchronous inference using a callback function. A user argument can be used to synchronize the input with the output of the callback.  </p> <p>Inference Engine RunAsync, Callback, User Argument (Python)</p> <p><code>run_async_model.py</code></p> <pre><code>from dx_engine import InferenceEngine\n...\n\nq = queue.Queue()\ngLoopCount = 0\n\nlock = threading.Lock()\n\ndef onInferenceCallbackFunc(outputs, user_arg):\n\n    # the outputs are guaranteed to be valid only within this callback function\n    # processing this callback functions as quickly as possible is beneficial \n    # for improving inference performance\n\n    global gLoopCount\n\n    # Mutex locks should be properly adjusted \n    # to ensure that callback functions are thread-safe.\n    with lock:\n        # user data type casting\n        index, loop_count = user_arg\n\n\n        # post processing\n        #postProcessing(outputs);\n\n        # something to do\n\n        print(\"Inference output (callback) index=\", index)\n\n        gLoopCount += 1\n        if ( gLoopCount == loop_count ) :\n            print(\"Complete Callback\")\n            q.put(0)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n\n    ...    \n\n    # create inference engine instance with model\n    ie = InferenceEngine(modelPath)\n\n    # register call back function\n    ie.register_callback(onInferenceCallbackFunc)\n\n\n    input = [np.zeros(ie.GetInputSize(), dtype=np.uint8)]\n\n    # inference loop\n    for i in range(loop_count):\n\n        # inference asynchronously, use all npu cores\n        # if device-load &gt;= max-load-value, this function will block  \n        ie.RunAsync(input, user_arg=[i, loop_count])\n\n        print(\"Inference start (async)\", i)\n\n    exit(q.get())\n</code></pre> <p>The following is an example where multiple threads start input and inference, and a single callback processes the output.</p> <p>Inference Engine RunAsync, Callback, User Argument, Thread  </p> <ul> <li>the outputs are guaranteed to be valid only within this callback function  </li> <li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>Inference Engine RunAsync, Callback, User Argument, Thread (Python)</p> <p><code>run_async_model_thread.py</code> <pre><code>from dx_engine import InferenceEngine\n...\n\nTHRAD_COUNT = 3\ntotal_count = 0\nq = queue.Queue()\n\nlock = threading.Lock()\n\n\ndef inferenceThreadFunc(ie, threadIndex, loopCount):\n\n    # input\n    input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]\n\n    # inference loop\n    for i in range(loopCount):\n\n        # inference asynchronously, use all npu cores\n        # if device-load &gt;= max-load-value, this function will block  \n        ie.RunAsync(input,user_arg = [i, loopCount, threadIndex])\n\n    return 0\n\ndef onInferenceCallbackFunc(outputs, user_arg):\n    # the outputs are guaranteed to be valid only within this callback function\n    # processing this callback functions as quickly as possible is beneficial \n    # for improving inference performance\n\n    global total_count\n\n    # Mutex locks should be properly adjusted \n    # to ensure that callback functions are thread-safe.\n    with lock:\n        # user data type casting\n        index = user_arg[0]\n        loop_count = user_arg[1]\n        thread_index = user_arg[2]\n\n        # post processing\n        #postProcessing(outputs);\n\n        # something to do\n\n        total_count += 1\n\n        if ( total_count ==  loop_count * THRAD_COUNT) :\n            q.put(0)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    ...    \n\n    # create inference engine instance with model\n    ie = InferenceEngine(modelPath)\n\n    # register call back function\n    ie.register_callback(onInferenceCallbackFunc)\n\n\n    t1 = threading.Thread(target=inferenceThreadFunc, args=(ie, 0, loop_count))\n    t2 = threading.Thread(target=inferenceThreadFunc, args=(ie, 1, loop_count))\n    t3 = threading.Thread(target=inferenceThreadFunc, args=(ie, 2, loop_count))\n\n    # Start and join\n    t1.start()\n    t2.start()\n    t3.start()\n\n\n    # join\n    t1.join()\n    t2.join()\n    t3.join()\n\n\n    exit(q.get())\n</code></pre></p> <p>The following is an example of performing asynchronous inference by creating an inference wait thread. The main thread starts input and inference, and the inference wait thread retrieves the output data corresponding to the input.</p> <p>Inference Engine RunAsync, Wait  </p> <ul> <li>inference asynchronously, use all npu cores  </li> <li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li> </ul> <p>Inference Engine RunAsync, Wait (Python)</p> <p><code>run_async_model_wait.py</code> <pre><code># DX-RT imports\nfrom dx_engine import InferenceEngine\n...\n\nq = queue.Queue()\n\n\ndef inferenceThreadFunc(ie, loopCount):\n\n    count = 0\n\n    while(True):\n\n        # pop item from queue \n        jobId = q.get()\n\n        # waiting for the inference to complete by jobId\n        # ownership of the outputs is transferred to the user \n        outputs = ie.Wait(jobId)\n\n        # post processing\n        # postProcessing(outputs);\n\n        # something to do\n\n        count += 1\n        if ( count &gt;= loopCount ):\n            break\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    ...\n\n    # create inference engine instance with model\n    with InferenceEngine(modelPath) as ie:\n\n        # do not register call back function\n        # ie.register_callback(onInferenceCallbackFunc)\n\n        t1 = threading.Thread(target=inferenceThreadFunc, args=(ie, loop_count))\n\n        t1.start()\n\n        input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]\n\n        # inference loop\n        for i in range(loop_count):\n\n            # inference asynchronously, use all npu cores\n            # if device-load &gt;= max-load-value, this function will block  \n            jobId = ie.run_async(input, user_arg=0)\n\n            q.put(jobId)\n\n        t1.join()\n\n    exit(0)\n</code></pre></p>"},{"location":"docs/09_02_Python_Tutorials.html#run>_batch","title":"Run (Batch)","text":"<p>The following is an example of batch inference with multiple inputs and multiple outputs.</p> <p><code>run_batch_model.py</code></p> <pre><code>import numpy as np\nimport sys\nfrom dx_engine import InferenceEngine\nfrom dx_engine import InferenceOption\n\n\nif __name__ == \"__main__\":\n    ...\n\n    # create inference engine instance with model\n    with InferenceEngine(modelPath) as ie:\n\n        input_buffers = []\n        output_buffers = []\n        index = 0\n        for b in range(batch_count):\n            input_buffers.append([np.array([np.random.randint(0, 255)],  dtype=np.uint8)])\n            output_buffers.append([np.zeros(ie.get_output_size(), dtype=np.uint8)])\n            index = index + 1\n\n        # inference loop\n        for i in range(loop_count):\n\n            # batch inference\n            # It operates asynchronously internally \n            # for the specified number of batches and returns the results\n            results = ie.run_batch(input_buffers, output_buffers)\n\n            # post processing \n\n    exit(0)\n</code></pre>"},{"location":"docs/09_02_Python_Tutorials.html#inference>_option","title":"Inference Option","text":"<p>The following inference options allow you to specify an NPU core for performing inference.</p> <p>Inference Engine Run, Inference Option  </p> <ul> <li> select devices default device is <code>[]</code> Choose the device to utilize  (ex. <code>[0, 2]</code>)   </li> <li> select bound option per device <code>InferenceOption.BOUND_OPTION.NPU_ALL</code> <code>InferenceOption.BOUND_OPTION.NPU_0</code> <code>InferenceOption.BOUND_OPTION.NPU_1</code> <code>InferenceOption.BOUND_OPTION.NPU_2</code> <code>InferenceOption.BOUND_OPTION.NPU_01</code> <code>InferenceOption.BOUND_OPTION.NPU_12</code> <code>InferenceOption.BOUND_OPTION.NPU_02</code> </li> <li> use onnx runtime library (<code>ORT</code>) <code>set_use_ort / get_use_ort</code> </li> </ul> <p>NPU_ALL / NPU_0 / NPU_1 / NPU_2 <pre><code># DX-RT imports\nfrom dx_engine import InferenceEngine, InferenceOption\n...\n\nif __name__ == \"__main__\":\n    ...\n\n    # inference option\n    option = InferenceOption()\n\n    print(\"Inference Options:\")\n\n    # select devices\n    option.devices = [0]\n\n    # NPU bound opion (NPU_ALL or NPU_0 or NPU_1 or NPU_2)\n    option.bound_option = InferenceOption.BOUND_OPTION.NPU_ALL\n\n    # use ONNX Runtime (True or False)\n    option.use_ort = False\n\n    # create inference engine instance with model\n    with InferenceEngine(modelPath, option) as ie:\n\n        input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]\n\n        # inference loop\n        for i in range(loop_count):\n\n            # inference synchronously \n            # use only one npu core \n            # ownership of the outputs is transferred to the user \n            outputs = ie.run(input)\n\n            # post processing \n            #postProcessing(outputs)\n            print(\"Inference outputs \", i)\n\n    exit(0)\n</code></pre></p>"},{"location":"docs/09_02_Python_Tutorials.html#configuration>_and>_devicestatus","title":"Configuration and DeviceStatus","text":"<p>This guide explains how to use the <code>Configuration</code> class to set up the inference engine and the <code>DeviceStatus</code> class to monitor hardware status.</p>"},{"location":"docs/09_02_Python_Tutorials.html#engine>_configuration","title":"Engine Configuration \u2699\ufe0f","text":"<p>The <code>Configuration</code> class allows you to set engine parameters and retrieve version information before running inference.</p> <pre><code># Create a configuration object\nconfig = Configuration()\n\n# Enable options like showing model details or profiling information\nconfig.set_enable(Configuration.ITEM.SHOW_MODEL_INFO, True)\nconfig.set_enable(Configuration.ITEM.SHOW_PROFILE, True)\n\n# Retrieve version information\nlogger.info('Runtime framework version: ' + config.get_version())\nlogger.info('Device driver version: ' + config.get_driver_version())\n</code></pre> <ul> <li><code>Configuration()</code>: Creates an object to manage engine settings.</li> <li><code>config.set_enable(...)</code>: Turns specific engine features on or off. In this case, it enables printing model information and performance profiles upon loading.</li> <li><code>config.get_version()</code>: Fetches read-only information, such as software and driver versions.</li> </ul>"},{"location":"docs/09_02_Python_Tutorials.html#querying>_device>_status","title":"Querying Device Status \ud83d\udda5\ufe0f","text":"<p>The <code>DeviceStatus</code> class is used to get the real-time operational status of the NPU hardware, such as temperature and clock speed. This is typically done after inference is complete to check the hardware's state.</p> <pre><code># Get the number of available devices\ndevice_count = DeviceStatus.get_device_count()\n\n# Loop through each device\nfor i in range(device_count):\n    # Get a status snapshot for the current device\n    device_status = DeviceStatus.get_current_status(i)\n    logger.info(f'Device {device_status.get_id()}')\n\n    # Loop through each NPU core to get its metrics\n    for c in range(3): # Assuming 3 cores for this example\n        logger.info(\n            f'   NPU Core {c} '\n            f'Temperature: {device_status.get_temperature(c)} '\n            f'Voltage: {device_status.get_npu_voltage(c)} '\n            f'Clock: {device_status.get_npu_clock(c)}'\n        )\n</code></pre> <ul> <li><code>DeviceStatus.get_device_count()</code>: A static method that returns the number of connected DEEPX devices.</li> <li><code>DeviceStatus.get_current_status(i)</code>: Returns a status object containing a snapshot of the hardware metrics for device <code>i</code> at that moment.</li> <li><code>device_status.get_temperature(c)</code>: An instance method that returns the temperature (in Celsius) for a specific NPU core <code>c</code>. The <code>get_npu_voltage</code> and <code>get_npu_clock</code> methods work similarly.</li> </ul>"},{"location":"docs/09_02_Python_Tutorials.html#profiler>_configuration","title":"Profiler Configuration","text":"<p>This guide provides a simple, code-focused manual on how to configure the profiler using the DXRT Python wrapper. The profiler is a powerful tool for analyzing the performance of each layer within your model.</p> <p>Configuration is managed through an instance of the <code>Configuration</code> class.</p>"},{"location":"docs/09_02_Python_Tutorials.html#enabling>_the>_profiler","title":"Enabling the Profiler","text":"<p>Before you can use any profiler features, you must first create a <code>Configuration</code> object and enable the profiler. This is the essential first step.</p> <pre><code># Create a Configuration instance\nconfig = Configuration()\n\n# Enable the profiler feature\nconfig.set_enable(Configuration.ITEM.PROFILER, True)\n</code></pre> <ul> <li><code>config = Configuration()</code>: Creates the object that controls system-wide settings for the runtime.</li> <li><code>set_enable()</code>: This method activates or deactivates a specific DXRT feature.</li> <li><code>Configuration.ITEM.PROFILER</code>: Specifies that the target feature is the profiler.</li> <li><code>True</code>: Enables the profiler. Set to <code>False</code> to disable it.</li> </ul>"},{"location":"docs/09_02_Python_Tutorials.html#configuration>_options","title":"Configuration Options","text":"<p>Once enabled, you can set specific attributes for the profiler's behavior using the same <code>config</code> object.</p>"},{"location":"docs/09_02_Python_Tutorials.html#displaying>_profiler>_data>_in>_the>_console","title":"Displaying Profiler Data in the Console","text":"<p>To see the profiling results printed directly to your console after the inference runs, use the <code>PROFILER_SHOW_DATA</code> attribute.</p> <pre><code># Configure the profiler to print its report to the console\nconfig.set_attribute(Configuration.ITEM.PROFILER,\n                     Configuration.ATTRIBUTE.PROFILER_SHOW_DATA, \"ON\")\n</code></pre> <ul> <li><code>set_attribute()</code>: Sets a specific property for a DXRT feature.</li> <li><code>PROFILER_SHOW_DATA</code>: The attribute to control console output.</li> <li><code>\"ON\"</code>: A string value to enable this attribute. Use <code>\"OFF\"</code> to disable it.</li> </ul>"},{"location":"docs/09_02_Python_Tutorials.html#saving>_profiler>_data>_to>_a>_file","title":"Saving Profiler Data to a File","text":"<p>To save the profiling report to a file for later analysis, use the <code>PROFILER_SAVE_DATA</code> attribute. The resulting report is generated in the same folder with the name <code>profiler.json</code>. \ud83d\udcc4</p> <pre><code># Configure the profiler to save its report to a file\nconfig.set_attribute(Configuration.ITEM.PROFILER,\n                     Configuration.ATTRIBUTE.PROFILER_SAVE_DATA, \"ON\")\n</code></pre> <ul> <li><code>PROFILER_SAVE_DATA</code>: The attribute to control file output.</li> <li><code>\"ON\"</code>: A string value to enable file saving. Use <code>\"OFF\"</code> to disable it.</li> </ul>"},{"location":"docs/09_02_Python_Tutorials.html#complete>_code>_example","title":"Complete Code Example","text":"<p>Here is a complete example showing how to apply all the configurations at the start of your script. These settings are applied globally, and any <code>InferenceEngine</code> instance created afterward will automatically use them.</p> <pre><code>if __name__ == \"__main__\":\n\n    # Step 1: Create a Configuration instance and enable the profiler\n    config = Configuration()\n    config.set_enable(Configuration.ITEM.PROFILER, True)\n\n    # Step 2: Set attributes to show data in console and save to a file\n    config.set_attribute(Configuration.ITEM.PROFILER,\n                         Configuration.ATTRIBUTE.PROFILER_SHOW_DATA, \"ON\")\n    config.set_attribute(Configuration.ITEM.PROFILER,\n                         Configuration.ATTRIBUTE.PROFILER_SAVE_DATA, \"ON\")\n\n    # The configuration is now active.\n    # ...\n\n    # Create an inference engine instance that will now be profiled\n    with InferenceEngine(modelPath) as ie:\n        # ... register callback and run inference loop ...\n</code></pre>"},{"location":"docs/09_02_Python_Tutorials.html#multi-input>_inference","title":"Multi-input Inference","text":"<p>This guide explains various methods for performing inference on multi-input models using the <code>InferenceEngine</code>. The examples cover different input formats, synchronous and asynchronous execution, and batch processing.</p>"},{"location":"docs/09_02_Python_Tutorials.html#model>_information","title":"Model Information","text":"<p>Before running inference, it's useful to inspect the model's properties. The <code>print_model_info</code> function in the example script shows how to query the inference engine for details about the model's input and output tensors.</p> <ul> <li><code>ie.is_multi_input_model()</code>: Checks if the loaded model has multiple inputs.</li> <li><code>ie.get_input_tensor_count()</code>: Gets the number of input tensors.</li> <li><code>ie.get_input_tensor_names()</code>: Retrieves the names of all input tensors.</li> <li><code>ie.get_input_tensor_sizes()</code>: Gets the size (in bytes) of each input tensor.</li> <li><code>ie.get_output_tensor_names()</code> / <code>ie.get_output_tensor_sizes()</code>: Provide similar information for output tensors.</li> </ul> <pre><code>def print_model_info(ie: InferenceEngine) -&gt; None:\n    if ie.is_multi_input_model():\n        print(f\"Input tensor count: {ie.get_input_tensor_count()}\")\n        input_names = ie.get_input_tensor_names()\n        input_sizes = ie.get_input_tensor_sizes()\n        for i, name in enumerate(input_names):\n            print(f\"  {name}: {input_sizes[i]} bytes\")\n</code></pre>"},{"location":"docs/09_02_Python_Tutorials.html#synchronous>_single>_inference","title":"Synchronous Single Inference","text":"<p>These examples demonstrate different ways to run a single inference request synchronously.</p>"},{"location":"docs/09_02_Python_Tutorials.html#input>_formats","title":"Input Formats","text":""},{"location":"docs/09_02_Python_Tutorials.html#a>_dictionary>_format>_dictstr>_npndarray","title":"A. Dictionary Format (<code>Dict[str, np.ndarray]</code>)","text":"<p>This is the most robust method. You provide a dictionary where keys are the tensor names and values are the <code>numpy</code> arrays. This format is not sensitive to the order of tensors.</p> <ul> <li>API: <code>ie.run_multi_input(input_tensors)</code></li> <li>Use Case: Recommended for clarity and to avoid errors from tensor reordering.</li> </ul> <pre><code># Create input data\ninput_names = ie.get_input_tensor_names()\ninput_sizes = ie.get_input_tensor_sizes()\ninput_tensors = {name: create_dummy_input(size) for name, size in zip(input_names, input_sizes)}\n\n# Run inference\noutputs = ie.run_multi_input(input_tensors)\n</code></pre>"},{"location":"docs/09_02_Python_Tutorials.html#b>_list>_format>_listnpndarray","title":"B. List Format (<code>List[np.ndarray]</code>)","text":"<p>You provide a list of <code>numpy</code> arrays. The order of arrays in the list must match the order returned by <code>ie.get_input_tensor_names()</code>.</p> <ul> <li>API: <code>ie.run(input_list)</code></li> <li>Use Case: When tensor order is known and fixed. Can be slightly more performant than the dictionary-based approach due to less overhead.</li> </ul> <pre><code># Create input data in the correct order\ninput_sizes = ie.get_input_tensor_sizes()\ninput_list = [create_dummy_input(size) for size in input_sizes]\n\n# Run inference\noutputs = ie.run(input_list)\n</code></pre>"},{"location":"docs/09_02_Python_Tutorials.html#c>_auto-split>_concatenated>_buffer","title":"C. Auto-Split Concatenated Buffer","text":"<p>You provide a single, contiguous <code>numpy</code> array containing all input data concatenated together. The engine automatically splits this buffer into the correct tensor inputs based on their sizes. The concatenation order must match the order from <code>ie.get_input_tensor_names()</code>.</p> <ul> <li>API: <code>ie.run(concatenated_input)</code></li> <li>Use Case: Efficient when input data is already in a single block or when interfacing with systems that provide data this way.</li> </ul> <pre><code># Create a single buffer with all input data concatenated\ntotal_input_size = ie.get_input_size()\nconcatenated_input = create_dummy_input(total_input_size)\n\n# Run inference\noutputs = ie.run(concatenated_input)\n</code></pre>"},{"location":"docs/09_02_Python_Tutorials.html#output>_buffer>_management","title":"Output Buffer Management","text":"<p>For each synchronous method, you can either let the engine allocate output memory automatically or provide pre-allocated buffers for performance gains.</p> <ul> <li> <p>Auto-Allocated Output (No Buffer Provided): Simpler to use. The engine returns a new list of <code>numpy</code> arrays.</p> <pre><code># Engine allocates and manages output memory\noutputs = ie.run_multi_input(input_tensors)\n</code></pre> </li> <li> <p>User-Provided Output Buffers: More performant as it avoids repeated memory allocations. The user is responsible for creating a list of <code>numpy</code> arrays with the correct sizes.</p> <pre><code># User creates the output buffers\noutput_sizes = ie.get_output_tensor_sizes()\noutput_buffers = [np.zeros(size, dtype=np.uint8) for size in output_sizes]\n\n# Run inference, placing results in the provided buffers\noutputs = ie.run_multi_input(input_tensors, output_buffers=output_buffers)\n</code></pre> </li> </ul>"},{"location":"docs/09_02_Python_Tutorials.html#synchronous>_batch>_inference","title":"Synchronous Batch Inference","text":"<p>For processing multiple inputs at once to maximize throughput, you can use the batch inference capabilities of the <code>run</code> method. This is more efficient than running single inferences in a loop.</p>"},{"location":"docs/09_02_Python_Tutorials.html#a>_explicit>_batch>_format>_listlistnpndarray","title":"A. Explicit Batch Format (<code>List[List[np.ndarray]]</code>)","text":"<p>This is the clearest way to represent a batch. The input is a list of lists, where the outer list represents the batch and each inner list contains all input tensors for a single sample.</p> <ul> <li>API: <code>ie.run(batch_inputs, output_buffers=...)</code></li> <li>Input: A <code>List[List[np.ndarray]]</code>.</li> <li>Output: A <code>List[List[np.ndarray]]</code>.</li> </ul> <pre><code>batch_size = 3\ninput_sizes = ie.get_input_tensor_sizes()\nbatch_inputs = []\nfor i in range(batch_size):\n    sample_inputs = [create_dummy_input(size) for size in input_sizes]\n    batch_inputs.append(sample_inputs)\n\n# Output buffers must also match the batch structure\n# ... create batch_outputs ...\n\n# Run batch inference\nresults = ie.run(batch_inputs, output_buffers=batch_outputs)\n</code></pre>"},{"location":"docs/09_02_Python_Tutorials.html#b>_flattened>_batch>_format>_listnpndarray","title":"B. Flattened Batch Format (<code>List[np.ndarray]</code>)","text":"<p>As a convenience, the API can also accept a single \"flattened\" list of <code>numpy</code> arrays. The total number of arrays must be a multiple of the model's input tensor count. The engine will automatically group them into batches.</p> <ul> <li>API: <code>ie.run(flattened_inputs, output_buffers=...)</code></li> <li>Input: A <code>List[np.ndarray]</code> containing <code>batch_size * num_input_tensors</code> arrays.</li> <li>Output: The result is still returned in the explicit batch format (<code>List[List[np.ndarray]]</code>).</li> </ul> <pre><code>batch_size = 3\ninput_sizes = ie.get_input_tensor_sizes()\nflattened_inputs = []\nfor i in range(batch_size):\n    for size in input_sizes:\n        flattened_inputs.append(create_dummy_input(size))\n\n# ... create flattened_output_buffers ...\n\n# Run batch inference\nresults = ie.run(flattened_inputs, output_buffers=flattened_output_buffers)\n</code></pre>"},{"location":"docs/09_02_Python_Tutorials.html#asynchronous>_inference","title":"Asynchronous Inference","text":"<p>Asynchronous APIs allow you to submit inference requests without blocking the calling thread. The results are returned later via a callback function. This is ideal for applications that need to remain responsive.</p> <ul> <li>APIs:<ul> <li><code>ie.run_async_multi_input(input_tensors, user_arg=...)</code></li> <li><code>ie.run_async(input_data, user_arg=...)</code></li> </ul> </li> <li>Callback Registration: <code>ie.register_callback(callback_function)</code></li> </ul> <p>The <code>AsyncInferenceHandler</code> class in the example demonstrates how to manage state across multiple asynchronous calls.</p> <ul> <li>Register a Callback: Provide a function that the engine will call upon completion of each async request. The callback receives the output arrays and a <code>user_arg</code> for context.</li> <li>Submit Requests: Call an <code>run_async</code> variant. This call returns immediately with a job ID.</li> <li>Process in Callback: The callback function is executed in a separate worker thread. Here, you can process the results. It's crucial to ensure thread safety (e.g., using a <code>threading.Lock</code>) if you modify shared data.</li> </ul> <pre><code># 1. Create a handler and register its callback method\nhandler = AsyncInferenceHandler(async_count)\nie.register_callback(handler.callback)\n\n# 2. Submit multiple async requests in a loop\nfor i in range(async_count):\n    user_arg = f\"async_sample_{i}\"\n    # Each call is non-blocking\n    job_id = ie.run_async_multi_input(input_tensors, user_arg=user_arg)\n\n# 3. Wait for all callbacks to complete\nhandler.wait_for_completion()\n\n# 4. Clear the callback when done\nie.register_callback(None)\n</code></pre>"},{"location":"docs/09_02_Python_Tutorials.html#examples","title":"Examples","text":"<p>The examples provided earlier are actual code samples that can be executed. Please refer to them for practical use. (<code>examples/python</code>)  </p> <ul> <li> <code>run_async_model.py</code> A performance-optimized example using a callback function   </li> <li> <code>run_async_model_thread.py</code> An example using a single inference engine, callback function, and thread   Usage method when there is a single AI model and multiple inputs   </li> <li> <code>run_async_model_wait.py</code> An example using threads and waits   </li> <li> <code>run_async_model_conf.py</code> An example using configuration and device status   </li> <li> <code>run_async_model_profiler.py</code> An example using profiler configuration  </li> <li> <code>run_sync_model.py</code> An example using a single thread   </li> <li> <code>run_sync_model_thread.py</code> An example running an inference engine on multiple threads   </li> <li> <code>run_sync_model_bound.py</code> An example of specifying an NPU using the bound option   </li> <li> <code>multi_input_model_inference.py</code> An example of using multi-input model inference   </li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html","title":"C++ API Reference","text":""},{"location":"docs/10_01_C%2B%2B_API_Reference.html#class>_dxrtinferenceengine","title":"<code>class dxrt::InferenceEngine</code>","text":"<p>This class abstracts the runtime inference executor for a user's compiled model. After a model is loaded, real-time device tasks are scheduled by internal runtime libraries. It supports both synchronous and asynchronous inference modes.</p>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#constructor","title":"Constructor","text":""},{"location":"docs/10_01_C%2B%2B_API_Reference.html#explicit>_inferenceengineconst>_stdstring>_modelpath>_inferenceoption>_option>_defaultinferenceoption","title":"<code>explicit InferenceEngine(const std::string &amp;modelPath, InferenceOption &amp;option = DefaultInferenceOption)</code>","text":"<ul> <li>Description: Loads a model from the specified path and configures the NPU to run it.</li> <li>Parameters:<ul> <li><code>modelPath</code>: The file path to the compiled model (e.g., <code>model.dxnn</code>).</li> <li><code>option</code>: A reference to an <code>InferenceOption</code> object to configure devices and NPU cores.</li> </ul> </li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#member>_functions","title":"Member Functions","text":""},{"location":"docs/10_01_C%2B%2B_API_Reference.html#dispose","title":"<code>Dispose()</code>","text":"<ul> <li>Signature: <code>void Dispose()</code></li> <li>Description: Deallocates resources and performs cleanup. This should be called to release memory and handles held by the engine.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getalltaskoutputs","title":"<code>GetAllTaskOutputs()</code>","text":"<ul> <li>Signature: <code>std::vector&lt;TensorPtrs&gt; GetAllTaskOutputs()</code></li> <li>Description: Retrieves the output tensors of all internal tasks in the model.</li> <li>Returns: A vector of <code>TensorPtrs</code>, where each element represents the outputs of a single task.</li> <li>Note: The legacy function <code>get_outputs()</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getbitmatchmaskint>_index","title":"<code>GetBitmatchMask(int index)</code>","text":"<ul> <li>Signature: <code>std::vector&lt;uint8_t&gt; GetBitmatchMask(int index)</code></li> <li>Description: An internal function to get the bitmatch mask for a given NPU task index.</li> <li>Parameters:<ul> <li><code>index</code>: The index of the NPU task.</li> </ul> </li> <li>Returns: A vector of <code>uint8_t</code> representing the mask.</li> <li>Note: The legacy function <code>bitmatch_mask(int index)</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getcompiletype","title":"<code>GetCompileType()</code>","text":"<ul> <li>Signature: <code>std::string GetCompileType()</code></li> <li>Description: Returns the compile type of the loaded model.</li> <li>Returns: The compile type as a <code>std::string</code>.</li> <li>Note: The legacy function <code>get_compile_type()</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getinputsize","title":"<code>GetInputSize()</code>","text":"<ul> <li>Signature: <code>uint64_t GetInputSize()</code></li> <li>Description: Gets the total size of all input tensors combined in bytes.</li> <li>Returns: The total input size as a <code>uint64_t</code>.</li> <li>Note: The legacy function <code>input_size()</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getinputsvoid>_ptr>_nullptr>_uint64>_t>_phyaddr>_0","title":"<code>GetInputs(void *ptr = nullptr, uint64_t phyAddr = 0)</code>","text":"<ul> <li>Signature: <code>Tensors GetInputs(void *ptr = nullptr, uint64_t phyAddr = 0)</code></li> <li>Description: Retrieves the input tensors for the model. If <code>ptr</code> is null, it returns information about the input memory area within the engine. If <code>ptr</code> and <code>phyAddr</code> are provided, it returns tensor objects pointing to those addresses.</li> <li>Parameters:<ul> <li><code>ptr</code>: An optional pointer to a virtual address for the input data.</li> <li><code>phyAddr</code>: An optional pointer to a physical address for the input data.</li> </ul> </li> <li>Returns: A <code>Tensors</code> (vector of <code>Tensor</code>) object.</li> <li>Note: The legacy function <code>inputs(...)</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getinputsint>_devid","title":"<code>GetInputs(int devId)</code>","text":"<ul> <li>Signature: <code>std::vector&lt;Tensors&gt; GetInputs(int devId)</code></li> <li>Description: Retrieves the input tensors for a specific device ID.</li> <li>Parameters:<ul> <li><code>devId</code>: The ID of the device.</li> </ul> </li> <li>Returns: A vector of <code>Tensors</code> objects.</li> <li>Note: The legacy function <code>inputs(int devId)</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getinputtensorcount","title":"<code>GetInputTensorCount()</code>","text":"<ul> <li>Signature: <code>int GetInputTensorCount() const</code></li> <li>Description: Returns the number of input tensors required by the model.</li> <li>Returns: The count of input tensors.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getinputtensornames","title":"<code>GetInputTensorNames()</code>","text":"<ul> <li>Signature: <code>std::vector&lt;std::string&gt; GetInputTensorNames() const</code></li> <li>Description: Returns the names of all input tensors in the order they should be provided.</li> <li>Returns: A vector of input tensor names.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getinputtensorsizes","title":"<code>GetInputTensorSizes()</code>","text":"<ul> <li>Signature: <code>std::vector&lt;uint64_t&gt; GetInputTensorSizes()</code></li> <li>Description: Gets the individual sizes (in bytes) of each input tensor for multi-input models.</li> <li>Returns: A vector of input tensor sizes, in the order specified by <code>GetInputTensorNames()</code>.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getinputtensortotaskmapping","title":"<code>GetInputTensorToTaskMapping()</code>","text":"<ul> <li>Signature: <code>std::map&lt;std::string, std::string&gt; GetInputTensorToTaskMapping() const</code></li> <li>Description: Returns the mapping from input tensor names to their target tasks within the model graph.</li> <li>Returns: A map where the key is the tensor name and the value is the task name.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getlatency","title":"<code>GetLatency()</code>","text":"<ul> <li>Signature: <code>int GetLatency()</code></li> <li>Description: Gets the latency of the most recent inference in microseconds.</li> <li>Returns: The latency value.</li> <li>Note: The legacy function <code>latency()</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getlatencycnt","title":"<code>GetLatencyCnt()</code>","text":"<ul> <li>Signature: <code>int GetLatencyCnt()</code></li> <li>Description: Gets the total count of latency measurements recorded.</li> <li>Returns: The number of latency measurements.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getlatencymean","title":"<code>GetLatencyMean()</code>","text":"<ul> <li>Signature: <code>double GetLatencyMean()</code></li> <li>Description: Gets the mean (average) of all collected latency values.</li> <li>Returns: The mean latency in microseconds.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getlatencystddev","title":"<code>GetLatencyStdDev()</code>","text":"<ul> <li>Signature: <code>double GetLatencyStdDev()</code></li> <li>Description: Gets the standard deviation of all collected latency values.</li> <li>Returns: The standard deviation of latency.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getlatencyvector","title":"<code>GetLatencyVector()</code>","text":"<ul> <li>Signature: <code>std::vector&lt;int&gt; GetLatencyVector()</code></li> <li>Description: Gets a vector of recent latency measurements.</li> <li>Returns: A vector of latencies in microseconds.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getmodelname","title":"<code>GetModelName()</code>","text":"<ul> <li>Signature: <code>std::string GetModelName()</code></li> <li>Description: Gets the name of the model.</li> <li>Returns: The model name as a <code>std::string</code>.</li> <li>Note: The legacy function <code>name()</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getmodelversion","title":"<code>GetModelVersion()</code>","text":"<ul> <li>Signature: <code>std::string GetModelVersion()</code></li> <li>Description: Returns the DXNN file format version of the loaded model.</li> <li>Returns: The model version string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getnpuinferencetime","title":"<code>GetNpuInferenceTime()</code>","text":"<ul> <li>Signature: <code>uint32_t GetNpuInferenceTime()</code></li> <li>Description: Gets the pure NPU processing time for the most recent inference in microseconds.</li> <li>Returns: The NPU inference time.</li> <li>Note: The legacy function <code>inference_time()</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getnpuinferencetimecnt","title":"<code>GetNpuInferenceTimeCnt()</code>","text":"<ul> <li>Signature: <code>int GetNpuInferenceTimeCnt()</code></li> <li>Description: Gets the total count of NPU inference time measurements recorded.</li> <li>Returns: The number of measurements.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getnpuinferencetimemean","title":"<code>GetNpuInferenceTimeMean()</code>","text":"<ul> <li>Signature: <code>double GetNpuInferenceTimeMean()</code></li> <li>Description: Gets the mean (average) of all collected NPU inference times.</li> <li>Returns: The mean NPU inference time in microseconds.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getnpuinferencetimestddev","title":"<code>GetNpuInferenceTimeStdDev()</code>","text":"<ul> <li>Signature: <code>double GetNpuInferenceTimeStdDev()</code></li> <li>Description: Gets the standard deviation of all collected NPU inference times.</li> <li>Returns: The standard deviation of NPU inference time.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getnpuinferencetimevector","title":"<code>GetNpuInferenceTimeVector()</code>","text":"<ul> <li>Signature: <code>std::vector&lt;uint32_t&gt; GetNpuInferenceTimeVector()</code></li> <li>Description: Gets a vector of recent NPU inference time measurements.</li> <li>Returns: A vector of NPU inference times in microseconds.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getnumtailtasks","title":"<code>GetNumTailTasks()</code>","text":"<ul> <li>Signature: <code>int GetNumTailTasks()</code></li> <li>Description: Returns the number of \"tail\" tasks in the model, which are tasks that have no subsequent tasks.</li> <li>Returns: The number of tail tasks.</li> <li>Note: The legacy function <code>get_num_tails()</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getoutputsvoid>_ptr>_nullptr>_uint64>_t>_phyaddr>_0","title":"<code>GetOutputs(void *ptr = nullptr, uint64_t phyAddr = 0)</code>","text":"<ul> <li>Signature: <code>Tensors GetOutputs(void *ptr = nullptr, uint64_t phyAddr = 0)</code></li> <li>Description: Retrieves the output tensors. If <code>ptr</code> is null, it returns information about the output memory area within the engine. If <code>ptr</code> and <code>phyAddr</code> are provided, it returns tensor objects pointing to those addresses.</li> <li>Parameters:<ul> <li><code>ptr</code>: An optional pointer to a virtual address for the output data.</li> <li><code>phyAddr</code>: An optional pointer to a physical address for the output data.</li> </ul> </li> <li>Returns: A <code>Tensors</code> (vector of <code>Tensor</code>) object.</li> <li>Note: The legacy function <code>outputs(...)</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getoutputsize","title":"<code>GetOutputSize()</code>","text":"<ul> <li>Signature: <code>uint64_t GetOutputSize()</code></li> <li>Description: Gets the total size of all output tensors combined in bytes.</li> <li>Returns: The total output size as a <code>uint64_t</code>.</li> <li>Note: The legacy function <code>output_size()</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getoutputtensornames","title":"<code>GetOutputTensorNames()</code>","text":"<ul> <li>Signature: <code>std::vector&lt;std::string&gt; GetOutputTensorNames() const</code></li> <li>Description: Returns the names of all output tensors in the order they are produced.</li> <li>Returns: A vector of output tensor names.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getoutputtensoroffsetconst>_stdstring>_tensorname>_const","title":"<code>GetOutputTensorOffset(const std::string&amp; tensorName) const</code>","text":"<ul> <li>Signature: <code>size_t GetOutputTensorOffset(const std::string&amp; tensorName) const</code></li> <li>Description: Gets the byte offset for a specific output tensor within the final concatenated output buffer.</li> <li>Parameters:<ul> <li><code>tensorName</code>: The name of the output tensor.</li> </ul> </li> <li>Returns: The offset in bytes.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getoutputtensorsizes","title":"<code>GetOutputTensorSizes()</code>","text":"<ul> <li>Signature: <code>std::vector&lt;uint64_t&gt; GetOutputTensorSizes()</code></li> <li>Description: Gets the individual sizes (in bytes) of each output tensor.</li> <li>Returns: A vector of output tensor sizes, in the order specified by <code>GetOutputTensorNames()</code>.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#gettaskorder","title":"<code>GetTaskOrder()</code>","text":"<ul> <li>Signature: <code>std::vector&lt;std::string&gt; GetTaskOrder()</code></li> <li>Description: Gets the model's task execution order.</li> <li>Returns: A vector of strings representing the task order.</li> <li>Note: The legacy function <code>task_order()</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#ismultiinputmodel","title":"<code>IsMultiInputModel()</code>","text":"<ul> <li>Signature: <code>bool IsMultiInputModel() const</code></li> <li>Description: Checks if the loaded model requires multiple input tensors.</li> <li>Returns: <code>true</code> if the model has multiple inputs, <code>false</code> otherwise.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#isortconfigured","title":"<code>IsOrtConfigured()</code>","text":"<ul> <li>Signature: <code>bool IsOrtConfigured()</code></li> <li>Description: Checks whether ONNX Runtime (ORT) is configured and available for use.</li> <li>Returns: <code>true</code> if ORT is configured, <code>false</code> otherwise.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#isppu","title":"<code>IsPPU()</code>","text":"<ul> <li>Signature: <code>bool IsPPU()</code></li> <li>Description: Checks if the loaded model utilizes a Post-Processing Unit (PPU).</li> <li>Returns: <code>true</code> if the model uses a PPU, <code>false</code> otherwise.</li> <li>Note: The legacy function <code>is_PPU()</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#registercallbackstdfunctioninttensorptrs>_outputs>_void>_userarg>_callbackfunc","title":"<code>RegisterCallback(std::function&lt;int(TensorPtrs&amp; outputs, void* userArg)&gt; callbackFunc)</code>","text":"<ul> <li>Signature: <code>void RegisterCallback(std::function&lt;int(TensorPtrs&amp; outputs, void* userArg)&gt; callbackFunc)</code></li> <li>Description: Registers a user-defined callback function that will be executed upon completion of an asynchronous inference request.</li> <li>Parameters:<ul> <li><code>callbackFunc</code>: The function to be called. It receives the output tensors and the user-provided argument.</li> </ul> </li> <li>Note: The legacy function <code>RegisterCallBack(...)</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#runvoid>_inputptr>_void>_userarg>_nullptr>_void>_outputptr>_nullptr","title":"<code>Run(void *inputPtr, void *userArg = nullptr, void *outputPtr = nullptr)</code>","text":"<ul> <li>Signature: <code>TensorPtrs Run(void *inputPtr, void *userArg = nullptr, void *outputPtr = nullptr)</code></li> <li>Description: Performs a synchronous inference for a single input, blocking until the operation is complete.</li> <li>Parameters:<ul> <li><code>inputPtr</code>: A pointer to the input data.</li> <li><code>userArg</code>: An optional user-defined argument.</li> <li><code>outputPtr</code>: An optional pointer to a pre-allocated output buffer.</li> </ul> </li> <li>Returns: A <code>TensorPtrs</code> object containing the output data.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#runconst>_stdvectorvoid>_inputbuffers>_const>_stdvectorvoid>_outputbuffers>_const>_stdvectorvoid>_userargs","title":"<code>Run(const std::vector&lt;void*&gt;&amp; inputBuffers, const std::vector&lt;void*&gt;&amp; outputBuffers, const std::vector&lt;void*&gt;&amp; userArgs = {})</code>","text":"<ul> <li>Signature: <code>std::vector&lt;TensorPtrs&gt; Run(const std::vector&lt;void*&gt;&amp; inputBuffers, const std::vector&lt;void*&gt;&amp; outputBuffers, const std::vector&lt;void*&gt;&amp; userArgs = {})</code></li> <li>Description: Performs a synchronous batch inference.</li> <li>Parameters:<ul> <li><code>inputBuffers</code>: A vector of pointers to input data for each sample in the batch.</li> <li><code>outputBuffers</code>: A vector of pointers to pre-allocated output buffers.</li> <li><code>userArgs</code>: An optional vector of user-defined arguments.</li> </ul> </li> <li>Returns: A vector of <code>TensorPtrs</code>, where each element corresponds to the output of one sample.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#runasyncvoid>_inputptr>_void>_userargnullptr>_void>_outputptr>_nullptr","title":"<code>RunAsync(void *inputPtr, void *userArg=nullptr, void *outputPtr = nullptr)</code>","text":"<ul> <li>Signature: <code>int RunAsync(void *inputPtr, void *userArg=nullptr, void *outputPtr = nullptr)</code></li> <li>Description: Submits a non-blocking, asynchronous inference request for a single input.</li> <li>Parameters:<ul> <li><code>inputPtr</code>: A pointer to the input data.</li> <li><code>userArg</code>: An optional user-defined argument to be passed to the callback.</li> <li><code>outputPtr</code>: An optional pointer to a pre-allocated output buffer.</li> </ul> </li> <li>Returns: An integer <code>jobId</code> for this asynchronous operation.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#runasyncconst>_stdvectorvoid>_inputptrs>_void>_userargnullptr>_void>_outputptr>_nullptr","title":"<code>RunAsync(const std::vector&lt;void*&gt;&amp; inputPtrs, void *userArg=nullptr, void *outputPtr = nullptr)</code>","text":"<ul> <li>Signature: <code>int RunAsync(const std::vector&lt;void*&gt;&amp; inputPtrs, void *userArg=nullptr, void *outputPtr = nullptr)</code></li> <li>Description: Submits an asynchronous inference request, automatically detecting if the input is for a multi-input model.</li> <li>Parameters:<ul> <li><code>inputPtrs</code>: A vector of pointers to input data.</li> <li><code>userArg</code>: An optional user-defined argument.</li> <li><code>outputPtr</code>: An optional pointer to a pre-allocated output buffer.</li> </ul> </li> <li>Returns: An integer <code>jobId</code>.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#runasyncmultiinputconst>_stdmapstdstring>_void>_inputtensors>_void>_userargnullptr>_void>_outputptr>_nullptr","title":"<code>RunAsyncMultiInput(const std::map&lt;std::string, void*&gt;&amp; inputTensors, void *userArg=nullptr, void *outputPtr = nullptr)</code>","text":"<ul> <li>Signature: <code>int RunAsyncMultiInput(const std::map&lt;std::string, void*&gt;&amp; inputTensors, void *userArg=nullptr, void *outputPtr = nullptr)</code></li> <li>Description: Submits an asynchronous inference request for a multi-input model using a map of named tensors.</li> <li>Parameters:<ul> <li><code>inputTensors</code>: A map of tensor names to input data pointers.</li> <li><code>userArg</code>: An optional user-defined argument.</li> <li><code>outputPtr</code>: An optional pointer to a pre-allocated output buffer.</li> </ul> </li> <li>Returns: An integer <code>jobId</code>.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#runasyncmultiinputconst>_stdvectorvoid>_inputptrs>_void>_userargnullptr>_void>_outputptr>_nullptr","title":"<code>RunAsyncMultiInput(const std::vector&lt;void*&gt;&amp; inputPtrs, void *userArg=nullptr, void *outputPtr = nullptr)</code>","text":"<ul> <li>Signature: <code>int RunAsyncMultiInput(const std::vector&lt;void*&gt;&amp; inputPtrs, void *userArg=nullptr, void *outputPtr = nullptr)</code></li> <li>Description: Submits an asynchronous inference request for a multi-input model using a vector of input pointers.</li> <li>Parameters:<ul> <li><code>inputPtrs</code>: A vector of input pointers in the order specified by <code>GetInputTensorNames()</code>.</li> <li><code>userArg</code>: An optional user-defined argument.</li> <li><code>outputPtr</code>: An optional pointer to a pre-allocated output buffer.</li> </ul> </li> <li>Returns: An integer <code>jobId</code>.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#runbenchmarkint>_num>_void>_inputptr>_nullptr","title":"<code>RunBenchmark(int num, void* inputPtr = nullptr)</code>","text":"<ul> <li>Signature: <code>float RunBenchmark(int num, void* inputPtr = nullptr)</code></li> <li>Description: Runs a performance benchmark for a specified number of loops.</li> <li>Parameters:<ul> <li><code>num</code>: The number of inference iterations to run.</li> <li><code>inputPtr</code>: An optional pointer to the input data to use for the benchmark.</li> </ul> </li> <li>Returns: The average frames per second (FPS) as a float.</li> <li>Note: The legacy function <code>RunBenchMark(...)</code> is deprecated.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#runmultiinputconst>_stdmapstdstring>_void>_inputtensors>_void>_userargnullptr>_void>_outputptrnullptr","title":"<code>RunMultiInput(const std::map&lt;std::string, void*&gt;&amp; inputTensors, void *userArg=nullptr, void *outputPtr=nullptr)</code>","text":"<ul> <li>Signature: <code>TensorPtrs RunMultiInput(const std::map&lt;std::string, void*&gt;&amp; inputTensors, void *userArg=nullptr, void *outputPtr=nullptr)</code></li> <li>Description: Runs synchronous inference for a multi-input model using a map of named tensors.</li> <li>Parameters:<ul> <li><code>inputTensors</code>: A map of tensor names to input data pointers.</li> <li><code>userArg</code>: An optional user-defined argument.</li> <li><code>outputPtr</code>: An optional pointer to a pre-allocated output buffer.</li> </ul> </li> <li>Returns: A <code>TensorPtrs</code> object containing the output.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#runmultiinputconst>_stdvectorvoid>_inputptrs>_void>_userargnullptr>_void>_outputptrnullptr","title":"<code>RunMultiInput(const std::vector&lt;void*&gt;&amp; inputPtrs, void *userArg=nullptr, void *outputPtr=nullptr)</code>","text":"<ul> <li>Signature: <code>TensorPtrs RunMultiInput(const std::vector&lt;void*&gt;&amp; inputPtrs, void *userArg=nullptr, void *outputPtr=nullptr)</code></li> <li>Description: Runs synchronous inference for a multi-input model using a vector of input pointers.</li> <li>Parameters:<ul> <li><code>inputPtrs</code>: A vector of input pointers in the order specified by <code>GetInputTensorNames()</code>.</li> <li><code>userArg</code>: An optional user-defined argument.</li> <li><code>outputPtr</code>: An optional pointer to a pre-allocated output buffer.</li> </ul> </li> <li>Returns: A <code>TensorPtrs</code> object containing the output.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#waitint>_jobid","title":"<code>Wait(int jobId)</code>","text":"<ul> <li>Signature: <code>TensorPtrs Wait(int jobId)</code></li> <li>Description: Blocks execution and waits until the asynchronous request identified by <code>jobId</code> is complete.</li> <li>Parameters:<ul> <li><code>jobId</code>: The job ID returned from a <code>RunAsync</code> call.</li> </ul> </li> <li>Returns: A <code>TensorPtrs</code> object containing the output from the completed job.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#class>_dxrtinferenceoption","title":"<code>class dxrt::InferenceOption</code>","text":"<p>This class specifies inference options applied to an <code>InferenceEngine</code>, allowing users to configure which devices and NPU cores are used.</p>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#nested>_enums","title":"Nested Enums","text":""},{"location":"docs/10_01_C%2B%2B_API_Reference.html#enum>_bound>_option","title":"<code>enum BOUND_OPTION</code>","text":"<ul> <li>Description: Defines how NPU cores are bound or utilized for inference.</li> <li>Members: <code>NPU_ALL</code>, <code>NPU_0</code>, <code>NPU_1</code>, <code>NPU_2</code>, <code>NPU_01</code>, <code>NPU_12</code>, <code>NPU_02</code>.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#public>_members","title":"Public Members","text":"<ul> <li><code>boundOption</code>: <code>uint32_t</code>. Selects the NPU core(s) to use within a device, using a value from the <code>BOUND_OPTION</code> enum. Default is <code>NPU_ALL</code>.</li> <li><code>devices</code>: <code>std::vector&lt;int&gt;</code>. A list of device IDs to be used for inference. If the list is empty (default), all available devices are used.</li> <li><code>useORT</code>: <code>bool</code>. If <code>true</code>, both NPU and CPU (via ONNX Runtime) tasks will be executed. If <code>false</code>, only NPU tasks will run.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#class>_dxrtconfiguration","title":"<code>class dxrt::Configuration</code>","text":"<p>A singleton class for managing global application configurations. Access is thread-safe and should be done via the <code>GetInstance()</code> method.</p>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#nested>_enums_1","title":"Nested Enums","text":""},{"location":"docs/10_01_C%2B%2B_API_Reference.html#enum>_class>_item","title":"<code>enum class ITEM</code>","text":"<ul> <li>Description: Defines configuration categories.</li> <li>Members: <code>DEBUG</code>, <code>PROFILER</code>, <code>SERVICE</code>, <code>DYNAMIC_CPU_THREAD</code>, <code>TASK_FLOW</code>, <code>SHOW_THROTTLING</code>, <code>SHOW_PROFILE</code>, <code>SHOW_MODEL_INFO</code>.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#enum>_class>_attribute","title":"<code>enum class ATTRIBUTE</code>","text":"<ul> <li>Description: Defines attributes for configuration items.</li> <li>Members: <code>PROFILER_SHOW_DATA</code>, <code>PROFILER_SAVE_DATA</code>.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#static>_member>_functions","title":"Static Member Functions","text":""},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getinstance","title":"<code>GetInstance()</code>","text":"<ul> <li>Signature: <code>static Configuration&amp; GetInstance()</code></li> <li>Description: Returns the unique static instance of the <code>Configuration</code> class. This is the only way to access the configuration object.</li> <li>Returns: A reference to the <code>Configuration</code> instance.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#member>_functions_1","title":"Member Functions","text":""},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getattributeconst>_item>_item>_const>_attribute>_attrib","title":"<code>GetAttribute(const ITEM item, const ATTRIBUTE attrib)</code>","text":"<ul> <li>Signature: <code>std::string GetAttribute(const ITEM item, const ATTRIBUTE attrib)</code></li> <li>Description: Retrieves the value of a specific attribute for a given configuration item.</li> <li>Parameters:<ul> <li><code>item</code>: The configuration item.</li> <li><code>attrib</code>: The attribute to retrieve.</li> </ul> </li> <li>Returns: The attribute value as a <code>std::string</code>.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getdriverversion","title":"<code>GetDriverVersion()</code>","text":"<ul> <li>Signature: <code>std::string GetDriverVersion() const</code></li> <li>Description: Retrieves the version of the associated device driver.</li> <li>Returns: The driver version string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getenableconst>_item>_item","title":"<code>GetEnable(const ITEM item)</code>","text":"<ul> <li>Signature: <code>bool GetEnable(const ITEM item)</code></li> <li>Description: Retrieves the enabled status of a specific configuration item.</li> <li>Parameters:<ul> <li><code>item</code>: The configuration item to check.</li> </ul> </li> <li>Returns: <code>true</code> if the item is enabled, <code>false</code> otherwise.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getfirmwareversions","title":"<code>GetFirmwareVersions()</code>","text":"<ul> <li>Signature: <code>std::vector&lt;std::pair&lt;int, std::string&gt;&gt; GetFirmwareVersions() const</code></li> <li>Description: Retrieves the firmware versions of all detected devices.</li> <li>Returns: A vector of pairs, where each pair contains a device ID and its firmware version string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getonnxruntimeversion","title":"<code>GetONNXRuntimeVersion()</code>","text":"<ul> <li>Signature: <code>std::string GetONNXRuntimeVersion() const</code></li> <li>Description: Retrieves the version of the ONNX Runtime library being used.</li> <li>Returns: The ONNX Runtime version string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getpciedriverversion","title":"<code>GetPCIeDriverVersion()</code>","text":"<ul> <li>Signature: <code>std::string GetPCIeDriverVersion() const</code></li> <li>Description: Retrieves the version of the PCIe driver.</li> <li>Returns: The PCIe driver version string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getversion","title":"<code>GetVersion()</code>","text":"<ul> <li>Signature: <code>std::string GetVersion() const</code></li> <li>Description: Retrieves the version of the DXRT library.</li> <li>Returns: The library version string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#loadconfigfileconst>_stdstring>_filename","title":"<code>LoadConfigFile(const std::string&amp; fileName)</code>","text":"<ul> <li>Signature: <code>void LoadConfigFile(const std::string&amp; fileName)</code></li> <li>Description: Loads configuration settings from the specified file.</li> <li>Parameters:<ul> <li><code>fileName</code>: The path and name of the configuration file.</li> </ul> </li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#lockenableconst>_item>_item","title":"<code>LockEnable(const ITEM item)</code>","text":"<ul> <li>Signature: <code>void LockEnable(const ITEM item)</code></li> <li>Description: Locks a specific configuration item, making it read-only.</li> <li>Parameters:<ul> <li><code>item</code>: The configuration item to lock.</li> </ul> </li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#setattributeconst>_item>_item>_const>_attribute>_attrib>_const>_stdstring>_value","title":"<code>SetAttribute(const ITEM item, const ATTRIBUTE attrib, const std::string&amp; value)</code>","text":"<ul> <li>Signature: <code>void SetAttribute(const ITEM item, const ATTRIBUTE attrib, const std::string&amp; value)</code></li> <li>Description: Sets a specific attribute value for a given configuration item (e.g., setting <code>PROFILER_SAVE_DATA</code> to <code>\"ON\"</code>).</li> <li>Parameters:<ul> <li><code>item</code>: The configuration item.</li> <li><code>attrib</code>: The attribute to set.</li> <li><code>value</code>: The attribute value as a string.</li> </ul> </li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#setenableconst>_item>_item>_bool>_enabled","title":"<code>SetEnable(const ITEM item, bool enabled)</code>","text":"<ul> <li>Signature: <code>void SetEnable(const ITEM item, bool enabled)</code></li> <li>Description: Sets the enabled status for a specific configuration item (e.g., enables the profiler).</li> <li>Parameters:<ul> <li><code>item</code>: The configuration item.</li> <li><code>enabled</code>: <code>true</code> to enable, <code>false</code> to disable.</li> </ul> </li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#class>_dxrtdevicestatus","title":"<code>class dxrt::DeviceStatus</code>","text":"<p>Provides an abstraction for retrieving device information and real-time status.</p>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#static>_member>_functions_1","title":"Static Member Functions","text":""},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getcurrentstatusint>_id","title":"<code>GetCurrentStatus(int id)</code>","text":"<ul> <li>Signature: <code>static DeviceStatus GetCurrentStatus(int id)</code></li> <li>Description: Retrieves the real-time status for the device with the specified ID.</li> <li>Parameters:<ul> <li><code>id</code>: The unique identifier of the device.</li> </ul> </li> <li>Returns: A <code>DeviceStatus</code> object containing the device's current status.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getdevicecount","title":"<code>GetDeviceCount()</code>","text":"<ul> <li>Signature: <code>static int GetDeviceCount()</code></li> <li>Description: Retrieves the total number of hardware devices currently recognized by the system.</li> <li>Returns: The total number of available devices.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#member>_functions_2","title":"Member Functions","text":""},{"location":"docs/10_01_C%2B%2B_API_Reference.html#allmemoryinfostr","title":"<code>AllMemoryInfoStr()</code>","text":"<ul> <li>Signature: <code>std::string AllMemoryInfoStr() const</code></li> <li>Description: Retrieves a summary of the device's memory specifications (type, frequency, size) in a single line.</li> <li>Returns: A formatted string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#boardtypestr","title":"<code>BoardTypeStr()</code>","text":"<ul> <li>Signature: <code>std::string BoardTypeStr() const</code></li> <li>Description: Returns the device board type.</li> <li>Returns: A string such as \"SOM\" or \"M.2\".</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#ddrbiterrstr","title":"<code>DdrBitErrStr()</code>","text":"<ul> <li>Signature: <code>std::string DdrBitErrStr() const</code></li> <li>Description: Retrieves the count of LPDDR Double-bit &amp; Single-bit Errors.</li> <li>Returns: A formatted string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#ddrstatusstrint>_ch","title":"<code>DdrStatusStr(int ch)</code>","text":"<ul> <li>Signature: <code>std::string DdrStatusStr(int ch) const</code></li> <li>Description: Retrieves the status of a specified LPDDR memory channel.</li> <li>Parameters:<ul> <li><code>ch</code>: The LPDDR memory channel index (0 to 3).</li> </ul> </li> <li>Returns: A formatted string containing the channel status.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#devicetypestr","title":"<code>DeviceTypeStr()</code>","text":"<ul> <li>Signature: <code>std::string DeviceTypeStr() const</code></li> <li>Description: Retrieves the device type as a three-letter abbreviation.</li> <li>Returns: A string (\"ACC\" for Accelerator or \"STD\" for Standalone).</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#devicetypeword","title":"<code>DeviceTypeWord()</code>","text":"<ul> <li>Signature: <code>std::string DeviceTypeWord() const</code></li> <li>Description: Retrieves the full name of the device type.</li> <li>Returns: A string (\"Accelerator\" or \"Standalone\").</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#devicevariantstr","title":"<code>DeviceVariantStr()</code>","text":"<ul> <li>Signature: <code>std::string DeviceVariantStr() const</code></li> <li>Description: Returns the device chip variant type.</li> <li>Returns: A string such as \"L1\" or \"M1\".</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#dmachannel","title":"<code>DmaChannel()</code>","text":"<ul> <li>Signature: <code>uint64_t DmaChannel() const</code></li> <li>Description: Retrieves the number of DMA (Direct Memory Access) channels available for the NPU.</li> <li>Returns: The number of DMA channels.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#dvfsstateinfostr","title":"<code>DvfsStateInfoStr()</code>","text":"<ul> <li>Signature: <code>std::string DvfsStateInfoStr() const</code></li> <li>Description: Retrieves the current Dynamic Voltage and Frequency Scaling (DVFS) state of the device.</li> <li>Returns: A formatted string indicating the DVFS state.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#firmwareversionstr","title":"<code>FirmwareVersionStr()</code>","text":"<ul> <li>Signature: <code>std::string FirmwareVersionStr() const</code></li> <li>-Description: Retrieves the firmware version of the NPU.</li> <li>Returns: The version string (e.g., \"1.2.3\").</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getdevicetype","title":"<code>GetDeviceType()</code>","text":"<ul> <li>Signature: <code>DeviceType GetDeviceType() const</code></li> <li>Description: Retrieves the device type as a <code>DeviceType</code> enum.</li> <li>Returns: A <code>DeviceType</code> enum value.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getid","title":"<code>GetId()</code>","text":"<ul> <li>Signature: <code>int GetId() const</code></li> <li>Description: Retrieves the unique identifier of the device.</li> <li>Returns: The device ID as an integer.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getinfostring","title":"<code>GetInfoString()</code>","text":"<ul> <li>Signature: <code>std::string GetInfoString() const</code></li> <li>Description: Retrieves detailed static information about the device, equivalent to <code>dxrt-cli -i</code>.</li> <li>Returns: A formatted string with device specifications.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getnpuclockint>_ch","title":"<code>GetNpuClock(int ch)</code>","text":"<ul> <li>Signature: <code>uint32_t GetNpuClock(int ch) const</code></li> <li>Description: Retrieves the current clock frequency of the specified NPU channel.</li> <li>Parameters:<ul> <li><code>ch</code>: The NPU channel index.</li> </ul> </li> <li>Returns: The clock frequency in megahertz (MHz).</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getnpuvoltageint>_ch","title":"<code>GetNpuVoltage(int ch)</code>","text":"<ul> <li>Signature: <code>uint32_t GetNpuVoltage(int ch) const</code></li> <li>Description: Retrieves the voltage level of the specified NPU channel.</li> <li>Parameters:<ul> <li><code>ch</code>: The NPU channel index.</li> </ul> </li> <li>Returns: The voltage level in millivolts (mV).</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#getstatusstring","title":"<code>GetStatusString()</code>","text":"<ul> <li>Signature: <code>std::string GetStatusString() const</code></li> <li>Description: Retrieves the real-time status of the device, equivalent to <code>dxrt-cli -s</code>.</li> <li>Returns: A formatted string with real-time status.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#gettemperatureint>_ch","title":"<code>GetTemperature(int ch)</code>","text":"<ul> <li>Signature: <code>int GetTemperature(int ch) const</code></li> <li>Description: Retrieves the temperature of the specified NPU channel.</li> <li>Parameters:<ul> <li><code>ch</code>: The NPU channel index.</li> </ul> </li> <li>Returns: The temperature in degrees Celsius.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#memoryclock","title":"<code>MemoryClock()</code>","text":"<ul> <li>Signature: <code>uint64_t MemoryClock() const</code></li> <li>Description: Retrieves the memory clock frequency of the NPU.</li> <li>Returns: The frequency in megahertz (MHz).</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#memoryfrequency","title":"<code>MemoryFrequency()</code>","text":"<ul> <li>Signature: <code>int MemoryFrequency() const</code></li> <li>Description: Retrieves the memory operating frequency of the device.</li> <li>Returns: The frequency in megahertz (MHz).</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#memorysize","title":"<code>MemorySize()</code>","text":"<ul> <li>Signature: <code>int64_t MemorySize() const</code></li> <li>Description: Retrieves the total memory size available for the NPU.</li> <li>Returns: The total memory size in bytes.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#memorysizestrbinaryprefix","title":"<code>MemorySizeStrBinaryPrefix()</code>","text":"<ul> <li>Signature: <code>std::string MemorySizeStrBinaryPrefix() const</code></li> <li>Description: Retrieves the total memory size as a string using binary units (e.g., \"1.98 GiB\").</li> <li>Returns: A formatted string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#memorysizestrwithcomma","title":"<code>MemorySizeStrWithComma()</code>","text":"<ul> <li>Signature: <code>std::string MemorySizeStrWithComma() const</code></li> <li>Description: Retrieves the total memory size as a string in bytes, formatted with commas.</li> <li>Returns: A formatted string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#memorytypestr","title":"<code>MemoryTypeStr()</code>","text":"<ul> <li>Signature: <code>std::string MemoryTypeStr() const</code></li> <li>Description: Retrieves the type of memory used in the device.</li> <li>Returns: A string (e.g., \"LPDDR4\").</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#npustatusstrint>_ch","title":"<code>NpuStatusStr(int ch)</code>","text":"<ul> <li>Signature: <code>std::string NpuStatusStr(int ch) const</code></li> <li>Description: Retrieves the status of a specific NPU as a formatted string (voltage, clock, temperature).</li> <li>Parameters:<ul> <li><code>ch</code>: The NPU index.</li> </ul> </li> <li>Returns: A formatted string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#pcieinfostrint>_spd>_int>_wd>_int>_bus>_int>_dev>_int>_func","title":"<code>PcieInfoStr(int spd, int wd, int bus, int dev, int func)</code>","text":"<ul> <li>Signature: <code>std::string PcieInfoStr(int spd, int wd, int bus, int dev, int func) const</code></li> <li>Description: Returns PCIe information (speed, generation, etc.) as a string.</li> <li>Parameters:<ul> <li><code>spd</code>, <code>wd</code>, <code>bus</code>, <code>dev</code>, <code>func</code>: PCIe configuration parameters.</li> </ul> </li> <li>Returns: A formatted string with PCIe information.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#class>_dxrttensor","title":"<code>class dxrt::Tensor</code>","text":"<p>This class abstracts a DXRT tensor object, which defines a data array composed of uniform elements.</p>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#constructor_1","title":"Constructor","text":""},{"location":"docs/10_01_C%2B%2B_API_Reference.html#tensorstdstring>_name>_stdvectorint64>_t>_shape>_datatype>_type>_void>_data>_nullptr","title":"<code>Tensor(std::string name_, std::vector&lt;int64_t&gt; shape_, DataType type_, void *data_=nullptr)</code>","text":"<ul> <li>Description: Constructs a Tensor object.</li> <li>Parameters:<ul> <li><code>name_</code>: The name of the tensor.</li> <li><code>shape_</code>: A vector defining the tensor's shape (dimensions).</li> <li><code>type_</code>: The tensor's data type.</li> <li><code>data_</code>: An optional pointer to the tensor's data.</li> </ul> </li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#member>_functions_3","title":"Member Functions","text":""},{"location":"docs/10_01_C%2B%2B_API_Reference.html#data","title":"<code>data()</code>","text":"<ul> <li>Signature: <code>void* &amp;data()</code></li> <li>Description: Accessor for the tensor's data pointer.</li> <li>Returns: A reference to the void pointer holding the tensor's data.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#dataint>_height>_int>_width>_int>_channel","title":"<code>data(int height, int width, int channel)</code>","text":"<ul> <li>Signature: <code>void* data(int height, int width, int channel)</code></li> <li>Description: Gets a pointer to a specific element by its index, assuming NHWC data layout.</li> <li>Parameters:<ul> <li><code>height</code>: The height index.</li> <li><code>width</code>: The width index.</li> <li><code>channel</code>: The channel index.</li> </ul> </li> <li>Returns: A void pointer to the specified element.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#elem>_size","title":"<code>elem_size()</code>","text":"<ul> <li>Signature: <code>uint32_t &amp;elem_size()</code></li> <li>Description: Accessor for the size of a single element in the tensor.</li> <li>Returns: A reference to the element size in bytes.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#name","title":"<code>name()</code>","text":"<ul> <li>Signature: <code>const std::string &amp;name() const</code></li> <li>Description: Accessor for the tensor's name.</li> <li>Returns: A constant reference to the tensor's name string.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#phy>_addr","title":"<code>phy_addr()</code>","text":"<ul> <li>Signature: <code>uint64_t &amp;phy_addr()</code></li> <li>Description: Accessor for the physical address of the tensor's data.</li> <li>Returns: A reference to the physical address.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#shape","title":"<code>shape()</code>","text":"<ul> <li>Signature: <code>std::vector&lt;int64_t&gt; &amp;shape()</code></li> <li>Description: Accessor for the tensor's shape.</li> <li>Returns: A reference to the vector defining the tensor's dimensions.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#size>_in>_bytes","title":"<code>size_in_bytes()</code>","text":"<ul> <li>Signature: <code>uint64_t size_in_bytes() const</code></li> <li>Description: Calculates and returns the total size of the tensor's data in bytes based on its shape and element size.</li> <li>Returns: The total size in bytes.</li> </ul>"},{"location":"docs/10_01_C%2B%2B_API_Reference.html#type","title":"<code>type()</code>","text":"<ul> <li>Signature: <code>DataType &amp;type()</code></li> <li>Description: Accessor for the tensor's data type.</li> <li>Returns: A reference to the <code>DataType</code> enum.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html","title":"Python API Reference","text":""},{"location":"docs/10_02_Python_API_Reference.html#class>_dx>_engineinferenceengine","title":"<code>class dx_engine.InferenceEngine</code>","text":"<p>This class is the main Python wrapper for the DXRT Inference Engine. It provides an interface to load a compiled model and perform inference tasks, either synchronously or asynchronously, supporting both single and batch inference.</p>"},{"location":"docs/10_02_Python_API_Reference.html#constructor","title":"Constructor","text":""},{"location":"docs/10_02_Python_API_Reference.html#>_init>_self>_model>_path>_str>_inference>_option>_optionalinferenceoption>_none","title":"<code>__init__(self, model_path: str, inference_option: Optional[InferenceOption] = None)</code>","text":"<ul> <li>Description: Initializes the InferenceEngine by loading a compiled model from the specified path.</li> <li>Parameters:<ul> <li><code>model_path</code>: <code>str</code>. Path to the compiled model file (e.g., <code>*.dxnn</code>).</li> <li><code>inference_option</code>: <code>Optional[InferenceOption]</code>. An <code>InferenceOption</code> object for configuration. If <code>None</code>, default options are used.</li> </ul> </li> <li>Raises: <code>RuntimeError</code> if the underlying C++ engine fails to initialize.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#member>_functions","title":"Member Functions","text":""},{"location":"docs/10_02_Python_API_Reference.html#disposeself","title":"<code>dispose(self)</code>","text":"<ul> <li>Signature: <code>def dispose(self) -&gt; None</code></li> <li>Description: Explicitly releases the underlying C++ resources held by the inference engine. This is automatically called when using a <code>with</code> statement, so manual invocation is typically not required.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_all>_task>_outputsself","title":"<code>get_all_task_outputs(self)</code>","text":"<ul> <li>Signature: <code>def get_all_task_outputs(self) -&gt; List[List[np.ndarray]]</code></li> <li>Description: Retrieves the outputs of all internal tasks in their execution order. This is useful for debugging the intermediate steps of a model.</li> <li>Returns: A list of lists, where each inner list contains the output <code>np.ndarray</code> objects for a single task.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_bitmatch>_maskself>_index>_int>_0","title":"<code>get_bitmatch_mask(self, index: int = 0)</code>","text":"<ul> <li>Signature: <code>def get_bitmatch_mask(self, index: int = 0) -&gt; np.ndarray</code></li> <li>Description: Retrieves a bitmatch mask for a specific NPU task, which can be used for validation and debugging purposes.</li> <li>Parameters:<ul> <li><code>index</code>: <code>int</code>. The index of the NPU task.</li> </ul> </li> <li>Returns: A boolean <code>np.ndarray</code> representing the bitmatch mask.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_compile>_typeself","title":"<code>get_compile_type(self)</code>","text":"<ul> <li>Signature: <code>def get_compile_type(self) -&gt; str</code></li> <li>Description: Returns the compilation type or strategy of the loaded model (e.g., \"debug\", \"release\").</li> <li>Returns: The compilation type as a string.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_input>_sizeself","title":"<code>get_input_size(self)</code>","text":"<ul> <li>Signature: <code>def get_input_size(self) -&gt; int</code></li> <li>Description: Gets the total expected size of all input tensors combined in bytes.</li> <li>Returns: The total input size as an integer.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_input>_tensor>_countself","title":"<code>get_input_tensor_count(self)</code>","text":"<ul> <li>Signature: <code>def get_input_tensor_count(self) -&gt; int</code></li> <li>Description: Returns the number of input tensors required by the model.</li> <li>Returns: The number of input tensors.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_input>_tensor>_namesself","title":"<code>get_input_tensor_names(self)</code>","text":"<ul> <li>Signature: <code>def get_input_tensor_names(self) -&gt; List[str]</code></li> <li>Description: Returns the names of all input tensors in the order they should be provided.</li> <li>Returns: A list of input tensor names.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_input>_tensor>_sizesself","title":"<code>get_input_tensor_sizes(self)</code>","text":"<ul> <li>Signature: <code>def get_input_tensor_sizes(self) -&gt; List[int]</code></li> <li>Description: Gets the individual sizes of each input tensor in bytes, in their correct order.</li> <li>Returns: A list of integer sizes.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_input>_tensor>_to>_task>_mappingself","title":"<code>get_input_tensor_to_task_mapping(self)</code>","text":"<ul> <li>Signature: <code>def get_input_tensor_to_task_mapping(self) -&gt; Dict[str, str]</code></li> <li>Description: Returns the mapping from input tensor names to their target tasks within the model graph.</li> <li>Returns: A dictionary mapping tensor names to task names.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_input>_tensors>_infoself","title":"<code>get_input_tensors_info(self)</code>","text":"<ul> <li>Signature: <code>def get_input_tensors_info(self) -&gt; List[Dict[str, Any]]</code></li> <li>Description: Returns detailed information for each input tensor.</li> <li>Returns: A list of dictionaries, where each dictionary contains keys: <code>'name'</code> (str), <code>'shape'</code> (List[int]), <code>'dtype'</code> (np.dtype), and <code>'elem_size'</code> (int).</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_latencyself","title":"<code>get_latency(self)</code>","text":"<ul> <li>Signature: <code>def get_latency(self) -&gt; int</code></li> <li>Description: Returns the latency of the most recent inference in microseconds.</li> <li>Returns: The latency value as an integer.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_latency>_countself","title":"<code>get_latency_count(self)</code>","text":"<ul> <li>Signature: <code>def get_latency_count(self) -&gt; int</code></li> <li>Description: Returns the total count of latency values collected since initialization.</li> <li>Returns: The number of measurements.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_latency>_listself","title":"<code>get_latency_list(self)</code>","text":"<ul> <li>Signature: <code>def get_latency_list(self) -&gt; List[int]</code></li> <li>Description: Returns a list of recent latency measurements in microseconds.</li> <li>Returns: A list of integers.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_latency>_meanself","title":"<code>get_latency_mean(self)</code>","text":"<ul> <li>Signature: <code>def get_latency_mean(self) -&gt; float</code></li> <li>Description: Returns the mean (average) of all collected latency values.</li> <li>Returns: The mean latency as a float.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_latency>_stdself","title":"<code>get_latency_std(self)</code>","text":"<ul> <li>Signature: <code>def get_latency_std(self) -&gt; float</code></li> <li>Description: Returns the standard deviation of all collected latency values.</li> <li>Returns: The standard deviation as a float.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_model>_versionself","title":"<code>get_model_version(self)</code>","text":"<ul> <li>Signature: <code>def get_model_version(self) -&gt; str</code></li> <li>Description: Returns the DXNN file format version of the loaded model.</li> <li>Returns: The model version string.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_npu>_inference>_timeself","title":"<code>get_npu_inference_time(self)</code>","text":"<ul> <li>Signature: <code>def get_npu_inference_time(self) -&gt; int</code></li> <li>Description: Returns the pure NPU processing time for the most recent inference in microseconds.</li> <li>Returns: The NPU inference time as an integer.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_npu>_inference>_time>_countself","title":"<code>get_npu_inference_time_count(self)</code>","text":"<ul> <li>Signature: <code>def get_npu_inference_time_count(self) -&gt; int</code></li> <li>Description: Returns the total count of NPU inference time values collected.</li> <li>Returns: The number of measurements.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_npu>_inference>_time>_listself","title":"<code>get_npu_inference_time_list(self)</code>","text":"<ul> <li>Signature: <code>def get_npu_inference_time_list(self) -&gt; List[int]</code></li> <li>Description: Returns a list of recent NPU inference time measurements in microseconds.</li> <li>Returns: A list of integers.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_npu>_inference>_time>_meanself","title":"<code>get_npu_inference_time_mean(self)</code>","text":"<ul> <li>Signature: <code>def get_npu_inference_time_mean(self) -&gt; float</code></li> <li>Description: Returns the mean (average) of all collected NPU inference times.</li> <li>Returns: The mean time as a float.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_npu>_inference>_time>_stdself","title":"<code>get_npu_inference_time_std(self)</code>","text":"<ul> <li>Signature: <code>def get_npu_inference_time_std(self) -&gt; float</code></li> <li>Description: Returns the standard deviation of all collected NPU inference times.</li> <li>Returns: The standard deviation as a float.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_num>_tail>_tasksself","title":"<code>get_num_tail_tasks(self)</code>","text":"<ul> <li>Signature: <code>def get_num_tail_tasks(self) -&gt; int</code></li> <li>Description: Returns the number of 'tail' tasks (tasks with no successors) in the model graph.</li> <li>Returns: The number of tail tasks.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_output>_sizeself","title":"<code>get_output_size(self)</code>","text":"<ul> <li>Signature: <code>def get_output_size(self) -&gt; int</code></li> <li>Description: Gets the total size of all output tensors combined in bytes.</li> <li>Returns: The total output size as an integer.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_output>_tensor>_countself","title":"<code>get_output_tensor_count(self)</code>","text":"<ul> <li>Signature: <code>def get_output_tensor_count(self) -&gt; int</code></li> <li>Description: Returns the number of output tensors produced by the model.</li> <li>Returns: The number of output tensors.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_output>_tensor>_namesself","title":"<code>get_output_tensor_names(self)</code>","text":"<ul> <li>Signature: <code>def get_output_tensor_names(self) -&gt; List[str]</code></li> <li>Description: Returns the names of all output tensors in the order they are produced.</li> <li>Returns: A list of output tensor names.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_output>_tensor>_sizesself","title":"<code>get_output_tensor_sizes(self)</code>","text":"<ul> <li>Signature: <code>def get_output_tensor_sizes(self) -&gt; List[int]</code></li> <li>Description: Gets the individual sizes of each output tensor in bytes, in their correct order.</li> <li>Returns: A list of integer sizes.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_output>_tensors>_infoself","title":"<code>get_output_tensors_info(self)</code>","text":"<ul> <li>Signature: <code>def get_output_tensors_info(self) -&gt; List[Dict[str, Any]]</code></li> <li>Description: Returns detailed information for each output tensor.</li> <li>Returns: A list of dictionaries with keys: <code>'name'</code> (str), <code>'shape'</code> (List[int]), <code>'dtype'</code> (np.dtype), and <code>'elem_size'</code> (int).</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_task>_orderself","title":"<code>get_task_order(self)</code>","text":"<ul> <li>Signature: <code>def get_task_order(self) -&gt; np.ndarray</code></li> <li>Description: Returns the execution order of tasks/subgraphs within the model.</li> <li>Returns: A numpy array of strings representing the task order.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#is>_multi>_input>_modelself","title":"<code>is_multi_input_model(self)</code>","text":"<ul> <li>Signature: <code>def is_multi_input_model(self) -&gt; bool</code></li> <li>Description: Checks if the loaded model requires multiple input tensors.</li> <li>Returns: <code>True</code> if the model has multiple inputs, <code>False</code> otherwise.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#is>_ppuself","title":"<code>is_ppu(self)</code>","text":"<ul> <li>Signature: <code>def is_ppu(self) -&gt; bool</code></li> <li>Description: Checks if the loaded model utilizes a Post-Processing Unit (PPU).</li> <li>Returns: <code>True</code> if the model uses a PPU, <code>False</code> otherwise.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#register>_callbackself>_callback>_optionalcallablelistnpndarray>_any>_int","title":"<code>register_callback(self, callback: Optional[Callable[[List[np.ndarray], Any], int]])</code>","text":"<ul> <li>Signature: <code>def register_callback(self, callback: Optional[Callable[[List[np.ndarray], Any], int]]) -&gt; None</code></li> <li>Description: Registers a user-defined callback function to be executed upon completion of an asynchronous inference.</li> <li>Parameters:<ul> <li><code>callback</code>: A callable function or <code>None</code> to unregister. The callback receives the list of output arrays and the user argument.</li> </ul> </li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#runself>_input>_data>_unionnpndarray>_listnpndarray>_listlistnpndarray>_output>_buffers>_optionalunionlistnpndarray>_listlistnpndarray>_none>_user>_args>_optionalunionany>_listany>_none","title":"<code>run(self, input_data: Union[np.ndarray, List[np.ndarray], List[List[np.ndarray]]], output_buffers: Optional[Union[List[np.ndarray], List[List[np.ndarray]]]] = None, user_args: Optional[Union[Any, List[Any]]] = None)</code>","text":"<ul> <li>Signature: <code>def run(self, input_data, output_buffers=None, user_args=None) -&gt; Union[List[np.ndarray], List[List[np.ndarray]]]</code></li> <li>Description: Runs inference synchronously. This versatile method handles single-item, multi-input, and batch inference based on the format of <code>input_data</code>.</li> <li>Parameters:<ul> <li><code>input_data</code>: Input data in various formats (<code>np.ndarray</code>, <code>List[np.ndarray]</code>, <code>List[List[np.ndarray]]</code>).</li> <li><code>output_buffers</code>: Optional pre-allocated buffers for the output.</li> <li><code>user_args</code>: Optional user-defined argument or list of arguments for batch mode.</li> </ul> </li> <li>Returns: The inference result(s). A <code>List[np.ndarray]</code> for single inference or a <code>List[List[np.ndarray]]</code> for batch inference.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#run>_asyncself>_input>_data>_unionnpndarray>_listnpndarray>_user>_arg>_any>_none>_output>_buffer>_optionalunionnpndarray>_listnpndarray>_none","title":"<code>run_async(self, input_data: Union[np.ndarray, List[np.ndarray]], user_arg: Any = None, output_buffer: Optional[Union[np.ndarray, List[np.ndarray]]] = None)</code>","text":"<ul> <li>Signature: <code>def run_async(self, input_data, user_arg=None, output_buffer=None) -&gt; int</code></li> <li>Description: Runs inference asynchronously for a single item. Batch processing is not supported with this method.</li> <li>Parameters:<ul> <li><code>input_data</code>: A single <code>np.ndarray</code> or a <code>List[np.ndarray]</code> for multi-input models.</li> <li><code>user_arg</code>: An optional user-defined argument to be passed to the callback.</li> <li><code>output_buffer</code>: An optional pre-allocated buffer for the output.</li> </ul> </li> <li>Returns: An integer <code>job_id</code> for this asynchronous operation.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#run>_async>_multi>_inputself>_input>_tensors>_dictstr>_npndarray>_user>_arg>_any>_none>_output>_buffer>_optionallistnpndarray>_none","title":"<code>run_async_multi_input(self, input_tensors: Dict[str, np.ndarray], user_arg: Any = None, output_buffer: Optional[List[np.ndarray]] = None)</code>","text":"<ul> <li>Signature: <code>def run_async_multi_input(self, input_tensors, user_arg=None, output_buffer=None) -&gt; int</code></li> <li>Description: A convenience method to run asynchronous inference on a multi-input model using a dictionary of named tensors.</li> <li>Parameters:<ul> <li><code>input_tensors</code>: A dictionary mapping input tensor names to <code>np.ndarray</code> data.</li> <li><code>user_arg</code>: An optional user-defined argument.</li> <li><code>output_buffer</code>: An optional list of pre-allocated output arrays.</li> </ul> </li> <li>Returns: An integer <code>job_id</code>.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#run>_benchmarkself>_num>_loops>_int>_input>_data>_optionallistnpndarray>_none","title":"<code>run_benchmark(self, num_loops: int, input_data: Optional[List[np.ndarray]] = None)</code>","text":"<ul> <li>Signature: <code>def run_benchmark(self, num_loops: int, input_data: Optional[List[np.ndarray]] = None) -&gt; float</code></li> <li>Description: Runs a performance benchmark for a specified number of loops.</li> <li>Parameters:<ul> <li><code>num_loops</code>: The number of inference iterations to run.</li> <li><code>input_data</code>: An optional list of <code>np.ndarray</code> to use as input for the benchmark.</li> </ul> </li> <li>Returns: The average frames per second (FPS) as a float.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#run>_multi>_inputself>_input>_tensors>_dictstr>_npndarray>_output>_buffers>_optionallistnpndarray>_none>_user>_arg>_any>_none","title":"<code>run_multi_input(self, input_tensors: Dict[str, np.ndarray], output_buffers: Optional[List[np.ndarray]] = None, user_arg: Any = None)</code>","text":"<ul> <li>Signature: <code>def run_multi_input(self, input_tensors, output_buffers=None, user_arg=None) -&gt; List[np.ndarray]</code></li> <li>Description: A convenience method to run synchronous inference on a multi-input model using a dictionary of named tensors.</li> <li>Parameters:<ul> <li><code>input_tensors</code>: A dictionary mapping input tensor names to <code>np.ndarray</code> data.</li> <li><code>output_buffers</code>: An optional list of pre-allocated output arrays.</li> <li><code>user_arg</code>: An optional user-defined argument.</li> </ul> </li> <li>Returns: A list of <code>np.ndarray</code> objects containing the output.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#waitself>_job>_id>_int","title":"<code>wait(self, job_id: int)</code>","text":"<ul> <li>Signature: <code>def wait(self, job_id: int) -&gt; List[np.ndarray]</code></li> <li>Description: Waits for an asynchronous job (identified by <code>job_id</code>) to complete and retrieves its output.</li> <li>Parameters:<ul> <li><code>job_id</code>: The integer job ID returned from a <code>run_async</code> call.</li> </ul> </li> <li>Returns: A list of <code>np.ndarray</code> objects containing the output from the completed job.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#class>_dx>_engineinferenceoption","title":"<code>class dx_engine.InferenceOption</code>","text":"<p>This class provides a Pythonic interface to configure inference options such as device selection and core binding. It wraps the C++ <code>InferenceOption</code> struct.</p>"},{"location":"docs/10_02_Python_API_Reference.html#constructor_1","title":"Constructor","text":""},{"location":"docs/10_02_Python_API_Reference.html#>_init>_self","title":"<code>__init__(self)</code>","text":"<ul> <li>Signature: <code>def __init__(self) -&gt; None</code></li> <li>Description: Initializes a new <code>InferenceOption</code> object with default values from the C++ backend.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#properties","title":"Properties","text":""},{"location":"docs/10_02_Python_API_Reference.html#bound>_option","title":"<code>bound_option</code>","text":"<ul> <li>Description: Gets or sets the NPU core binding strategy.</li> <li>Type: <code>InferenceOption.BOUND_OPTION</code> (Enum).</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#devices","title":"<code>devices</code>","text":"<ul> <li>Description: Gets or sets the list of device IDs to be used for inference. An empty list means all available devices will be used.</li> <li>Type: <code>List[int]</code>.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#use>_ort","title":"<code>use_ort</code>","text":"<ul> <li>Description: Gets or sets whether to use the ONNX Runtime for executing CPU-based tasks in the model graph.</li> <li>Type: <code>bool</code>.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#member>_functions_1","title":"Member Functions","text":""},{"location":"docs/10_02_Python_API_Reference.html#get>_bound>_optionself","title":"<code>get_bound_option(self)</code>","text":"<ul> <li>Signature: <code>def get_bound_option(self) -&gt; BOUND_OPTION</code></li> <li>Description: Returns the current NPU core binding option.</li> <li>Returns: An <code>InferenceOption.BOUND_OPTION</code> enum member.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_devicesself","title":"<code>get_devices(self)</code>","text":"<ul> <li>Signature: <code>def get_devices(self) -&gt; List[int]</code></li> <li>Description: Returns the list of device IDs targeted for inference.</li> <li>Returns: A list of integers.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_use>_ortself","title":"<code>get_use_ort(self)</code>","text":"<ul> <li>Signature: <code>def get_use_ort(self) -&gt; bool</code></li> <li>Description: Returns whether ONNX Runtime usage is enabled.</li> <li>Returns: A boolean value.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#set>_bound>_optionself>_boundoption>_bound>_option","title":"<code>set_bound_option(self, boundOption: BOUND_OPTION)</code>","text":"<ul> <li>Signature: <code>def set_bound_option(self, boundOption: BOUND_OPTION)</code></li> <li>Description: Sets the NPU core binding option.</li> <li>Parameters:<ul> <li><code>boundOption</code>: An <code>InferenceOption.BOUND_OPTION</code> enum member.</li> </ul> </li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#set>_devicesself>_devices>_listint","title":"<code>set_devices(self, devices: List[int])</code>","text":"<ul> <li>Signature: <code>def set_devices(self, devices: List[int])</code></li> <li>Description: Sets the list of device IDs to be used for inference.</li> <li>Parameters:<ul> <li><code>devices</code>: A list of integers representing device IDs.</li> </ul> </li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#set>_use>_ortself>_use>_ort>_bool","title":"<code>set_use_ort(self, use_ort: bool)</code>","text":"<ul> <li>Signature: <code>def set_use_ort(self, use_ort: bool)</code></li> <li>Description: Enables or disables the use of ONNX Runtime for CPU tasks.</li> <li>Parameters:<ul> <li><code>use_ort</code>: A boolean value.</li> </ul> </li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#nested>_classes","title":"Nested Classes","text":""},{"location":"docs/10_02_Python_API_Reference.html#class>_bound>_optionenum","title":"<code>class BOUND_OPTION(Enum)</code>","text":"<ul> <li>Description: An enumeration defining how NPU cores are utilized.</li> <li>Members: <code>NPU_ALL</code>, <code>NPU_0</code>, <code>NPU_1</code>, <code>NPU_2</code>, <code>NPU_01</code>, <code>NPU_12</code>, <code>NPU_02</code>.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#class>_dx>_engineconfiguration","title":"<code>class dx_engine.Configuration</code>","text":"<p>Provides access to the global DXRT configuration singleton, allowing for system-wide settings like enabling the profiler.</p>"},{"location":"docs/10_02_Python_API_Reference.html#constructor_2","title":"Constructor","text":""},{"location":"docs/10_02_Python_API_Reference.html#>_init>_self_1","title":"<code>__init__(self)</code>","text":"<ul> <li>Signature: <code>def __init__(self)</code></li> <li>Description: Initializes the Configuration object by getting a reference to the underlying C++ singleton instance.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#member>_functions_2","title":"Member Functions","text":""},{"location":"docs/10_02_Python_API_Reference.html#get>_attributeself>_item>_item>_attrib>_attribute","title":"<code>get_attribute(self, item: ITEM, attrib: ATTRIBUTE)</code>","text":"<ul> <li>Signature: <code>def get_attribute(self, item: ITEM, attrib: ATTRIBUTE) -&gt; str</code></li> <li>Description: Retrieves the value of a specific attribute for a configuration item.</li> <li>Parameters:<ul> <li><code>item</code>: The configuration category (e.g., <code>Configuration.ITEM.PROFILER</code>).</li> <li><code>attrib</code>: The attribute to retrieve (e.g., <code>Configuration.ATTRIBUTE.PROFILER_SHOW_DATA</code>).</li> </ul> </li> <li>Returns: The attribute value as a string.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_driver>_versionself","title":"<code>get_driver_version(self)</code>","text":"<ul> <li>Signature: <code>def get_driver_version(self) -&gt; str</code></li> <li>Description: Returns the version of the installed device driver.</li> <li>Returns: The driver version string.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_enableself>_item>_item","title":"<code>get_enable(self, item: ITEM)</code>","text":"<ul> <li>Signature: <code>def get_enable(self, item: ITEM) -&gt; bool</code></li> <li>Description: Checks if a specific configuration item is enabled.</li> <li>Parameters:<ul> <li><code>item</code>: The configuration category to check.</li> </ul> </li> <li>Returns: <code>True</code> if enabled, <code>False</code> otherwise.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_pcie>_driver>_versionself","title":"<code>get_pcie_driver_version(self)</code>","text":"<ul> <li>Signature: <code>def get_pcie_driver_version(self) -&gt; str</code></li> <li>Description: Returns the version of the installed PCIe driver.</li> <li>Returns: The PCIe driver version string.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_versionself","title":"<code>get_version(self)</code>","text":"<ul> <li>Signature: <code>def get_version(self) -&gt; str</code></li> <li>Description: Returns the version of the DXRT library.</li> <li>Returns: The library version string.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#load>_config>_fileself>_file>_name>_str","title":"<code>load_config_file(self, file_name: str)</code>","text":"<ul> <li>Signature: <code>def load_config_file(self, file_name: str)</code></li> <li>Description: Loads configuration settings from a specified file.</li> <li>Parameters:<ul> <li><code>file_name</code>: The path to the configuration file.</li> </ul> </li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#set>_attributeself>_item>_item>_attrib>_attribute>_value>_str","title":"<code>set_attribute(self, item: ITEM, attrib: ATTRIBUTE, value: str)</code>","text":"<ul> <li>Signature: <code>def set_attribute(self, item: ITEM, attrib: ATTRIBUTE, value: str)</code></li> <li>Description: Sets a string value for a specific attribute of a configuration item (e.g., setting <code>PROFILER_SAVE_DATA</code> to <code>\"ON\"</code>).</li> <li>Parameters:<ul> <li><code>item</code>: The configuration category.</li> <li><code>attrib</code>: The attribute to set.</li> <li><code>value</code>: The string value to assign.</li> </ul> </li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#set>_enableself>_item>_item>_enabled>_bool","title":"<code>set_enable(self, item: ITEM, enabled: bool)</code>","text":"<ul> <li>Signature: <code>def set_enable(self, item: ITEM, enabled: bool)</code></li> <li>Description: Enables or disables a global configuration item, such as <code>PROFILER</code>.</li> <li>Parameters:<ul> <li><code>item</code>: The configuration category.</li> <li><code>enabled</code>: A boolean value to enable (<code>True</code>) or disable (<code>False</code>) the item.</li> </ul> </li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#nested>_classes_1","title":"Nested Classes","text":""},{"location":"docs/10_02_Python_API_Reference.html#class>_item","title":"<code>class ITEM</code>","text":"<ul> <li>Description: An enumeration-like class defining configuration categories.</li> <li>Members: <code>DEBUG</code>, <code>PROFILER</code>, <code>SERVICE</code>, <code>DYNAMIC_CPU_THREAD</code>, <code>TASK_FLOW</code>, <code>SHOW_THROTTLING</code>, <code>SHOW_PROFILE</code>, <code>SHOW_MODEL_INFO</code>.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#class>_attribute","title":"<code>class ATTRIBUTE</code>","text":"<ul> <li>Description: An enumeration-like class defining attributes for configuration items.</li> <li>Members: <code>PROFILER_SHOW_DATA</code>, <code>PROFILER_SAVE_DATA</code>.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#class>_dx>_enginedevicestatus","title":"<code>class dx_engine.DeviceStatus</code>","text":"<p>Provides an interface to query real-time status and static information about hardware devices.</p>"},{"location":"docs/10_02_Python_API_Reference.html#class>_methods","title":"Class Methods","text":""},{"location":"docs/10_02_Python_API_Reference.html#get>_current>_statuscls>_deviceid>_int","title":"<code>get_current_status(cls, deviceId: int)</code>","text":"<ul> <li>Signature: <code>def get_current_status(cls, deviceId: int) -&gt; object</code></li> <li>Description: Creates and returns a <code>DeviceStatus</code> object populated with the current status of the specified device.</li> <li>Parameters:<ul> <li><code>deviceId</code>: The integer ID of the device to query.</li> </ul> </li> <li>Returns: An instance of <code>DeviceStatus</code>.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_device>_countcls","title":"<code>get_device_count(cls)</code>","text":"<ul> <li>Signature: <code>def get_device_count(cls) -&gt; int</code></li> <li>Description: Returns the total number of hardware devices detected by the system.</li> <li>Returns: The number of devices as an integer.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#instance>_methods","title":"Instance Methods","text":""},{"location":"docs/10_02_Python_API_Reference.html#get>_idself","title":"<code>get_id(self)</code>","text":"<ul> <li>Signature: <code>def get_id(self) -&gt; int</code></li> <li>Description: Returns the unique ID of the device associated with this <code>DeviceStatus</code> instance.</li> <li>Returns: The device ID as an integer.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_npu>_clockself>_ch>_int","title":"<code>get_npu_clock(self, ch: int)</code>","text":"<ul> <li>Signature: <code>def get_npu_clock(self, ch: int) -&gt; int</code></li> <li>Description: Returns the current clock frequency of a specific NPU core.</li> <li>Parameters:<ul> <li><code>ch</code>: The integer index of the NPU core.</li> </ul> </li> <li>Returns: The clock speed in MHz.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_npu>_voltageself>_ch>_int","title":"<code>get_npu_voltage(self, ch: int)</code>","text":"<ul> <li>Signature: <code>def get_npu_voltage(self, ch: int) -&gt; int</code></li> <li>Description: Returns the current voltage of a specific NPU core.</li> <li>Parameters:<ul> <li><code>ch</code>: The integer index of the NPU core.</li> </ul> </li> <li>Returns: The voltage in millivolts (mV).</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#get>_temperatureself>_ch>_int","title":"<code>get_temperature(self, ch: int)</code>","text":"<ul> <li>Signature: <code>def get_temperature(self, ch: int) -&gt; int</code></li> <li>Description: Returns the current temperature of a specific NPU core.</li> <li>Parameters:<ul> <li><code>ch</code>: The integer index of the NPU core.</li> </ul> </li> <li>Returns: The temperature in degrees Celsius.</li> </ul>"},{"location":"docs/10_02_Python_API_Reference.html#standalone>_functions","title":"Standalone Functions","text":""},{"location":"docs/10_02_Python_API_Reference.html#dx>_engineparse>_modelmodel>_path>_str","title":"<code>dx_engine.parse_model(model_path: str)</code>","text":"<ul> <li>Signature: <code>def parse_model(model_path: str) -&gt; str</code></li> <li>Description: Parses a model file using the C++ backend and returns a string containing information about the model's structure and properties.</li> <li>Parameters:<ul> <li><code>model_path</code>: The path to the compiled model file.</li> </ul> </li> <li>Returns: A string with model information.</li> </ul>"},{"location":"docs/Appendix_Change_Log.html","title":"Change Log","text":""},{"location":"docs/Appendix_Change_Log.html#v300>_august>_2025","title":"v3.0.0 (August 2025)","text":"<ul> <li>Update minimum versions    <pre><code>- Driver : 1.5.0 -&gt; 1.7.1\n- PCIe Driver : 1.4.0 -&gt; 1.4.1\n- Firmware : 2.0.5 -&gt; 2.1.0\n</code></pre></li> <li>fix kernel panic issue caused by wrong NPU channel number</li> <li>Update DeviceOutputWorker to use 4 threads for 4 DMA channels (3 channels to 4 channels)</li> <li>feat: Improve error message readability in install, build scripts (TFT-101)[https://deepx.atlassian.net/browse/TFT-101]   <pre><code>- Apply color to error messages\n- Reorder message output to display errors before help messages\n</code></pre></li> <li>Update Python Package version (v1.1.1 -&gt; v1.1.2)</li> <li>Modify run_async_model and run_async_model_output examples</li> <li>Modify build.sh (print python package install info)</li> <li>removed some unnecessary items from header files</li> <li>use Pyproject.toml instead setup.py (now setup.py is not recommended)</li> <li>fix some rapidjson issue from clients.</li> <li>remove bad using namespace std from model.h (some programs need change)</li> <li>Add usb inference module (tcp/ip) (MACRO : DXRT_USB_NETWORK_DRIVER)</li> <li>Add options to SanityCheck.sh    <pre><code>- Usage: sudo SanityCheck.sh [all(default) | dx_rt | dx_driver | help]\n</code></pre></li> <li>Change build compiler has been updated to version 14 for both USE_ORT=ON and USE_ORT=OFF configurations.</li> <li>Fix an issue where temporary files from the ONNX Runtime installation would accumulate.</li> <li>Fix a cross-compilation error related to the ncurses library for the dxtop utility.</li> <li>Add Sanity Check Features    <pre><code>- Dependency version check.\n- Executable file check.\n</code></pre></li> <li>Add APIs to the Configuration class for retrieving version information.</li> <li>PCIE details displayed on some device errors</li> <li>dxrt-cli --errorstat option added (this shows pcie detailed information)</li> <li>Modify run_model logging to include host info (Linux only).</li> <li>Add Python examples for configuration and device status.</li> <li>Add Python API for configuration and device status. (dx-engine-1.1.1)</li> <li>Add functionality to query the framework &amp; driver versions in the Configuration class.</li> <li>Add weight checksum info for service</li> <li>Add ENABLE_SHOW_MODEL_INFO build option and configuration item</li> <li>Update code for compatibility with v3 environment </li> <li>Enhance UI for better clarity, enabled dynamic data rendering, and added visual graphs for NPU Memory usage.</li> <li>Fix: fix dx-rt build error caused by pybind11 incompatibility with Python 3.6.9 on Ubuntu 18.04 TFT-82 <pre><code>- Support automatic installation of minimum required Python version (&gt;= 3.8.2)  \n- Install Python 3.8.2 if the system Python version is not supported\n- On Ubuntu 18.04, install via source build; on Ubuntu 20.04+, use apt install\n- Added support in install.sh to optionally accept --python_version and --venv_path for installation\n- Added support in build.sh to accept and use --python_exec\n- Added support in build.sh to optionally accept --venv_path and activate the specified virtual environment\n</code></pre></li> <li>The default build option for DX-RT has been changed from USE_ORT=OFF to USE_ORT=ON. If the inference engine option is not specified separately, use_ort will be enabled by default, activating the CPU task for .dxnn models.</li> <li>Add dxtop tool, a terminal-based monitoring tool for Linux environments. It provides real-time insights into NPU core utilization and DRAM usage per NPU device.</li> </ul>"},{"location":"docs/Appendix_Change_Log.html#v295>_may>_2025","title":"v2.9.5 (May 2025)","text":"<ul> <li>Added full support for Python run_model.  </li> <li>Updated the run_model option and its description  </li> <li>Improve the Python API <pre><code>- InferenceOption is now supported identically to the C++ API.  \n   - set_devices(...) \u2192 devices = [0]  \n   - set_bound_option(...) \u2192 bound_option = InferenceOption.BOUND_OPTION.NPU_ALL\n   - set_use_ort(...) \u2192 use_ort = True\n- Callback functions registered via register_callback now accept user_arg of custom types. (removed .value)\n   - user_arg.value \u2192 user_arg\n- run() now supports both single-input and batch-input modes, depending on the input format.\n</code></pre></li> <li>Modify the build.sh script according to cmake options. <pre><code>- CMake option USE_ORT=ON, running build.sh --clean installs ONNX Runtime.  \n- CMake option USE_PYTHON=ON, running build.sh installs the Python package.  \n- CMake option USE_SERVICE=ON, running build.sh starts or restarts the service.  \n</code></pre></li> <li>Add dxrt-cli -v to display minimum driver &amp; compiler versions  </li> <li>Addressed multithreading issues by implementing additional locks, improving stability under heavy load.  </li> <li>Fix crash on multi-device environments with more than 2 H1 cards. (&gt;=8 devices)  </li> <li>Resolved data corruption errors that could occur in different scenarios, ensuring data integrity.  </li> <li>Fix profiler bugs.  </li> <li>Addressed issues identified by static analysis and other tools, enhancing code quality and reliability.  </li> <li>Add --use_ort flag to the run_model.py example for ONNX Runtime.  </li> <li>Add run batch function. (Python &amp; C++) <pre><code>- batch inference with multiple inputs and multiple outputs. \n</code></pre></li> <li>Minimum model file versions <pre><code>- .dxnn file format version &gt;= v6  \n- compiler version &gt;= v1.15.2  \n</code></pre></li> <li>Minimum Driver and Firmware versions <pre><code>- RT Driver Version &gt;= v1.5.0  \n- PCIe Driver Version &gt;= v1.4.0  \n- Firmware Version &gt;= v2.0.5  \n</code></pre></li> </ul>"},{"location":"docs/Appendix_Change_Log.html#v282>_april>_2025","title":"v2.8.2 (April 2025)","text":"<ul> <li>Modify Inference Engine to be used with 'with' statements, and update relevant examples.  </li> <li>Add Python inference option interface with the following configurations  </li> <li>NPU Device Selection / NPU Bound Option / ORT Usage Flag  </li> <li>Display dxnn versions in parse_model (.dxnn file format version &amp; compiler version)  </li> <li>Added instructions on how to retrieve device status information  </li> <li>Driver and Firmware versions <pre><code>- RT Driver &gt;= v1.3.3  \n- Firmware &gt;= v1.6.3\n</code></pre></li> </ul>"},{"location":"temp/inference_api_reference.html","title":"DXRT Inference API \uc644\uc804 \ucc38\uc870 \uac00\uc774\ub4dc","text":""},{"location":"temp/inference_api_reference.html#_1","title":"\uac1c\uc694","text":"<p>\uc774 \ubb38\uc11c\ub294 DXRT\uc758 \ubaa8\ub4e0 \ucd94\ub860 \uad00\ub828 API\ub4e4\uc5d0 \ub300\ud55c \uc0c1\uc138\ud55c \ucc38\uc870 \uac00\uc774\ub4dc\uc785\ub2c8\ub2e4. C++\uacfc Python \ubc84\uc804 \ubaa8\ub450\uc758 \ud5c8\uc6a9 \uc785\ub825 \ud615\ud0dc\uc640 \uc608\uc0c1 \ucd9c\ub825 \ud615\ud0dc\ub97c comprehensive\ud558\uac8c \uc815\ub9ac\ud588\uc2b5\ub2c8\ub2e4.</p>"},{"location":"temp/inference_api_reference.html#_2","title":"\ubaa9\ucc28","text":"<ol> <li>C++ Inference Engine API</li> <li>Python Inference Engine API</li> <li>\uc785\ub825 \ud615\ud0dc \ubd84\uc11d \ub85c\uc9c1</li> <li>\ucd9c\ub825 \ud615\ud0dc \uaddc\uce59</li> <li>\ud2b9\uc218 \ucf00\uc774\uc2a4</li> </ol>"},{"location":"temp/inference_api_reference.html#c>_inference>_engine>_api","title":"C++ Inference Engine API","text":""},{"location":"temp/inference_api_reference.html#1>_api","title":"1. \ub3d9\uae30 \ucd94\ub860 API","text":""},{"location":"temp/inference_api_reference.html#11>_run>_single>_inputoutput","title":"1.1 Run (Single Input/Output)","text":"<pre><code>TensorPtrs Run(void *inputPtr, void *userArg = nullptr, void *outputPtr = nullptr)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc124\uba85 \ubaa8\ub378 \ud0c0\uc785 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>void* inputPtr</code> \ub2e8\uc77c \uc785\ub825 \ud3ec\uc778\ud130 Single-Input <code>TensorPtrs</code> (Vector) \uc804\ud1b5\uc801\uc778 \ubc29\uc2dd <code>void* inputPtr</code> \uc5f0\uacb0\ub41c \ubc84\ud37c \ud3ec\uc778\ud130 Multi-Input <code>TensorPtrs</code> (Vector) Auto-split \uc801\uc6a9 <p>\uc608\uc81c: <pre><code>// Single input model\nauto outputs = ie.Run(inputData);\n\n// Multi-input model (auto-split)\nauto outputs = ie.Run(concatenatedInput);\n</code></pre></p>"},{"location":"temp/inference_api_reference.html#12>_run>_batch","title":"1.2 Run (Batch)","text":"<pre><code>std::vector&lt;TensorPtrs&gt; Run(\n    const std::vector&lt;void*&gt;&amp; inputBuffers,\n    const std::vector&lt;void*&gt;&amp; outputBuffers,\n    const std::vector&lt;void*&gt;&amp; userArgs\n)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc870\uac74 \ud574\uc11d \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>vector&lt;void*&gt;</code> (size=1) Single-Input \ub2e8\uc77c \ucd94\ub860 <code>vector&lt;TensorPtrs&gt;</code> (size=1) \ud2b9\uc218 \ucf00\uc774\uc2a4 <code>vector&lt;void*&gt;</code> (size=N) Single-Input \ubc30\uce58 \ucd94\ub860 <code>vector&lt;TensorPtrs&gt;</code> (size=N) N\uac1c \uc0d8\ud50c <code>vector&lt;void*&gt;</code> (size=M) Multi-Input, M==input_count \ub2e8\uc77c \ucd94\ub860 <code>vector&lt;TensorPtrs&gt;</code> (size=1) Multi-input \ub2e8\uc77c <code>vector&lt;void*&gt;</code> (size=N*M) Multi-Input, N*M==\ubc30\uc218 \ubc30\uce58 \ucd94\ub860 <code>vector&lt;TensorPtrs&gt;</code> (size=N) N\uac1c \uc0d8\ud50c, M\uac1c \uc785\ub825 <p>\uc608\uc81c: <pre><code>// Single input batch\nstd::vector&lt;void*&gt; batchInputs = {sample1, sample2, sample3};\nauto batchOutputs = ie.Run(batchInputs, outputBuffers, userArgs);\n\n// Multi-input single\nstd::vector&lt;void*&gt; multiInputs = {input1, input2}; // M=2\nauto singleOutput = ie.Run(multiInputs, {outputBuffer}, {userArg});\n\n// Multi-input batch  \nstd::vector&lt;void*&gt; multiBatch = {s1_i1, s1_i2, s2_i1, s2_i2}; // N=2, M=2\nauto batchOutputs = ie.Run(multiBatch, outputBuffers, userArgs);\n</code></pre></p>"},{"location":"temp/inference_api_reference.html#13>_runmultiinput>_dictionary","title":"1.3 RunMultiInput (Dictionary)","text":"<pre><code>TensorPtrs RunMultiInput(\n    const std::map&lt;std::string, void*&gt;&amp; inputTensors, \n    void *userArg = nullptr, \n    void *outputPtr = nullptr\n)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>map&lt;string, void*&gt;</code> \ubaa8\ub4e0 \uc785\ub825 \ud150\uc11c \uc774\ub984 \ud3ec\ud568 <code>TensorPtrs</code> Multi-input \uc804\uc6a9 <p>\uc608\uc81c: <pre><code>std::map&lt;std::string, void*&gt; inputs = {\n    {\"input1\", data1},\n    {\"input2\", data2}\n};\nauto outputs = ie.RunMultiInput(inputs);\n</code></pre></p>"},{"location":"temp/inference_api_reference.html#14>_runmultiinput>_vector","title":"1.4 RunMultiInput (Vector)","text":"<pre><code>TensorPtrs RunMultiInput(\n    const std::vector&lt;void*&gt;&amp; inputPtrs, \n    void *userArg = nullptr, \n    void *outputPtr = nullptr\n)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>vector&lt;void*&gt;</code> size == input_tensor_count <code>TensorPtrs</code> \uc21c\uc11c\ub294 GetInputTensorNames()"},{"location":"temp/inference_api_reference.html#2>_api","title":"2. \ube44\ub3d9\uae30 \ucd94\ub860 API","text":""},{"location":"temp/inference_api_reference.html#21>_runasync>_single","title":"2.1 RunAsync (Single)","text":"<pre><code>int RunAsync(void *inputPtr, void *userArg = nullptr, void *outputPtr = nullptr)\n</code></pre> \uc785\ub825 \ud615\ud0dc \ubaa8\ub378 \ud0c0\uc785 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>void* inputPtr</code> Single-Input <code>int</code> (jobId) Wait(jobId)\ub85c \uacb0\uacfc \uc218\uc2e0 <code>void* inputPtr</code> Multi-Input <code>int</code> (jobId) Auto-split \uc801\uc6a9"},{"location":"temp/inference_api_reference.html#22>_runasync>_vector","title":"2.2 RunAsync (Vector)","text":"<pre><code>int RunAsync(const std::vector&lt;void*&gt;&amp; inputPtrs, void *userArg = nullptr, void *outputPtr = nullptr)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc870\uac74 \ud574\uc11d \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>vector&lt;void*&gt;</code> (size==input_count) Multi-Input Multi-input \ub2e8\uc77c <code>int</code> (jobId) \uad8c\uc7a5 \ubc29\uc2dd <code>vector&lt;void*&gt;</code> (size!=input_count) Any \uccab \ubc88\uc9f8 \uc694\uc18c\ub9cc \uc0ac\uc6a9 <code>int</code> (jobId) Fallback"},{"location":"temp/inference_api_reference.html#23>_runasyncmultiinput>_dictionary","title":"2.3 RunAsyncMultiInput (Dictionary)","text":"<pre><code>int RunAsyncMultiInput(\n    const std::map&lt;std::string, void*&gt;&amp; inputTensors, \n    void *userArg = nullptr, \n    void *outputPtr = nullptr\n)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>map&lt;string, void*&gt;</code> Multi-input \ubaa8\ub378 \uc804\uc6a9 <code>int</code> (jobId) \uac00\uc7a5 \uba85\ud655\ud55c \ubc29\uc2dd"},{"location":"temp/inference_api_reference.html#24>_runasyncmultiinput>_vector","title":"2.4 RunAsyncMultiInput (Vector)","text":"<pre><code>int RunAsyncMultiInput(\n    const std::vector&lt;void*&gt;&amp; inputPtrs, \n    void *userArg = nullptr, \n    void *outputPtr = nullptr\n)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud615\ud0dc \ube44\uace0 <code>vector&lt;void*&gt;</code> size == input_tensor_count <code>int</code> (jobId) Dictionary\ub85c \ubcc0\ud658\ub428"},{"location":"temp/inference_api_reference.html#3>_api","title":"3. \uc7a5\uce58 \uac80\uc99d API","text":""},{"location":"temp/inference_api_reference.html#31>_validatedevice>_single","title":"3.1 ValidateDevice (Single)","text":"<pre><code>TensorPtrs ValidateDevice(void *inputPtr, int deviceId = 0)\n</code></pre> \uc785\ub825 \ud615\ud0dc \ubaa8\ub378 \ud0c0\uc785 \ucd9c\ub825 \ud615\ud0dc \uc81c\uc57d \uc870\uac74 <code>void* inputPtr</code> Any <code>TensorPtrs</code> Debug \ubaa8\ub4dc \ubaa8\ub378\ub9cc"},{"location":"temp/inference_api_reference.html#32>_validatedevice>_vector","title":"3.2 ValidateDevice (Vector)","text":"<pre><code>TensorPtrs ValidateDevice(const std::vector&lt;void*&gt;&amp; inputPtrs, int deviceId = 0)\n</code></pre> \uc785\ub825 \ud615\ud0dc \uc870\uac74 \ud574\uc11d \ucd9c\ub825 \ud615\ud0dc <code>vector&lt;void*&gt;</code> (size==input_count) Multi-Input Multi-input \uac80\uc99d <code>TensorPtrs</code> <code>vector&lt;void*&gt;</code> (other) Any \uccab \ubc88\uc9f8 \uc694\uc18c\ub9cc \uc0ac\uc6a9 <code>TensorPtrs</code>"},{"location":"temp/inference_api_reference.html#33>_validatedevicemultiinput","title":"3.3 ValidateDeviceMultiInput","text":"<pre><code>TensorPtrs ValidateDeviceMultiInput(const std::map&lt;std::string, void*&gt;&amp; inputTensors, int deviceId = 0)\nTensorPtrs ValidateDeviceMultiInput(const std::vector&lt;void*&gt;&amp; inputPtrs, int deviceId = 0)\n</code></pre>"},{"location":"temp/inference_api_reference.html#python>_inference>_engine>_api","title":"Python Inference Engine API","text":""},{"location":"temp/inference_api_reference.html#1>_api_1","title":"1. \ub3d9\uae30 \ucd94\ub860 API","text":""},{"location":"temp/inference_api_reference.html#11>_run>_unified>_api","title":"1.1 run (Unified API)","text":"<pre><code>def run(\n    input_data: Union[np.ndarray, List[np.ndarray], List[List[np.ndarray]]],\n    output_buffers: Optional[Union[List[np.ndarray], List[List[np.ndarray]]]] = None,\n    user_args: Optional[Union[Any, List[Any]]] = None\n) -&gt; Union[List[np.ndarray], List[List[np.ndarray]]]\n</code></pre> <p>\uc0c1\uc138 \uc785\ub825/\ucd9c\ub825 \ub9e4\ud2b8\ub9ad\uc2a4:</p> \uc785\ub825 \ud0c0\uc785 \uc785\ub825 \uc870\uac74 \ubaa8\ub378 \ud0c0\uc785 \ud574\uc11d \ucd9c\ub825 \ud0c0\uc785 \ucd9c\ub825 \uad6c\uc870 <code>np.ndarray</code> size == total_input_size Multi-Input Auto-split \ub2e8\uc77c <code>List[np.ndarray]</code> \ub2e8\uc77c \uc0d8\ud50c \ucd9c\ub825 <code>np.ndarray</code> size != total_input_size Single-Input \ub2e8\uc77c \ucd94\ub860 <code>List[np.ndarray]</code> \ub2e8\uc77c \uc0d8\ud50c \ucd9c\ub825 <code>List[np.ndarray]</code> len == 1 Single-Input \ub2e8\uc77c \ucd94\ub860 <code>List[np.ndarray]</code> \ub2e8\uc77c \uc0d8\ud50c \ucd9c\ub825 <code>List[np.ndarray]</code> len == input_count Multi-Input \ub2e8\uc77c \ucd94\ub860 <code>List[np.ndarray]</code> \ub2e8\uc77c \uc0d8\ud50c \ucd9c\ub825 <code>List[np.ndarray]</code> len == N*input_count Multi-Input \ubc30\uce58 \ucd94\ub860 (N\uc0d8\ud50c) <code>List[List[np.ndarray]]</code> N\uac1c \uc0d8\ud50c \ucd9c\ub825 <code>List[np.ndarray]</code> len &gt; 1 Single-Input \ubc30\uce58 \ucd94\ub860 <code>List[List[np.ndarray]]</code> len\uac1c \uc0d8\ud50c \ucd9c\ub825 <code>List[List[np.ndarray]]</code> \uba85\uc2dc\uc801 \ubc30\uce58 Any \ubc30\uce58 \ucd94\ub860 <code>List[List[np.ndarray]]</code> \uc678\ubd80 \ub9ac\uc2a4\ud2b8 \ud06c\uae30\ub9cc\ud07c <p>Auto-split \ud2b9\uc218 \ucf00\uc774\uc2a4:</p> \uc870\uac74 \uc785\ub825 \uc608\uc2dc \ud574\uc11d \ucd9c\ub825 Multi-input + \uccab \ubc88\uc9f8 \uc694\uc18c\uac00 total_size <code>[concatenated_array]</code> Auto-split \ub2e8\uc77c <code>List[np.ndarray]</code> Multi-input + \ubaa8\ub4e0 \uc694\uc18c\uac00 total_size <code>[concat1, concat2, concat3]</code> Auto-split \ubc30\uce58 <code>List[List[np.ndarray]]</code> <p>\uc608\uc81c: <pre><code># 1. Single array auto-split (multi-input)\nconcatenated = np.zeros(ie.get_input_size(), dtype=np.uint8)\noutputs = ie.run(concatenated)  # List[np.ndarray]\n\n# 2. Multi-input single\ninput_list = [input1_array, input2_array]  # len == 2\noutputs = ie.run(input_list)  # List[np.ndarray]\n\n# 3. Multi-input batch (flattened)\nflattened = [s1_i1, s1_i2, s2_i1, s2_i2]  # 2 samples, 2 inputs each\noutputs = ie.run(flattened)  # List[List[np.ndarray]], len=2\n\n# 4. Multi-input batch (explicit)\nexplicit_batch = [[s1_i1, s1_i2], [s2_i1, s2_i2]]\noutputs = ie.run(explicit_batch)  # List[List[np.ndarray]], len=2\n\n# 5. Single-input batch\nsingle_batch = [sample1, sample2, sample3]\noutputs = ie.run(single_batch)  # List[List[np.ndarray]], len=3\n</code></pre></p>"},{"location":"temp/inference_api_reference.html#12>_run>_multi>_input>_dictionary","title":"1.2 run_multi_input (Dictionary)","text":"<pre><code>def run_multi_input(\n    input_tensors: Dict[str, np.ndarray],\n    output_buffers: Optional[List[np.ndarray]] = None,\n    user_arg: Any = None\n) -&gt; List[np.ndarray]\n</code></pre> \uc785\ub825 \ud0c0\uc785 \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud0c0\uc785 \ube44\uace0 <code>Dict[str, np.ndarray]</code> \ubaa8\ub4e0 \uc785\ub825 \ud150\uc11c \ud3ec\ud568 <code>List[np.ndarray]</code> Multi-input \uc804\uc6a9"},{"location":"temp/inference_api_reference.html#2>_api_1","title":"2. \ube44\ub3d9\uae30 \ucd94\ub860 API","text":""},{"location":"temp/inference_api_reference.html#21>_run>_async","title":"2.1 run_async","text":"<pre><code>def run_async(\n    input_data: Union[np.ndarray, List[np.ndarray]],\n    user_arg: Any = None,\n    output_buffer: Optional[Union[np.ndarray, List[np.ndarray]]] = None\n) -&gt; int\n</code></pre> \uc785\ub825 \ud0c0\uc785 \uc870\uac74 \ud574\uc11d \ucd9c\ub825 \ud0c0\uc785 \uc81c\uc57d <code>np.ndarray</code> Any \ub2e8\uc77c \ucd94\ub860 <code>int</code> (jobId) \ubc30\uce58 \uc9c0\uc6d0 \uc548\ud568 <code>List[np.ndarray]</code> len == input_count Multi-input \ub2e8\uc77c <code>int</code> (jobId) \ubc30\uce58 \uc9c0\uc6d0 \uc548\ud568 <code>List[np.ndarray]</code> len == 1 Single-input \ub2e8\uc77c <code>int</code> (jobId) \ubc30\uce58 \uc9c0\uc6d0 \uc548\ud568"},{"location":"temp/inference_api_reference.html#22>_run>_async>_multi>_input","title":"2.2 run_async_multi_input","text":"<pre><code>def run_async_multi_input(\n    input_tensors: Dict[str, np.ndarray],\n    user_arg: Any = None,\n    output_buffer: Optional[List[np.ndarray]] = None\n) -&gt; int\n</code></pre> \uc785\ub825 \ud0c0\uc785 \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud0c0\uc785 \ube44\uace0 <code>Dict[str, np.ndarray]</code> Multi-input \ubaa8\ub378 \uc804\uc6a9 <code>int</code> (jobId) \ub2e8\uc77c \ucd94\ub860\ub9cc"},{"location":"temp/inference_api_reference.html#3>_api_1","title":"3. \uc7a5\uce58 \uac80\uc99d API","text":""},{"location":"temp/inference_api_reference.html#31>_validate>_device","title":"3.1 validate_device","text":"<pre><code>def validate_device(\n    input_data: Union[np.ndarray, List[np.ndarray]], \n    device_id: int = 0\n) -&gt; List[np.ndarray]\n</code></pre> \uc785\ub825 \ud0c0\uc785 \uc870\uac74 \ud574\uc11d \ucd9c\ub825 \ud0c0\uc785 <code>np.ndarray</code> Any \ub2e8\uc77c \uac80\uc99d <code>List[np.ndarray]</code> <code>List[np.ndarray]</code> len == input_count Multi-input \uac80\uc99d <code>List[np.ndarray]</code> <code>List[np.ndarray]</code> len != input_count \uccab \ubc88\uc9f8 \uc694\uc18c\ub9cc <code>List[np.ndarray]</code>"},{"location":"temp/inference_api_reference.html#32>_validate>_device>_multi>_input","title":"3.2 validate_device_multi_input","text":"<pre><code>def validate_device_multi_input(\n    input_tensors: Dict[str, np.ndarray], \n    device_id: int = 0\n) -&gt; List[np.ndarray]\n</code></pre>"},{"location":"temp/inference_api_reference.html#4>_api","title":"4. \uae30\ud0c0 API","text":""},{"location":"temp/inference_api_reference.html#41>_run>_benchmark","title":"4.1 run_benchmark","text":"<pre><code>def run_benchmark(\n    num_loops: int, \n    input_data: Optional[List[np.ndarray]] = None\n) -&gt; float\n</code></pre> \uc785\ub825 \ud0c0\uc785 \uc81c\uc57d \uc870\uac74 \ucd9c\ub825 \ud0c0\uc785 \ube44\uace0 <code>List[np.ndarray]</code> Single input format <code>float</code> (FPS) \uccab \ubc88\uc9f8 \uc694\uc18c \ubc18\ubcf5 \uc0ac\uc6a9"},{"location":"temp/inference_api_reference.html#42>_wait","title":"4.2 wait","text":"<pre><code>def wait(job_id: int) -&gt; List[np.ndarray]\n</code></pre>"},{"location":"temp/inference_api_reference.html#_3","title":"\uc785\ub825 \ud615\ud0dc \ubd84\uc11d \ub85c\uc9c1","text":""},{"location":"temp/inference_api_reference.html#python","title":"Python \uc785\ub825 \ubd84\uc11d \ud50c\ub85c\uc6b0","text":"<pre><code>def _analyze_input_format(input_data):\n    # 1. np.ndarray \uac80\uc0ac\n    if isinstance(input_data, np.ndarray):\n        if should_auto_split_input(input_data):\n            return auto_split_single_inference()\n        else:\n            return single_inference()\n\n    # 2. List \uac80\uc0ac\n    if isinstance(input_data, list):\n        if isinstance(input_data[0], list):\n            # List[List[np.ndarray]] - \uba85\uc2dc\uc801 \ubc30\uce58\n            return explicit_batch_inference()\n        else:\n            # List[np.ndarray] - \ubcf5\uc7a1\ud55c \ubd84\uc11d \ud544\uc694\n            return analyze_list_ndarray(input_data)\n</code></pre>"},{"location":"temp/inference_api_reference.html#listnpndarray","title":"List[np.ndarray] \ubd84\uc11d \uc0c1\uc138","text":"<pre><code>def analyze_list_ndarray(input_data):\n    input_count = len(input_data)\n\n    if is_multi_input_model():\n        expected_count = get_input_tensor_count()\n\n        if input_count == expected_count:\n            return single_inference()\n        elif input_count % expected_count == 0:\n            batch_size = input_count // expected_count\n            return batch_inference(batch_size)\n        elif all(should_auto_split_input(arr) for arr in input_data):\n            return auto_split_batch_inference()\n        else:\n            raise ValueError(\"Invalid input count\")\n    else:  # Single-input model\n        if input_count == 1:\n            return single_inference()\n        else:\n            return batch_inference(input_count)\n</code></pre>"},{"location":"temp/inference_api_reference.html#_4","title":"\ucd9c\ub825 \ud615\ud0dc \uaddc\uce59","text":""},{"location":"temp/inference_api_reference.html#1","title":"1. \ub2e8\uc77c \ucd94\ub860 \ucd9c\ub825","text":"API \ucd9c\ub825 \ud615\ud0dc \uad6c\uc870 C++ Run <code>TensorPtrs</code> <code>vector&lt;shared_ptr&lt;Tensor&gt;&gt;</code> Python run <code>List[np.ndarray]</code> <code>[output1, output2, ...]</code>"},{"location":"temp/inference_api_reference.html#2","title":"2. \ubc30\uce58 \ucd94\ub860 \ucd9c\ub825","text":"API \ucd9c\ub825 \ud615\ud0dc \uad6c\uc870 C++ Run (batch) <code>vector&lt;TensorPtrs&gt;</code> <code>[sample1_outputs, sample2_outputs, ...]</code> Python run (batch) <code>List[List[np.ndarray]]</code> <code>[[s1_o1, s1_o2], [s2_o1, s2_o2], ...]</code>"},{"location":"temp/inference_api_reference.html#3","title":"3. \ube44\ub3d9\uae30 \ucd9c\ub825","text":"API \uc989\uc2dc \ubc18\ud658 Wait \ud6c4 C++ RunAsync <code>int</code> (jobId) <code>TensorPtrs</code> Python run_async <code>int</code> (jobId) <code>List[np.ndarray]</code>"},{"location":"temp/inference_api_reference.html#_5","title":"\ud2b9\uc218 \ucf00\uc774\uc2a4","text":""},{"location":"temp/inference_api_reference.html#1>_auto-split","title":"1. Auto-Split \uc870\uac74","text":"<p>C++: <pre><code>bool shouldAutoSplitInput() const {\n    return _isMultiInput &amp;&amp; _inputTasks.size() == 1;\n}\n</code></pre></p> <p>Python: <pre><code>def _should_auto_split_input(input_data: np.ndarray) -&gt; bool:\n    if not self.is_multi_input_model():\n        return False\n\n    expected_total_size = self.get_input_size()\n    actual_size = input_data.nbytes\n\n    return actual_size == expected_total_size\n</code></pre></p>"},{"location":"temp/inference_api_reference.html#2_1","title":"2. \ubc30\uce58 \ud06c\uae30 \uacb0\uc815","text":"\uc870\uac74 \ubc30\uce58 \ud06c\uae30 \uacc4\uc0b0 Single-input + List[np.ndarray] <code>len(input_data)</code> Multi-input + List[np.ndarray] <code>len(input_data) // input_tensor_count</code> List[List[np.ndarray]] <code>len(input_data)</code>"},{"location":"temp/inference_api_reference.html#3_1","title":"3. \uc5d0\ub7ec \uc870\uac74","text":"\uc870\uac74 \uc5d0\ub7ec \ud0c0\uc785 \uba54\uc2dc\uc9c0 Multi-input + \uc798\ubabb\ub41c \ud06c\uae30 <code>ValueError</code> \"Invalid input count for multi-input model\" \ube44\ub3d9\uae30 + \ubc30\uce58 <code>ValueError</code> \"Batch inference not supported in async\" \ube48 \uc785\ub825 <code>ValueError</code> \"Input data cannot be empty\" \ud0c0\uc785 \ubd88\uc77c\uce58 <code>TypeError</code> \"Expected np.ndarray or List[np.ndarray]\""},{"location":"temp/inference_api_reference.html#4>_output>_buffer","title":"4. Output Buffer \ucc98\ub9ac","text":""},{"location":"temp/inference_api_reference.html#python>_output>_buffer","title":"Python Output Buffer \ub9e4\ud2b8\ub9ad\uc2a4","text":"\uc785\ub825 \ud615\ud0dc Output Buffer \ud615\ud0dc \ucc98\ub9ac \ubc29\uc2dd \ub2e8\uc77c \ucd94\ub860 <code>None</code> \uc790\ub3d9 \ud560\ub2f9 \ub2e8\uc77c \ucd94\ub860 <code>List[np.ndarray]</code> \uc0ac\uc6a9\uc790 \uc81c\uacf5 \ub2e8\uc77c \ucd94\ub860 <code>np.ndarray</code> (total_size) Auto-split \ud6c4 \uc0ac\uc6a9 \ubc30\uce58 \ucd94\ub860 <code>List[List[np.ndarray]]</code> \uba85\uc2dc\uc801 \ubc30\uce58 \ubc84\ud37c \ubc30\uce58 \ucd94\ub860 <code>List[np.ndarray]</code> \ud50c\ub798\ud2bc\ub41c \ubc30\uce58 \ubc84\ud37c"},{"location":"temp/inference_api_reference.html#_6","title":"\uc131\ub2a5 \uace0\ub824\uc0ac\ud56d","text":""},{"location":"temp/inference_api_reference.html#1_1","title":"1. \uba54\ubaa8\ub9ac \ud560\ub2f9","text":"\ubc29\uc2dd \uc7a5\uc810 \ub2e8\uc810 \uc790\ub3d9 \ud560\ub2f9 (No Buffer) \uc0ac\uc6a9 \ud3b8\uc758\uc131 \ub9e4\ubc88 \uba54\ubaa8\ub9ac \ud560\ub2f9 \uc0ac\uc6a9\uc790 \uc81c\uacf5 (With Buffer) \uc131\ub2a5 \ucd5c\uc801\ud654 \uba54\ubaa8\ub9ac \uad00\ub9ac \ubcf5\uc7a1"},{"location":"temp/inference_api_reference.html#2_2","title":"2. \ucd94\ub860 \ubc29\uc2dd","text":"\ubc29\uc2dd \uc6a9\ub3c4 \ud2b9\uc9d5 \ub3d9\uae30 \ucd94\ub860 \uac04\ub2e8\ud55c \ucc98\ub9ac \uc21c\ucc28 \uc2e4\ud589 \ube44\ub3d9\uae30 \ucd94\ub860 \ub192\uc740 \ucc98\ub9ac\ub7c9 \ucf5c\ubc31 \uad00\ub9ac \ud544\uc694 \ubc30\uce58 \ucd94\ub860 \ub300\ub7c9 \ucc98\ub9ac \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9 \uc99d\uac00 <p>\uc774 \ubb38\uc11c\ub294 DXRT\uc758 \ubaa8\ub4e0 \ucd94\ub860 API\uc5d0 \ub300\ud55c \uc644\uc804\ud55c \ucc38\uc870\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uac01 API\uc758 \uc815\ud655\ud55c \uc0ac\uc6a9\ubc95\uacfc \uc608\uc0c1 \ub3d9\uc791\uc744 \uc774\ud574\ud558\uc5ec \uc62c\ubc14\ub978 \ucd94\ub860 \ucf54\ub4dc\ub97c \uc791\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. </p>"},{"location":"temp/multi_input_guide.html","title":"Multi-Input \ubaa8\ub378 \ucd94\ub860 API \uac00\uc774\ub4dc","text":""},{"location":"temp/multi_input_guide.html#_1","title":"\uac1c\uc694","text":"<p>DXRT\ub294 \uc5ec\ub7ec \uac1c\uc758 \uc785\ub825 \ud150\uc11c\ub97c \uac00\uc9c4 multi-input \ubaa8\ub378\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ucd94\ub860 \ubc29\uc2dd\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc774 \ubb38\uc11c\ub294 multi-input \ubaa8\ub378\uc758 \ucd94\ub860 API \ud65c\uc6a9 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_guide.html#multi-input","title":"Multi-Input \ubaa8\ub378 \ud655\uc778","text":""},{"location":"temp/multi_input_guide.html#c","title":"C++","text":"<pre><code>dxrt::InferenceEngine ie(modelPath);\n\n// \ubaa8\ub378\uc774 multi-input\uc778\uc9c0 \ud655\uc778\nbool isMultiInput = ie.IsMultiInputModel();\n\n// \uc785\ub825 \ud150\uc11c \uac1c\uc218 \ud655\uc778\nint inputCount = ie.GetInputTensorCount();\n\n// \uc785\ub825 \ud150\uc11c \uc774\ub984\ub4e4 \ud655\uc778\nstd::vector&lt;std::string&gt; inputNames = ie.GetInputTensorNames();\n\n// \uc785\ub825 \ud150\uc11c\uc640 \ud0dc\uc2a4\ud06c \ub9e4\ud551 \ud655\uc778\nstd::map&lt;std::string, std::string&gt; mapping = ie.GetInputTensorToTaskMapping();\n</code></pre>"},{"location":"temp/multi_input_guide.html#python","title":"Python","text":"<pre><code>from dx_engine import InferenceEngine\n\nie = InferenceEngine(model_path)\n\n# \ubaa8\ub378\uc774 multi-input\uc778\uc9c0 \ud655\uc778\nis_multi_input = ie.is_multi_input_model()\n\n# \uc785\ub825 \ud150\uc11c \uac1c\uc218 \ud655\uc778\ninput_count = ie.get_input_tensor_count()\n\n# \uc785\ub825 \ud150\uc11c \uc774\ub984\ub4e4 \ud655\uc778\ninput_names = ie.get_input_tensor_names()\n\n# \uc785\ub825 \ud150\uc11c\uc640 \ud0dc\uc2a4\ud06c \ub9e4\ud551 \ud655\uc778\nmapping = ie.get_input_tensor_to_task_mapping()\n</code></pre>"},{"location":"temp/multi_input_guide.html#multi-input_1","title":"Multi-Input \ucd94\ub860 \ubc29\uc2dd","text":""},{"location":"temp/multi_input_guide.html#1>_no>_output>_buffer>_tests","title":"1. No Output Buffer Tests (\uc790\ub3d9 \ud560\ub2f9)","text":"<p>\ucd9c\ub825 \ubc84\ud37c\ub97c \uc81c\uacf5\ud558\uc9c0 \uc54a\uace0 \ucd94\ub860 \uc5d4\uc9c4\uc774 \uc790\ub3d9\uc73c\ub85c \uba54\ubaa8\ub9ac\ub97c \ud560\ub2f9\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_guide.html#11>_dictionary>_format","title":"1.1 Dictionary Format (\uad8c\uc7a5)","text":"<p>\uc785\ub825 \ud150\uc11c\ub97c \uc774\ub984\uc73c\ub85c \ub9e4\ud551\ud558\uc5ec \uc81c\uacf5\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \uac00\uc7a5 \uba85\ud655\ud558\uace0 \uc2e4\uc218\uac00 \uc801\uc740 \ubc29\ubc95\uc785\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_guide.html#c_1","title":"C++","text":"<pre><code>// Dictionary format \uc0ac\uc6a9 (\uc790\ub3d9 \ud560\ub2f9)\nstd::map&lt;std::string, void*&gt; inputTensors;\ninputTensors[\"input1\"] = input1_data;\ninputTensors[\"input2\"] = input2_data;\n\n// \ucd9c\ub825 \ubc84\ud37c \uc5c6\uc774 \ub3d9\uae30 \ucd94\ub860 (\uc790\ub3d9 \ud560\ub2f9)\nauto outputs = ie.RunMultiInput(inputTensors);\n</code></pre>"},{"location":"temp/multi_input_guide.html#python_1","title":"Python","text":"<pre><code># Dictionary format \uc0ac\uc6a9 (\uc790\ub3d9 \ud560\ub2f9)\ninput_tensors = {\n    \"input1\": input1_array,\n    \"input2\": input2_array\n}\n\n# \ucd9c\ub825 \ubc84\ud37c \uc5c6\uc774 \ub3d9\uae30 \ucd94\ub860 (\uc790\ub3d9 \ud560\ub2f9)\noutputs = ie.run_multi_input(input_tensors)\n</code></pre>"},{"location":"temp/multi_input_guide.html#12>_vector>_format","title":"1.2 Vector Format","text":"<p>\uc785\ub825 \ud150\uc11c\ub97c \uc21c\uc11c\ub300\ub85c \ubca1\ud130/\ub9ac\uc2a4\ud2b8\ub85c \uc81c\uacf5\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. <code>GetInputTensorNames()</code>\ub85c \ubc18\ud658\ub41c \uc21c\uc11c\uc640 \uc77c\uce58\ud574\uc57c \ud569\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_guide.html#c_2","title":"C++","text":"<pre><code>// Vector format \uc0ac\uc6a9 (GetInputTensorNames() \uc21c\uc11c\uc640 \uc77c\uce58)\nstd::vector&lt;void*&gt; inputPtrs = {input1_data, input2_data};\n\n// \ucd9c\ub825 \ubc84\ud37c \uc5c6\uc774 \ub3d9\uae30 \ucd94\ub860 (\uc790\ub3d9 \ud560\ub2f9)\nauto outputs = ie.RunMultiInput(inputPtrs);\n</code></pre>"},{"location":"temp/multi_input_guide.html#python_2","title":"Python","text":"<pre><code># Vector format \uc0ac\uc6a9 (get_input_tensor_names() \uc21c\uc11c\uc640 \uc77c\uce58)\ninput_list = [input1_array, input2_array]\n\n# \ucd9c\ub825 \ubc84\ud37c \uc5c6\uc774 \ub3d9\uae30 \ucd94\ub860 (\uc790\ub3d9 \ud560\ub2f9)\noutputs = ie.run(input_list)\n</code></pre>"},{"location":"temp/multi_input_guide.html#13>_auto-split>_format","title":"1.3 Auto-Split Format","text":"<p>\ub2e8\uc77c \uc5f0\uacb0\ub41c \ubc84\ud37c\ub97c \uc790\ub3d9\uc73c\ub85c \uc5ec\ub7ec \uc785\ub825\uc73c\ub85c \ubd84\ud560\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \ucd1d \uc785\ub825 \ud06c\uae30\uac00 \uc77c\uce58\ud560 \ub54c \uc790\ub3d9\uc73c\ub85c \uc801\uc6a9\ub429\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_guide.html#c_3","title":"C++","text":"<pre><code>// \ubaa8\ub4e0 \uc785\ub825\uc744 \uc5f0\uacb0\ud55c \ub2e8\uc77c \ubc84\ud37c\nstd::vector&lt;uint8_t&gt; concatenatedInput(ie.GetInputSize());\n// ... \ub370\uc774\ud130 \ucc44\uc6b0\uae30 ...\n\n// \uc790\ub3d9 \ubd84\ud560\ub418\uc5b4 \ucc98\ub9ac\ub428 (\ucd9c\ub825 \ubc84\ud37c \uc790\ub3d9 \ud560\ub2f9)\nauto outputs = ie.Run(concatenatedInput.data());\n</code></pre>"},{"location":"temp/multi_input_guide.html#python_3","title":"Python","text":"<pre><code># \ubaa8\ub4e0 \uc785\ub825\uc744 \uc5f0\uacb0\ud55c \ub2e8\uc77c \ubc30\uc5f4\nconcatenated_input = np.zeros(ie.get_input_size(), dtype=np.uint8)\n# ... \ub370\uc774\ud130 \ucc44\uc6b0\uae30 ...\n\n# \uc790\ub3d9 \ubd84\ud560\ub418\uc5b4 \ucc98\ub9ac\ub428 (\ucd9c\ub825 \ubc84\ud37c \uc790\ub3d9 \ud560\ub2f9)\noutputs = ie.run(concatenated_input)\n</code></pre>"},{"location":"temp/multi_input_guide.html#2>_with>_output>_buffer>_tests","title":"2. With Output Buffer Tests (\uc0ac\uc6a9\uc790 \uc81c\uacf5)","text":"<p>\uc0ac\uc6a9\uc790\uac00 \ucd9c\ub825 \ubc84\ud37c\ub97c \ubbf8\ub9ac \ud560\ub2f9\ud558\uc5ec \uc81c\uacf5\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \uba54\ubaa8\ub9ac \uad00\ub9ac\uc640 \uc131\ub2a5 \ucd5c\uc801\ud654\uc5d0 \uc720\ub9ac\ud569\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_guide.html#21>_dictionary>_format","title":"2.1 Dictionary Format","text":""},{"location":"temp/multi_input_guide.html#c_4","title":"C++","text":"<pre><code>// Dictionary format \uc0ac\uc6a9\nstd::map&lt;std::string, void*&gt; inputTensors;\ninputTensors[\"input1\"] = input1_data;\ninputTensors[\"input2\"] = input2_data;\n\n// \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// \ub3d9\uae30 \ucd94\ub860 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\nauto outputs = ie.RunMultiInput(inputTensors, userArg, outputBuffer.data());\n</code></pre>"},{"location":"temp/multi_input_guide.html#python_4","title":"Python","text":"<pre><code># Dictionary format \uc0ac\uc6a9\ninput_tensors = {\n    \"input1\": input1_array,\n    \"input2\": input2_array\n}\n\n# \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\noutput_buffers = [np.zeros(size, dtype=np.uint8) for size in ie.get_output_tensor_sizes()]\n\n# \ub3d9\uae30 \ucd94\ub860 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\noutputs = ie.run_multi_input(input_tensors, output_buffers=output_buffers)\n</code></pre>"},{"location":"temp/multi_input_guide.html#22>_vector>_format","title":"2.2 Vector Format","text":""},{"location":"temp/multi_input_guide.html#c_5","title":"C++","text":"<pre><code>// Vector format \uc0ac\uc6a9 (GetInputTensorNames() \uc21c\uc11c\uc640 \uc77c\uce58)\nstd::vector&lt;void*&gt; inputPtrs = {input1_data, input2_data};\n\n// \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// \ub3d9\uae30 \ucd94\ub860 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\nauto outputs = ie.RunMultiInput(inputPtrs, userArg, outputBuffer.data());\n</code></pre>"},{"location":"temp/multi_input_guide.html#python_5","title":"Python","text":"<pre><code># Vector format \uc0ac\uc6a9 (get_input_tensor_names() \uc21c\uc11c\uc640 \uc77c\uce58)\ninput_list = [input1_array, input2_array]\n\n# \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\noutput_buffers = [np.zeros(size, dtype=np.uint8) for size in ie.get_output_tensor_sizes()]\n\n# \ub3d9\uae30 \ucd94\ub860 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\noutputs = ie.run(input_list, output_buffers=output_buffers)\n</code></pre>"},{"location":"temp/multi_input_guide.html#23>_auto-split>_format","title":"2.3 Auto-Split Format","text":""},{"location":"temp/multi_input_guide.html#c_6","title":"C++","text":"<pre><code>// \ubaa8\ub4e0 \uc785\ub825\uc744 \uc5f0\uacb0\ud55c \ub2e8\uc77c \ubc84\ud37c\nstd::vector&lt;uint8_t&gt; concatenatedInput(ie.GetInputSize());\n// ... \ub370\uc774\ud130 \ucc44\uc6b0\uae30 ...\n\n// \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// \uc790\ub3d9 \ubd84\ud560\ub418\uc5b4 \ucc98\ub9ac\ub428 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\nauto outputs = ie.Run(concatenatedInput.data(), userArg, outputBuffer.data());\n</code></pre>"},{"location":"temp/multi_input_guide.html#python_6","title":"Python","text":"<pre><code># \ubaa8\ub4e0 \uc785\ub825\uc744 \uc5f0\uacb0\ud55c \ub2e8\uc77c \ubc30\uc5f4\nconcatenated_input = np.zeros(ie.get_input_size(), dtype=np.uint8)\n# ... \ub370\uc774\ud130 \ucc44\uc6b0\uae30 ...\n\n# \ucd9c\ub825 \ubc84\ud37c \uc0dd\uc131\noutput_buffers = [np.zeros(size, dtype=np.uint8) for size in ie.get_output_tensor_sizes()]\n\n# \uc790\ub3d9 \ubd84\ud560\ub418\uc5b4 \ucc98\ub9ac\ub428 (\uc0ac\uc6a9\uc790 \ucd9c\ub825 \ubc84\ud37c)\noutputs = ie.run(concatenated_input, output_buffers=output_buffers)\n</code></pre>"},{"location":"temp/multi_input_guide.html#multi-input>_batch","title":"Multi-Input Batch \ucd94\ub860","text":""},{"location":"temp/multi_input_guide.html#explicit>_batch>_format","title":"Explicit Batch Format","text":"<p>\uac01 \ubc30\uce58 \uc544\uc774\ud15c\uc5d0 \ub300\ud574 \uba85\uc2dc\uc801\uc73c\ub85c \uc785\ub825 \ud150\uc11c\ub4e4\uc744 \uc81c\uacf5\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_guide.html#python_7","title":"Python","text":"<pre><code># List[List[np.ndarray]] \ud615\ud0dc\nbatch_inputs = [\n    [sample1_input1, sample1_input2],  # \uccab \ubc88\uc9f8 \uc0d8\ud50c\n    [sample2_input1, sample2_input2],  # \ub450 \ubc88\uc9f8 \uc0d8\ud50c\n    [sample3_input1, sample3_input2]   # \uc138 \ubc88\uc9f8 \uc0d8\ud50c\n]\n\nbatch_outputs = [\n    [sample1_output1, sample1_output2],  # \uccab \ubc88\uc9f8 \uc0d8\ud50c \ucd9c\ub825 \ubc84\ud37c\n    [sample2_output1, sample2_output2],  # \ub450 \ubc88\uc9f8 \uc0d8\ud50c \ucd9c\ub825 \ubc84\ud37c\n    [sample3_output1, sample3_output2]   # \uc138 \ubc88\uc9f8 \uc0d8\ud50c \ucd9c\ub825 \ubc84\ud37c\n]\n\n# \ubc30\uce58 \ucd94\ub860\nresults = ie.run(batch_inputs, output_buffers=batch_outputs)\n</code></pre>"},{"location":"temp/multi_input_guide.html#c_7","title":"C++","text":"<pre><code>// \ubc30\uce58 \uc785\ub825 \ubc84\ud37c\ub4e4 (\uc5f0\uacb0\ub41c \ud615\ud0dc)\nstd::vector&lt;void*&gt; batchInputs = {sample1_ptr, sample2_ptr, sample3_ptr};\nstd::vector&lt;void*&gt; batchOutputs = {output1_ptr, output2_ptr, output3_ptr};\nstd::vector&lt;void*&gt; userArgs = {userArg1, userArg2, userArg3};\n\n// \ubc30\uce58 \ucd94\ub860\nauto results = ie.Run(batchInputs, batchOutputs, userArgs);\n</code></pre>"},{"location":"temp/multi_input_guide.html#flattened>_batch>_format","title":"Flattened Batch Format","text":"<p>\ubaa8\ub4e0 \uc785\ub825\uc744 \ud50c\ub798\ud2bc\ub41c \ud615\ud0dc\ub85c \uc81c\uacf5\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_guide.html#python_8","title":"Python","text":"<pre><code># \ud50c\ub798\ud2bc\ub41c \ud615\ud0dc: [sample1_input1, sample1_input2, sample2_input1, sample2_input2, ...]\nflattened_inputs = [\n    sample1_input1, sample1_input2,  # \uccab \ubc88\uc9f8 \uc0d8\ud50c\n    sample2_input1, sample2_input2,  # \ub450 \ubc88\uc9f8 \uc0d8\ud50c\n    sample3_input1, sample3_input2   # \uc138 \ubc88\uc9f8 \uc0d8\ud50c\n]\n\n# \uc790\ub3d9\uc73c\ub85c \ubc30\uce58\ub85c \uc778\uc2dd\ub428 (\uc785\ub825 \uac1c\uc218\uac00 \ubaa8\ub378 \uc785\ub825 \uac1c\uc218\uc758 \ubc30\uc218)\nresults = ie.run(flattened_inputs, output_buffers=batch_outputs)\n</code></pre>"},{"location":"temp/multi_input_guide.html#_2","title":"\ube44\ub3d9\uae30 \ucd94\ub860","text":""},{"location":"temp/multi_input_guide.html#1","title":"1. \ucf5c\ubc31 \uae30\ubc18 \ube44\ub3d9\uae30 \ucd94\ub860","text":""},{"location":"temp/multi_input_guide.html#c_8","title":"C++","text":"<pre><code>// \ucf5c\ubc31 \ud568\uc218 \ub4f1\ub85d\nie.RegisterCallback([](dxrt::TensorPtrs&amp; outputs, void* userArg) -&gt; int {\n    // \ucd9c\ub825 \ucc98\ub9ac\n    return 0;\n});\n\n// Dictionary format \ube44\ub3d9\uae30 \ucd94\ub860\nint jobId = ie.RunAsyncMultiInput(inputTensors, userArg);\n\n// Vector format \ube44\ub3d9\uae30 \ucd94\ub860\nint jobId = ie.RunAsyncMultiInput(inputPtrs, userArg);\n</code></pre>"},{"location":"temp/multi_input_guide.html#python_9","title":"Python","text":"<pre><code># \ucf5c\ubc31 \ud568\uc218 \uc815\uc758\ndef callback_handler(outputs, user_arg):\n    # \ucd9c\ub825 \ucc98\ub9ac \ubc0f \uac80\uc99d\n    return 0\n\n# \ucf5c\ubc31 \ub4f1\ub85d\nie.register_callback(callback_handler)\n\n# Dictionary format \ube44\ub3d9\uae30 \ucd94\ub860\njob_id = ie.run_async_multi_input(input_tensors, user_arg=user_arg)\n\n# Vector format \ube44\ub3d9\uae30 \ucd94\ub860\njob_id = ie.run_async(input_list, user_arg=user_arg)\n</code></pre>"},{"location":"temp/multi_input_guide.html#2","title":"2. \uac04\ub2e8\ud55c \ube44\ub3d9\uae30 \ucd94\ub860","text":""},{"location":"temp/multi_input_guide.html#c_9","title":"C++","text":"<pre><code>// \ub2e8\uc77c \ubc84\ud37c \ube44\ub3d9\uae30 \ucd94\ub860\nint jobId = ie.RunAsync(inputPtr, userArg);\n\n// \uacb0\uacfc \ub300\uae30\nauto outputs = ie.Wait(jobId);\n</code></pre>"},{"location":"temp/multi_input_guide.html#python_10","title":"Python","text":"<pre><code># \ub2e8\uc77c \ubc84\ud37c \ube44\ub3d9\uae30 \ucd94\ub860\njob_id = ie.run_async(input_buffer, user_arg=user_arg)\n\n# \uacb0\uacfc \ub300\uae30\noutputs = ie.wait(job_id)\n</code></pre>"},{"location":"temp/multi_input_guide.html#_3","title":"\uc7a5\uce58 \uac80\uc99d","text":"<p>Multi-input \ubaa8\ub378\uc5d0 \ub300\ud55c NPU \uc7a5\uce58 \uac80\uc99d\ub3c4 \uc9c0\uc6d0\ub429\ub2c8\ub2e4.</p>"},{"location":"temp/multi_input_guide.html#c_10","title":"C++","text":"<pre><code>// Dictionary format\nstd::map&lt;std::string, void*&gt; inputTensors;\ninputTensors[\"input1\"] = input1_data;\ninputTensors[\"input2\"] = input2_data;\n\nauto validationResults = ie.ValidateDeviceMultiInput(inputTensors, deviceId);\n\n// Vector format\nstd::vector&lt;void*&gt; inputPtrs = {input1_data, input2_data};\nauto validationResults = ie.ValidateDevice(inputPtrs, deviceId);\n</code></pre>"},{"location":"temp/multi_input_guide.html#python_11","title":"Python","text":"<pre><code># Dictionary format\ninput_tensors = {\"input1\": input1_array, \"input2\": input2_array}\nvalidation_results = ie.validate_device_multi_input(input_tensors, device_id=0)\n\n# Vector format  \ninput_list = [input1_array, input2_array]\nvalidation_results = ie.validate_device(input_list, device_id=0)\n</code></pre>"},{"location":"temp/multi_input_guide.html#_4","title":"\ucd9c\ub825 \uac80\uc99d","text":"<p>\uac15\ud654\ub41c \ucd9c\ub825 \uac80\uc99d \uae30\ub2a5\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4:</p>"},{"location":"temp/multi_input_guide.html#_5","title":"\uac80\uc99d \ud56d\ubaa9","text":"<ol> <li>\ucd9c\ub825 \uc874\uc7ac \uc5ec\ubd80: None \ub610\ub294 \ube48 \ub9ac\uc2a4\ud2b8 \uac80\uc0ac</li> <li>\ub370\uc774\ud130 \ud0c0\uc785: numpy.ndarray \ud0c0\uc785 \uac80\uc99d</li> <li>\ud150\uc11c \ud06c\uae30: \ube48 \ud150\uc11c (size=0) \uac80\uc0ac</li> <li>\ud615\ud0dc \uc720\ud6a8\uc131: \uc720\ud6a8\ud558\uc9c0 \uc54a\uc740 shape \uac80\uc0ac</li> <li>\uc218\uce58 \uc720\ud6a8\uc131: NaN, Inf \uac12 \uac80\uc0ac (Python)</li> <li>\ud3ec\uc778\ud130 \uc720\ud6a8\uc131: null \ud3ec\uc778\ud130 \uac80\uc0ac (C++)</li> <li>\ubc30\uce58 \uad6c\uc870: \ubc30\uce58 \ucd9c\ub825\uc758 \uc62c\ubc14\ub978 \uc911\ucca9 \uad6c\uc870 \uac80\uc99d</li> </ol>"},{"location":"temp/multi_input_guide.html#python_12","title":"\uac80\uc99d \uc608\uc81c (Python)","text":"<pre><code>def validate_outputs(outputs, expected_count, test_name):\n    # 1. \uae30\ubcf8 \uc874\uc7ac \uc5ec\ubd80 \uac80\uc0ac\n    if outputs is None or not isinstance(outputs, list):\n        return False\n\n    # 2. \ubc30\uce58/\ub2e8\uc77c \ucd9c\ub825 \uad6c\ubd84\n    is_batch = isinstance(outputs[0], list)\n\n    # 3. \uac01 \ud150\uc11c\ubcc4 \uc0c1\uc138 \uac80\uc99d\n    for output in outputs:\n        if not isinstance(output, np.ndarray):\n            return False\n        if output.size == 0:  # \ube48 \ud150\uc11c\n            return False\n        if len(output.shape) == 0:  # \uc798\ubabb\ub41c \ud615\ud0dc\n            return False\n        if np.any(np.isnan(output)) or np.any(np.isinf(output)):  # \uc218\uce58 \uc624\ub958\n            return False\n\n    return True\n</code></pre>"},{"location":"temp/multi_input_guide.html#_6","title":"\ud14c\uc2a4\ud2b8 \uc2dc\ub098\ub9ac\uc624","text":"<p>\ud604\uc7ac \uc608\uc81c \ucf54\ub4dc\ub294 \ub2e4\uc74c 10\uac00\uc9c0 \uc2dc\ub098\ub9ac\uc624\ub97c \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4:</p>"},{"location":"temp/multi_input_guide.html#no>_output>_buffer","title":"\ub2e8\uc77c \ucd94\ub860 (No Output Buffer)","text":"<ol> <li>Dictionary Format (No Buffer): \ub515\uc154\ub108\ub9ac \ud615\ud0dc, \uc790\ub3d9 \ud560\ub2f9</li> <li>Vector Format (No Buffer): \ubca1\ud130 \ud615\ud0dc, \uc790\ub3d9 \ud560\ub2f9</li> <li>Auto-Split (No Buffer): \uc790\ub3d9 \ubd84\ud560, \uc790\ub3d9 \ud560\ub2f9</li> </ol>"},{"location":"temp/multi_input_guide.html#with>_output>_buffer","title":"\ub2e8\uc77c \ucd94\ub860 (With Output Buffer)","text":"<ol> <li>Dictionary Format (With Buffer): \ub515\uc154\ub108\ub9ac \ud615\ud0dc, \uc0ac\uc6a9\uc790 \ubc84\ud37c</li> <li>Vector Format (With Buffer): \ubca1\ud130 \ud615\ud0dc, \uc0ac\uc6a9\uc790 \ubc84\ud37c</li> <li>Auto-Split (With Buffer): \uc790\ub3d9 \ubd84\ud560, \uc0ac\uc6a9\uc790 \ubc84\ud37c</li> </ol>"},{"location":"temp/multi_input_guide.html#_7","title":"\ubc30\uce58 \ucd94\ub860","text":"<ol> <li>Batch Explicit: \uba85\uc2dc\uc801 \ubc30\uce58 \ud615\ud0dc</li> <li>Batch Flattened: \ud50c\ub798\ud2bc\ub41c \ubc30\uce58 \ud615\ud0dc (Python\ub9cc)</li> </ol>"},{"location":"temp/multi_input_guide.html#_8","title":"\ube44\ub3d9\uae30 \ucd94\ub860","text":"<ol> <li>Async Callback: \ucf5c\ubc31 \uae30\ubc18 \ube44\ub3d9\uae30</li> <li>Simple Async: \uac04\ub2e8\ud55c \ube44\ub3d9\uae30</li> </ol>"},{"location":"temp/multi_input_guide.html#_9","title":"\uad8c\uc7a5\uc0ac\ud56d","text":"<ol> <li>Dictionary Format \uc0ac\uc6a9: \uac00\uc7a5 \uba85\ud655\ud558\uace0 \uc2e4\uc218\uac00 \uc801\uc74c</li> <li>\uc785\ub825 \ud150\uc11c \uc815\ubcf4 \ud655\uc778: <code>GetInputTensorNames()</code> \ub4f1\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \uc694\uad6c\uc0ac\ud56d \ud655\uc778</li> <li>\ub370\uc774\ud130 \uc5f0\uc18d\uc131: Python\uc5d0\uc11c\ub294 C-contiguous \ubc30\uc5f4 \uc0ac\uc6a9 \uad8c\uc7a5</li> <li>\uba54\ubaa8\ub9ac \uad00\ub9ac: \ucd9c\ub825 \ubc84\ud37c\ub97c \ubbf8\ub9ac \ud560\ub2f9\ud558\uc5ec \uc131\ub2a5 \ud5a5\uc0c1</li> <li>\uc5d0\ub7ec \ucc98\ub9ac: \uc798\ubabb\ub41c \uc785\ub825 \ud615\ud0dc\ub098 \ud06c\uae30\uc5d0 \ub300\ud55c \uc608\uc678 \ucc98\ub9ac</li> <li>\ucd9c\ub825 \uac80\uc99d: \uac15\ud654\ub41c \ucd9c\ub825 \uac80\uc99d\uc73c\ub85c False Positive \ubc29\uc9c0</li> </ol>"},{"location":"temp/multi_input_guide.html#_10","title":"\uc81c\uc57d\uc0ac\ud56d","text":"<ol> <li>\ube44\ub3d9\uae30 \ucd94\ub860: \ubc30\uce58 \ucd94\ub860\uc740 \uc9c0\uc6d0\ud558\uc9c0 \uc54a\uc74c (\ub2e8\uc77c \ucd94\ub860\ub9cc)</li> <li>\uc785\ub825 \uc21c\uc11c: Vector format \uc0ac\uc6a9 \uc2dc <code>GetInputTensorNames()</code> \uc21c\uc11c \uc900\uc218 \ud544\uc694</li> <li>\ub370\uc774\ud130 \ud0c0\uc785: \ubaa8\ub378\uc5d0\uc11c \uc694\uad6c\ud558\ub294 \ub370\uc774\ud130 \ud0c0\uc785\uacfc \uc77c\uce58\ud574\uc57c \ud568</li> <li>\ubc84\ud37c \ud06c\uae30: \uc785\ub825/\ucd9c\ub825 \ubc84\ud37c \ud06c\uae30\uac00 \ubaa8\ub378 \uc694\uad6c\uc0ac\ud56d\uacfc \uc77c\uce58\ud574\uc57c \ud568</li> <li>\uc7a5\uce58 \uac80\uc99d: Debug \ubaa8\ub4dc\ub85c \ucef4\ud30c\uc77c\ub41c \ubaa8\ub378\uc5d0\uc11c\ub9cc \uc9c0\uc6d0 </li> </ol>"},{"location":"temp/multi_input_guide_example_cpp.html","title":"Multi input guide example cpp","text":"<p>Of course! Here is a detailed explanation of the multi-input examples in Markdown (<code>.md</code>) format, based on the C++ code you provided.</p>"},{"location":"temp/multi_input_guide_example_cpp.html#dxrt>_multi-input>_inference>_examples","title":"DXRT Multi-Input Inference Examples","text":"<p>This document explains various methods for performing inference on multi-input models using the <code>dxrt::InferenceEngine</code>. The examples cover different input formats, synchronous and asynchronous execution, and batch processing.</p>"},{"location":"temp/multi_input_guide_example_cpp.html#1>_model>_information","title":"1. Model Information","text":"<p>Before running inference, it's useful to inspect the model's properties. The <code>printModelInfo</code> function shows how to query the inference engine for details about the model's input and output tensors.</p> <ul> <li><code>ie.IsMultiInputModel()</code>: Checks if the loaded model has multiple inputs.</li> <li><code>ie.GetInputTensorCount()</code>: Gets the number of input tensors.</li> <li><code>ie.GetInputTensorNames()</code>: Retrieves the names of all input tensors.</li> <li><code>ie.GetInputTensorSizes()</code>: Gets the size (in bytes) of each input tensor.</li> <li><code>ie.GetOutputTensorNames()</code> / <code>ie.GetOutputTensorSizes()</code>: Provide similar information for output tensors.</li> </ul> <pre><code>void printModelInfo(dxrt::InferenceEngine&amp; ie) {\n    if (ie.IsMultiInputModel()) {\n        std::cout &lt;&lt; \"Input tensor count: \" &lt;&lt; ie.GetInputTensorCount() &lt;&lt; std::endl;\n        auto inputNames = ie.GetInputTensorNames();\n        auto inputSizes = ie.GetInputTensorSizes();\n        for (size_t i = 0; i &lt; inputNames.size(); ++i) {\n            std::cout &lt;&lt; \"  \" &lt;&lt; inputNames[i] &lt;&lt; \": \" &lt;&lt; inputSizes[i] &lt;&lt; \" bytes\" &lt;&lt; std::endl;\n        }\n    }\n}\n</code></pre>"},{"location":"temp/multi_input_guide_example_cpp.html#2>_synchronous>_single>_inference","title":"2. Synchronous Single Inference","text":"<p>These examples demonstrate different ways to run a single inference request synchronously.</p>"},{"location":"temp/multi_input_guide_example_cpp.html#input>_formats","title":"Input Formats","text":""},{"location":"temp/multi_input_guide_example_cpp.html#a>_dictionary>_format>_stdmapstdstring>_void","title":"A. Dictionary Format (<code>std::map&lt;std::string, void*&gt;</code>)","text":"<p>This is the most robust method. You provide a map where keys are the tensor names and values are pointers to the input data. This format is not sensitive to the order of tensors.</p> <ul> <li>API: <code>ie.RunMultiInput(inputTensors)</code></li> <li>Use Case: Recommended for clarity and to avoid errors from tensor reordering.</li> </ul> <pre><code>// Create input data\nstd::map&lt;std::string, void*&gt; inputTensors;\ninputTensors[\"input_1\"] = inputData1.data();\ninputTensors[\"input_2\"] = inputData2.data();\n\n// Run inference\nauto outputs = ie.RunMultiInput(inputTensors);\n</code></pre>"},{"location":"temp/multi_input_guide_example_cpp.html#b>_vector>_format>_stdvectorvoid","title":"B. Vector Format (<code>std::vector&lt;void*&gt;</code>)","text":"<p>You provide a vector of pointers to the input data. The order of pointers in the vector must match the order returned by <code>ie.GetInputTensorNames()</code>.</p> <ul> <li>API: <code>ie.RunMultiInput(inputPtrs)</code></li> <li>Use Case: When tensor order is known and fixed. Can be slightly more performant than the map-based approach due to less overhead.</li> </ul> <pre><code>// Create input data in the correct order\nstd::vector&lt;void*&gt; inputPtrs;\ninputPtrs.push_back(inputData1.data()); // Corresponds to first name in GetInputTensorNames()\ninputPtrs.push_back(inputData2.data()); // Corresponds to second name\n\n// Run inference\nauto outputs = ie.RunMultiInput(inputPtrs);\n</code></pre>"},{"location":"temp/multi_input_guide_example_cpp.html#c>_auto-split>_concatenated>_buffer","title":"C. Auto-Split Concatenated Buffer","text":"<p>You provide a single, contiguous buffer containing all input data concatenated together. The engine automatically splits this buffer into the correct tensor inputs based on their sizes. The concatenation order must match the order from <code>ie.GetInputTensorNames()</code>.</p> <ul> <li>API: <code>ie.Run(concatenatedInput.data())</code></li> <li>Use Case: Efficient when input data is already in a single block or when interfacing with systems that provide data this way.</li> </ul> <pre><code>// Create a single buffer with all input data concatenated\nauto concatenatedInput = createDummyInput(ie.GetInputSize());\n\n// Run inference\nauto outputs = ie.Run(concatenatedInput.data());\n</code></pre>"},{"location":"temp/multi_input_guide_example_cpp.html#output>_buffer>_management","title":"Output Buffer Management","text":"<p>For each synchronous method, you can either let the engine allocate output memory automatically or provide a pre-allocated buffer for performance gains.</p> <ul> <li> <p>Auto-Allocated Output (No Buffer Provided): Simpler to use. The engine returns smart pointers to newly allocated memory.</p> <pre><code>// Engine allocates and manages output memory\nauto outputs = ie.RunMultiInput(inputTensors);\n</code></pre> </li> <li> <p>User-Provided Output Buffer: More performant as it avoids repeated memory allocations. The user is responsible for allocating a buffer of size <code>ie.GetOutputSize()</code>.</p> <pre><code>// User allocates the output buffer\nstd::vector&lt;uint8_t&gt; outputBuffer(ie.GetOutputSize());\n\n// Run inference, placing results in the provided buffer\nauto outputs = ie.RunMultiInput(inputTensors, nullptr, outputBuffer.data());\n</code></pre> </li> </ul>"},{"location":"temp/multi_input_guide_example_cpp.html#3>_synchronous>_batch>_inference","title":"3. Synchronous Batch Inference","text":"<p>For processing multiple inputs at once to maximize throughput, you can use the batch inference API. This is more efficient than running single inferences in a loop.</p> <ul> <li>API: <code>ie.Run(batchInputPtrs, batchOutputPtrs, userArgs)</code></li> <li>Input: A vector of pointers, where each pointer is a concatenated buffer for one sample in the batch.</li> <li>Output: A vector of pointers, where each pointer is a pre-allocated buffer for the corresponding sample's output.</li> </ul> <pre><code>int batchSize = 3;\nstd::vector&lt;void*&gt; batchInputPtrs;\nstd::vector&lt;void*&gt; batchOutputPtrs;\n\n// Prepare input and output buffers for each sample in the batch\nfor (int i = 0; i &lt; batchSize; ++i) {\n    // Each input is a full concatenated buffer\n    batchInputData[i] = createDummyInput(ie.GetInputSize());\n    batchInputPtrs.push_back(batchInputData[i].data());\n\n    // Pre-allocate output buffer for each sample\n    batchOutputData[i].resize(ie.GetOutputSize());\n    batchOutputPtrs.push_back(batchOutputData[i].data());\n}\n\n// Run batch inference\nauto batchOutputs = ie.Run(batchInputPtrs, batchOutputPtrs);\n</code></pre>"},{"location":"temp/multi_input_guide_example_cpp.html#4>_asynchronous>_inference","title":"4. Asynchronous Inference","text":"<p>Asynchronous APIs allow you to submit inference requests without blocking the calling thread. The results are returned later via a callback function. This is ideal for applications that need to remain responsive, such as those with a user interface.</p> <ul> <li>APIs:<ul> <li><code>ie.RunAsyncMultiInput(inputTensors, userArg)</code></li> <li><code>ie.RunAsync(concatenatedInput.data(), userArg)</code></li> </ul> </li> <li>Callback Registration: <code>ie.RegisterCallback(callback_function)</code></li> </ul> <p>The <code>AsyncInferenceHandler</code> class demonstrates how to manage state across multiple asynchronous calls.</p> <ol> <li>Register a Callback: Provide a function that the engine will call upon completion of each async request. The callback receives the output tensors and a <code>userArg</code> pointer for context.</li> <li>Submit Requests: Call an <code>RunAsync</code> variant. This call returns immediately with a job ID.</li> <li>Process in Callback: The callback function is executed in a separate worker thread. Here, you can process the results. It's crucial to ensure thread safety if you modify shared data.</li> </ol> <pre><code>// 1. Create a handler and register its callback method\nAsyncInferenceHandler handler(asyncCount);\nie.RegisterCallback([&amp;handler](dxrt::TensorPtrs&amp; outputs, void* userArg) -&gt; int {\n    return handler.callback(outputs, userArg);\n});\n\n// 2. Submit multiple async requests in a loop\nfor (int i = 0; i &lt; asyncCount; ++i) {\n    void* userArg = reinterpret_cast&lt;void*&gt;(static_cast&lt;uintptr_t&gt;(i));\n    // Each call is non-blocking\n    ie.RunAsyncMultiInput(asyncInputTensors[i], userArg);\n}\n\n// 3. Wait for all callbacks to complete\nhandler.waitForCompletion();\n\n// 4. Clear the callback when done\nie.RegisterCallback(nullptr);\n</code></pre>"},{"location":"temp/multi_input_guide_examples_python.html","title":"Multi input guide examples python","text":"<p>Of course! Here is a guide for the Python examples, created with reference to the C++ guide and formatted according to your request.</p>"},{"location":"temp/multi_input_guide_examples_python.html#1>_model>_information","title":"1. Model Information","text":"<p>Before running inference, it's useful to inspect the model's properties. The <code>print_model_info</code> function in the example script shows how to query the inference engine for details about the model's input and output tensors.</p> <ul> <li><code>ie.is_multi_input_model()</code>: Checks if the loaded model has multiple inputs.</li> <li><code>ie.get_input_tensor_count()</code>: Gets the number of input tensors.</li> <li><code>ie.get_input_tensor_names()</code>: Retrieves the names of all input tensors.</li> <li><code>ie.get_input_tensor_sizes()</code>: Gets the size (in bytes) of each input tensor.</li> <li><code>ie.get_output_tensor_names()</code> / <code>ie.get_output_tensor_sizes()</code>: Provide similar information for output tensors.</li> </ul> <pre><code>def print_model_info(ie: InferenceEngine) -&gt; None:\n    if ie.is_multi_input_model():\n        print(f\"Input tensor count: {ie.get_input_tensor_count()}\")\n        input_names = ie.get_input_tensor_names()\n        input_sizes = ie.get_input_tensor_sizes()\n        for i, name in enumerate(input_names):\n            print(f\"  {name}: {input_sizes[i]} bytes\")\n</code></pre>"},{"location":"temp/multi_input_guide_examples_python.html#2>_synchronous>_single>_inference","title":"2. Synchronous Single Inference","text":"<p>These examples demonstrate different ways to run a single inference request synchronously.</p>"},{"location":"temp/multi_input_guide_examples_python.html#input>_formats","title":"Input Formats","text":""},{"location":"temp/multi_input_guide_examples_python.html#a>_dictionary>_format>_dictstr>_npndarray","title":"A. Dictionary Format (<code>Dict[str, np.ndarray]</code>)","text":"<p>This is the most robust method. You provide a dictionary where keys are the tensor names and values are the <code>numpy</code> arrays. This format is not sensitive to the order of tensors.</p> <ul> <li>API: <code>ie.run_multi_input(input_tensors)</code></li> <li>Use Case: Recommended for clarity and to avoid errors from tensor reordering.</li> </ul> <pre><code># Create input data\ninput_names = ie.get_input_tensor_names()\ninput_sizes = ie.get_input_tensor_sizes()\ninput_tensors = {name: create_dummy_input(size) for name, size in zip(input_names, input_sizes)}\n\n# Run inference\noutputs = ie.run_multi_input(input_tensors)\n</code></pre>"},{"location":"temp/multi_input_guide_examples_python.html#b>_list>_format>_listnpndarray","title":"B. List Format (<code>List[np.ndarray]</code>)","text":"<p>You provide a list of <code>numpy</code> arrays. The order of arrays in the list must match the order returned by <code>ie.get_input_tensor_names()</code>.</p> <ul> <li>API: <code>ie.run(input_list)</code></li> <li>Use Case: When tensor order is known and fixed. Can be slightly more performant than the dictionary-based approach due to less overhead.</li> </ul> <pre><code># Create input data in the correct order\ninput_sizes = ie.get_input_tensor_sizes()\ninput_list = [create_dummy_input(size) for size in input_sizes]\n\n# Run inference\noutputs = ie.run(input_list)\n</code></pre>"},{"location":"temp/multi_input_guide_examples_python.html#c>_auto-split>_concatenated>_buffer","title":"C. Auto-Split Concatenated Buffer","text":"<p>You provide a single, contiguous <code>numpy</code> array containing all input data concatenated together. The engine automatically splits this buffer into the correct tensor inputs based on their sizes. The concatenation order must match the order from <code>ie.get_input_tensor_names()</code>.</p> <ul> <li>API: <code>ie.run(concatenated_input)</code></li> <li>Use Case: Efficient when input data is already in a single block or when interfacing with systems that provide data this way.</li> </ul> <pre><code># Create a single buffer with all input data concatenated\ntotal_input_size = ie.get_input_size()\nconcatenated_input = create_dummy_input(total_input_size)\n\n# Run inference\noutputs = ie.run(concatenated_input)\n</code></pre>"},{"location":"temp/multi_input_guide_examples_python.html#output>_buffer>_management","title":"Output Buffer Management","text":"<p>For each synchronous method, you can either let the engine allocate output memory automatically or provide pre-allocated buffers for performance gains.</p> <ul> <li> <p>Auto-Allocated Output (No Buffer Provided): Simpler to use. The engine returns a new list of <code>numpy</code> arrays.</p> <pre><code># Engine allocates and manages output memory\noutputs = ie.run_multi_input(input_tensors)\n</code></pre> </li> <li> <p>User-Provided Output Buffers: More performant as it avoids repeated memory allocations. The user is responsible for creating a list of <code>numpy</code> arrays with the correct sizes.</p> <pre><code># User creates the output buffers\noutput_sizes = ie.get_output_tensor_sizes()\noutput_buffers = [np.zeros(size, dtype=np.uint8) for size in output_sizes]\n\n# Run inference, placing results in the provided buffers\noutputs = ie.run_multi_input(input_tensors, output_buffers=output_buffers)\n</code></pre> </li> </ul>"},{"location":"temp/multi_input_guide_examples_python.html#3>_synchronous>_batch>_inference","title":"3. Synchronous Batch Inference","text":"<p>For processing multiple inputs at once to maximize throughput, you can use the batch inference capabilities of the <code>run</code> method. This is more efficient than running single inferences in a loop.</p>"},{"location":"temp/multi_input_guide_examples_python.html#a>_explicit>_batch>_format>_listlistnpndarray","title":"A. Explicit Batch Format (<code>List[List[np.ndarray]]</code>)","text":"<p>This is the clearest way to represent a batch. The input is a list of lists, where the outer list represents the batch and each inner list contains all input tensors for a single sample.</p> <ul> <li>API: <code>ie.run(batch_inputs, output_buffers=...)</code></li> <li>Input: A <code>List[List[np.ndarray]]</code>.</li> <li>Output: A <code>List[List[np.ndarray]]</code>.</li> </ul> <pre><code>batch_size = 3\ninput_sizes = ie.get_input_tensor_sizes()\nbatch_inputs = []\nfor i in range(batch_size):\n    sample_inputs = [create_dummy_input(size) for size in input_sizes]\n    batch_inputs.append(sample_inputs)\n\n# Output buffers must also match the batch structure\n# ... create batch_outputs ...\n\n# Run batch inference\nresults = ie.run(batch_inputs, output_buffers=batch_outputs)\n</code></pre>"},{"location":"temp/multi_input_guide_examples_python.html#b>_flattened>_batch>_format>_listnpndarray","title":"B. Flattened Batch Format (<code>List[np.ndarray]</code>)","text":"<p>As a convenience, the API can also accept a single \"flattened\" list of <code>numpy</code> arrays. The total number of arrays must be a multiple of the model's input tensor count. The engine will automatically group them into batches.</p> <ul> <li>API: <code>ie.run(flattened_inputs, output_buffers=...)</code></li> <li>Input: A <code>List[np.ndarray]</code> containing <code>batch_size * num_input_tensors</code> arrays.</li> <li>Output: The result is still returned in the explicit batch format (<code>List[List[np.ndarray]]</code>).</li> </ul> <pre><code>batch_size = 3\ninput_sizes = ie.get_input_tensor_sizes()\nflattened_inputs = []\nfor i in range(batch_size):\n    for size in input_sizes:\n        flattened_inputs.append(create_dummy_input(size))\n\n# ... create flattened_output_buffers ...\n\n# Run batch inference\nresults = ie.run(flattened_inputs, output_buffers=flattened_output_buffers)\n</code></pre>"},{"location":"temp/multi_input_guide_examples_python.html#4>_asynchronous>_inference","title":"4. Asynchronous Inference","text":"<p>Asynchronous APIs allow you to submit inference requests without blocking the calling thread. The results are returned later via a callback function. This is ideal for applications that need to remain responsive.</p> <ul> <li>APIs:<ul> <li><code>ie.run_async_multi_input(input_tensors, user_arg=...)</code></li> <li><code>ie.run_async(input_data, user_arg=...)</code></li> </ul> </li> <li>Callback Registration: <code>ie.register_callback(callback_function)</code></li> </ul> <p>The <code>AsyncInferenceHandler</code> class in the example demonstrates how to manage state across multiple asynchronous calls.</p> <ol> <li>Register a Callback: Provide a function that the engine will call upon completion of each async request. The callback receives the output arrays and a <code>user_arg</code> for context.</li> <li>Submit Requests: Call an <code>run_async</code> variant. This call returns immediately with a job ID.</li> <li>Process in Callback: The callback function is executed in a separate worker thread. Here, you can process the results. It's crucial to ensure thread safety (e.g., using a <code>threading.Lock</code>) if you modify shared data.</li> </ol> <pre><code># 1. Create a handler and register its callback method\nhandler = AsyncInferenceHandler(async_count)\nie.register_callback(handler.callback)\n\n# 2. Submit multiple async requests in a loop\nfor i in range(async_count):\n    user_arg = f\"async_sample_{i}\"\n    # Each call is non-blocking\n    job_id = ie.run_async_multi_input(input_tensors, user_arg=user_arg)\n\n# 3. Wait for all callbacks to complete\nhandler.wait_for_completion()\n\n# 4. Clear the callback when done\nie.register_callback(None)\n</code></pre>"}]}