{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":"<p>DX-RT is DEEPX Runtime SDK for AI inference based on DEEPX devices. It supports pre-built models from DEEPX model zoo, and compiled models by DXCOM(DEEPX Compiler SDK).  </p>"},{"location":"index.html#supported>_devices","title":"Supported devices","text":"<p>DX-RT provides common inference framework based on 2 kinds of inference mode.  </p> <ul> <li>Accelerator mode : inference by PCIe interface  </li> <li>Standalone mode : inference by direct AXI/APB interface  </li> </ul> Device Mode DX_V1 Standalone DX_V3 Standalone DX_M1 Accelerator DX_H1 Accelerator"},{"location":"index.html#resources","title":"Resources","text":""},{"location":"index.html#model>_zoo>_prebuilt>_models","title":"Model zoo (Prebuilt models)","text":"<p>Official modelzoo is in preparation.  </p>"},{"location":"index.html#documents","title":"Documents","text":"<p>Official website documents are in preparation. Currently, documents are provided with limited rights. Please consult with our contact point person. In other way, you can generate documents from repository (using markdown files in <code>docs</code>).  </p> <ul> <li>python&gt;=3.9 is needed. <pre><code># install MkDocs\npip install mkdocs mkdocs-material mkdocs-video pymdown-extensions\n# generate html to directory \"docs_generated\"\nmkdocs build\n</code></pre> You can also generate API reference using doxygen. <pre><code># install doxygen\nsudo apt install doxygen graphviz\n# generate API reference html\ncd docs/cpp_api\ndoxygen Doxyfile\n</code></pre></li> </ul>"},{"location":"API-Reference.html","title":"API Reference","text":""},{"location":"API-Reference.html#install>_doxygen","title":"Install doxygen","text":"<pre><code>sudo apt install doxygen graphviz\n</code></pre>"},{"location":"API-Reference.html#generate>_api>_reference","title":"Generate API reference","text":"<pre><code>cd docs/cpp_api\ndoxygen Doxyfile\n</code></pre>"},{"location":"Build.html","title":"Build","text":""},{"location":"Build.html#compile>_definitions","title":"Compile Definitions","text":"<p>You can set global compile definitions in build.cfg to use in your application(It doesn't affect to DXRT libraries). Following means, <code>#define EXAMPLE_FLAG 1</code> in compile time. </p><pre><code>EXAMPLE_FLAG=1\n</code></pre>"},{"location":"Build.html#options","title":"Options","text":"<p>You can configure DXRT options in cmake/dxrt.cfg.cmake <code>USE_ORT</code> : Use ONNX runtime for NN ops that NPU does not support. <code>USE_PYTHON</code> : Enable python RT. <code>USE_SERVICE</code> : Use service to support multi-process.  </p> <pre><code>option(USE_ORT \"Use ONNX Runtime\" OFF)\noption(USE_PYTHON \"Use Python\" OFF)\noption(USE_SERVICE \"Use Service\" OFF)\noption(USE_SHARED_DXRT_LIB \"Build for DXRT Shared Library\" ON)\n</code></pre>"},{"location":"Build.html#cross-compile>_setup","title":"Cross-compile Setup","text":"<p>DXRT supports 3 kinds of CPU Architecture - aarch64, riscv64, x86_64. Set compiler toolchain path in <code>cmake/toolchain.&lt;CMAKE_SYSTEM_PROCESSOR&gt;.cmake</code> Example :  cross-compile </p><pre><code>SET(CMAKE_C_COMPILER      /usr/bin/aarch64-linux-gnu-gcc )\nSET(CMAKE_CXX_COMPILER    /usr/bin/aarch64-linux-gnu-g++ )\nSET(CMAKE_LINKER          /usr/bin/aarch64-linux-gnu-ld  )\nSET(CMAKE_NM              /usr/bin/aarch64-linux-gnu-nm )\nSET(CMAKE_OBJCOPY         /usr/bin/aarch64-linux-gnu-objcopy )\nSET(CMAKE_OBJDUMP         /usr/bin/aarch64-linux-gnu-objdump )\nSET(CMAKE_RANLIB          /usr/bin/aarch64-linux-gnu-ranlib )\n</code></pre> Example : riscv64 cross-compile <pre><code>set(CMAKE_C_COMPILER /usr/bin/riscv64-linux-gnu-gcc)\nset(CMAKE_CXX_COMPILER /usr/bin/riscv64-linux-gnu-g++)\n</code></pre>"},{"location":"Build.html#build>_guide","title":"Build guide","text":""},{"location":"Build.html#non-cross-compile>_case","title":"non-cross-compile case","text":"<p>To build DXRT for the host system, execute the build script with the appropriate installation path. It is recommended to set the installation path to a location commonly referenced in the OS, such as <code>/usr/local</code>. </p><pre><code>./build.sh --install /usr/local\n</code></pre>"},{"location":"Build.html#cross-compile>_case","title":"cross-compile case","text":"<p>To cross-compile DXRT for a target system, target CPU architecture should be configured. (If not specified, target CPU will be set to the host system's CPU) </p><pre><code>./build.sh --arch &lt;target_cpu&gt;\n</code></pre> <p>Please refer to following options. </p><pre><code>./build.sh [ options ]\n    --help     show this help\n    --clean    clean build\n    --verbose  show build commands\n    --type     cmake build type : [ Release, Debug, RelWithDebInfo ]\n    --arch     target CPU architecture : [ x86_64, aarch64, riscv64 ]\n    --install  install path\n    --uninstall  uninstall dx-rt files\n</code></pre> Examples: <pre><code>./build.sh --arch aarch64\n./build.sh --arch riscv64\n./build.sh --arch x86_64\n</code></pre> Build directories and related output files will be generated according to the target CPU (e.g., <code>build_aarch64</code>, <code>build_riscv64</code>, ...). Demo application binary files can be found in <code>&lt;build directory&gt;</code>/bin/, and <code>bin</code>. <pre><code>&lt;build directory&gt;/bin/\n\u251c\u2500\u2500 dxrt-cli\n\u251c\u2500\u2500 dxrtd\n\u251c\u2500\u2500 run_model\n\u251c\u2500\u2500 parse_model\n</code></pre>"},{"location":"Examples.html","title":"Examples","text":"<p>Please find our demo applications implemented based on the Inference Guide.</p>"},{"location":"Examples.html#parse>_model","title":"Parse Model","text":"<p>Parse model, and show detailed model information (from <code>bin/parse_model.cpp</code>)  </p> <p>-m, --model     model path  -h, --help      show help </p><pre><code>parse_model -m &lt;model_dir&gt;\n</code></pre> <pre><code>$./parse_model -m model.dxnn\nmodelPath: /.../model.dxnn\nDXNN Model Ver. : 6\n...\n[  ] -&gt; npu_0 -&gt; [ ]\n  inputs\n     -&gt; npu_0\n      images\n  outputs\n    npu_0 -&gt;\n       ...\n  Task[0] npu_0, NPU, 117389760 bytes (input 786432, output 131072)\n    inputs\n      images, UINT8, [1, 512, 512, 3 ], 0\n    outputs\n      DX_tensor_5288, BBOX, [UNKNOWN ], 0\n</code></pre>"},{"location":"Examples.html#run>_model","title":"Run Model","text":"<p>Simple model run demo, which measure inference time, and check output data integrity (from <code>bin/run_model.cpp</code>)  </p> <p>-c, --config    config json file  -m, --model     model path  -i, --input     input data file  -o, --output    file to save output data  -r, --ref       reference output data file to compare  -l, --loop      loop test  -h, --help      show help </p><pre><code>run_model -m &lt;model_dir&gt; -i &lt;input bin.&gt; -o &lt;output bin.&gt; -r &lt;reference output bin.&gt; -l &lt;number of loops&gt;\n</code></pre> <pre><code>$ run_model -m /.../model.dxnn -i /.../input.bin -l 100\n\nmodelFile: /.../model.dxnn\ninputFile: /.../input.bin\noutputFile: output.bin\nbenchmark: 0\nloops: 100\nRun model target mode : Benchmark Mode\n============================================================================================\n* Processing File : /.../input.bin\n* Output Saved As : output.bin\n* Benchmark Result(3 Cores)\n  - FPS : 323.331604\n============================================================================================\n</code></pre>"},{"location":"Examples.html#firmware>_interface>_dxrt-cli>_tool","title":"Firmware Interface DXRT-CLI tool","text":"<p>Read device status, and handle them by commandline interface (only for accelerator device)  </p> <pre><code>dxrt-cli &lt;option&gt; &lt;argument&gt;\n</code></pre> <p>-s, --status             Get device status -i, --info               Get device info -m, --monitor arg        Monitoring device status every [arg] seconds -r, --reset arg          Reset device(0: reset only NPU, 1: reset entire device) (default: 0) -d, --device arg         Device ID (if not specified, CLI commands will be sent to all devices.) (default: -1) -u, --fwupdate arg       Update firmware with deepx firmware file. sub-option : [force:force update, unreset:device unreset(default:reset)] -w, --fwupload arg       Upload firmware with deepx firmware file.[2nd_boot/rtos] -g, --fwversion arg      Get firmware version with deepx firmware file -p, --dump arg           Dump device internals to a file -l, --fwlog arg          Extract firmware logs to a file -h, --help               Print usage  </p> <p></p><pre><code>$ dxrt-cli --status\nDXRT v2.0.x\n=======================================================\n * Device 0: M1, Accelator type\n---------------------   Version   ---------------------\n * RT Driver version   : v1.1.0\n * PCIe Driver version : v1.1.0\n-------------------------------------------------------\n * FW version          : v1.5.5\n--------------------- Device Info ---------------------\n * Memory : LPDDR5 5500 MHz, 2.98GiB\n * Board  : M.2, Rev 10.0\n * PCIe   : Gen3 X4 [04:00:00]\n\nNPU 0: voltage 750 mV, clock 1000 MHz, temperature 40'C\nNPU 1: voltage 750 mV, clock 1000 MHz, temperature 40'C\nNPU 2: voltage 750 mV, clock 1000 MHz, temperature 40'C\ndvfs Disabled\n=======================================================\n</code></pre> <pre><code>$ dxrt-cli --reset 0\nDXRT v2.0.x\n    Device 0 reset by option 0\n</code></pre> <pre><code>** Please reboot when updating firmware. **\n$ dxrt-cli --fwupdate fw.bin\nDXRT v2.0.x\n============ FW Binary Information ============\nSignature   : DEEPX GENESIS-M\nTotal Image : 6\nBoard Type  : 2\nDDR Type    : 2\nFirmware Ver: 1.5.2\n    Device 0 update firmware[1.5.2] by /.../1.5.2/fw.bin, SubCmd:0 : SUCCESS\n</code></pre> <pre><code>$ dxrt-cli -c 750 -c 1000 -c 750 -c 1000 -c 750 -c 1000\nDXRT v2.0.x\n    Device 0 update firmware config by 6\n</code></pre> <pre><code>$ dxrt-cli -c 750 -c 1000 -c 750 -c 1000 -c 750 -c 1000\nDXRT v2.0.x\n    Device 0 update firmware config by 6\n</code></pre> <pre><code>$ dxrt-cli -C firmware_settings.json\nDXRT v2.0.x\nnpuID[@0]:: voltage - 750 / freq - 1000\nnpuID[@1]:: voltage - 750 / freq - 1000\nnpuID[@2]:: voltage - 750 / freq - 1000\nnpu throttling threshold 1 - 60\nnpu throttling threshold 2 - 90\nnpu emergency threshold - 120\nnpu throttling enabled - 1\n    Device 0 update firmware config by 10\n</code></pre> <pre><code>$ dxrt-cli -m 1\nDXRT v2.0.x\nNPU 0: voltage 750 mV, clock 1000 MHz, temperature 39'C\nNPU 1: voltage 750 mV, clock 1000 MHz, temperature 39'C\nNPU 2: voltage 750 mV, clock 1000 MHz, temperature 38'C\ndvfs Disabled\n=======================================================\nNPU 0: voltage 750 mV, clock 1000 MHz, temperature 39'C\nNPU 1: voltage 750 mV, clock 1000 MHz, temperature 39'C\nNPU 2: voltage 750 mV, clock 1000 MHz, temperature 39'C\ndvfs Disabled\n=======================================================\nNPU 0: voltage 750 mV, clock 1000 MHz, temperature 39'C\nNPU 1: voltage 750 mV, clock 1000 MHz, temperature 38'C\nNPU 2: voltage 750 mV, clock 1000 MHz, temperature 38'C\ndvfs Disabled\n=======================================================\n</code></pre>"},{"location":"Getting-Started.html","title":"Prerequisite","text":""},{"location":"Getting-Started.html#prerequisite","title":"Prerequisite","text":"<p>To proceed with the details below, installation must first be completed. Ensure that the necessary dependencies and build tools are installed on the host and target systems.  </p>"},{"location":"Getting-Started.html#0>_build","title":"0. Build","text":""},{"location":"Getting-Started.html#non-cross-compile>_case","title":"non-cross-compile case","text":"<p>To build DXRT for the host system, execute the build script with the appropriate installation path. It is recommended to set the installation path to a location commonly referenced in the OS, such as <code>/usr/local</code>. </p><pre><code>./build.sh --install /usr/local\n</code></pre>"},{"location":"Getting-Started.html#cross-compile>_case","title":"cross-compile case","text":"<p>To cross-compile DXRT for a target system, please refer to <code>Build</code> chapter. </p><pre><code>./build.sh --arch &lt;target_cpu&gt;\n</code></pre>"},{"location":"Getting-Started.html#1>_prepare>_a>_model","title":"1. Prepare a model","text":"<p>Pick one prebuilt model from ModelZoo. Since ModelZoo is still not open, prebuilt model files will be provided separately. </p><pre><code>(You will see the file structure as below )  \nyolov5s_640\n\u2514\u2500\u2500 graph.dxnn\n</code></pre>"},{"location":"Getting-Started.html#2>_run>_simple>_cli>_to>_device","title":"2. Run simple CLI to device","text":"<p>Check basic device interface as following. </p><pre><code>./bin/dxrt-cli --status\n</code></pre> <pre><code>DXRT v2.0.3\nDevice 0: M1, Accelator type\nMemory: LPDDR5 5500 MHz, 2.98GiB\nBoard: M.2, Rev 10.0\nFW v1.5.1\n\nNPU 0: voltage 750 mV, clock 1000 MHz, temperature 39'C\nNPU 1: voltage 750 mV, clock 1000 MHz, temperature 39'C\nNPU 2: voltage 750 mV, clock 1000 MHz, temperature 39'C\n</code></pre>"},{"location":"Getting-Started.html#3>_run>_simple>_benchmark>_with>_the>_model","title":"3. Run simple benchmark with the model","text":"<pre><code>./bin/run_model -m &lt;model directory&gt;\n</code></pre> If the application completes successfully, you can see detection result as following. <pre><code>Start benchmark.\nCompleted benchmark.\nData test: Sequential\n-----------------------------------\nmodel\n  Inference time : 4.41787ms\n  FPS : 226.354\n  Data Integrity Check : SKIP (0 pass, 0 fail )\n-----------------------------------\n...\n</code></pre>"},{"location":"High-Level-Design.html","title":"High Level Design","text":""},{"location":"High-Level-Design.html#overview","title":"Overview","text":"<p>Compiled NN model is imported as an <code>InferenceEngine</code> instance, just run it. <code>InferenceEngine</code> will do : - initialization of DEEPX devices - memory management for inference - real-time inference job scheduling based on optimized NPU/CPU interaction  </p> <p>Before running <code>InferenceEngine</code>, <code>InferenceOption</code> should be established, which describes what device to use, how much resources to use. Also, example applications provide typical pre/post-processing attached to OpenCV for NN inference.  </p> <p></p>"},{"location":"High-Level-Design.html#static>_architecture","title":"Static Architecture","text":"<p>DXRT Software stack is partitioned to 4 layers.</p> <ul> <li>The NN API layer is designed to provide public inference APIs, tensor processing APIs for user applications, and Python APIs.</li> <li>The Library layer is designed for actual implementation of runtime framework in userspace, and partitioned to various C++ classes that abstracts resources for inference. It also plays an important role in exchanging data related to inference with device drivers and service.</li> <li>The Service layer is designed to support multiple processes and multiple devices, and is configured to communicate with each process to allocate inference to the most suitable device.</li> <li>The Device driver layer is responsible for communicating with firmware, managing inference, and HW-dependent implementation of NPU or PCIe functions (Device driver repository will be split from DXRT in future).  </li> </ul> <p></p>"},{"location":"High-Level-Design.html#inference>_concept","title":"Inference Concept","text":"<p>In point of high-level view, <code>InferenceEngine</code> abstracts all the steps of inference. When <code>InferenceEngine</code> is created, compiled model is loaded to one <code>Model</code>, which is partitioned and converted to graph of <code>Task</code>. <code>Task</code> represents NN operations information, and describes where(NPU or CPU) should it be done. Model parameters for each task will be written to proper device memory, and required memory allocation in both side(host, device) will be done automatically. Then, tensors are created based on the model parameters, and attached to each tasks. So, <code>Worker</code> instances will execute the task graph asynchronously internally. As a result, selected <code>Device</code> or <code>CpuHandle</code> by <code>Worker</code> will perform NN computations(if needed, memory write/read operations of tensors will be occured).  </p>"},{"location":"High-Level-Design.html#compiler>_interface","title":"Compiler Interface","text":"<p>Compiled model consists of NPU task, and CPU task parameters as followings.  </p> <p></p>"},{"location":"High-Level-Design.html#device>_allocation","title":"Device Allocation","text":"<p><code>Worker</code> will allocate real-time available device for each requested task.  </p>"},{"location":"High-Level-Design.html#memory>_allocation>_for>_deepx>_devices","title":"Memory Allocation for DEEPX devices","text":"<p>When the InferenceEngine was created, memory class allocation was required on the system.  In case of accelerator mode, each NPU memory and host memory is managed separately by the memory class allocator. NPU-side memory is divided into the feature area and the model area. The feature area is allocated for input and output data, and the model area is allocated for the compiled model. In host-side memory, the input feature is provided by the caller, and the output as inference result is managed by the buffer class.  In case of standalone mode, contiguous reserved memory is used for the NPU data interface by Linux CMA (Contiguous Memory Allocator).  </p> <p></p>"},{"location":"How-To-Create-CMake-Project.html","title":"How To Create CMake Project","text":""},{"location":"How-To-Create-CMake-Project.html#how>_to>_create>_a>_cmake>_project>_using>_dxrt","title":"How to Create a CMake Project Using DXRT","text":"<p>This guide provides step-by-step instructions on creating a new CMake project using the DXRT library. 1. Build DXRT library Refer to Installation, Build chapter for instructions on fetching and building the DXRT source code. 2. Create a new CMake project Create a new CMake project by making a project directory and creating a <code>CMakeLists.txt</code> file. </p><pre><code>mkdir MyProject\ncd MyProject\ntouch CMakeLists.txt\n</code></pre> 3. Helloworld with DXRT API Let's write a simple hello world app. that calls the DXRT API. main.cpp: <pre><code>#include \"dxrt/dxrt_api.h\"\n\nusing namespace std;\n\nint main(int argc, char *argv[])\n{\n    auto devices = dxrt::CheckDevices();\n    cout &lt;&lt; \"hello, world\" &lt;&lt; endl;\n    return 0;\n}\n</code></pre> 4. Modify CMakeLists.txt Open and modify <code>CMakeLists.txt</code> file as follows. <pre><code>cmake_minimum_required(VERSION 3.14)\nproject(app_template)\nset(CMAKE_CXX_STANDARD_REQUIRED \"ON\")\nset(CMAKE_CXX_STANDARD \"17\")\n\n# Specify the installation path where the DXRT library is installed.\nset(DXRT_LIB_PATH \"/usr/local/lib\") # Adjust this path to the actual installation path of DXRT library.\n\n# Find and retrieve the DXRT library.\nfind_library(DXRT_LIBRARY REQUIRED NAMES dxrt_${CMAKE_SYSTEM_PROCESSOR} PATHS ${DXRT_LIB_PATH})\n\n# add google protobuf library.\nadd_library(protobuf SHARED IMPORTED)\nset_target_properties(protobuf PROPERTIES\n  IMPORTED_LOCATION \"${DXRT_LIB_PATH}/libprotobuf.so.23\"  \n)\n\n# Add project source files and executable.\nadd_executable(HelloWorld main.cpp)\n\n# Link the DXRT library to the executable.\ntarget_link_libraries(HelloWorld PRIVATE ${DXRT_LIBRARY} protobuf)\n</code></pre> Set <code>DXRT_LIB_PATH</code> with the actual path where the DXRT library is installed. 5. Build the Project <pre><code>mkdir build\ncd build\ncmake ..\nmake\n</code></pre> 6. Run the Executable After a successful build, run the generated executable.   <p>Now, you have created a new CMake project using the DXRT library!  </p>"},{"location":"Inference-Guide.html","title":"Inference Guide","text":""},{"location":"Inference-Guide.html#model>_file>_format","title":"Model file format","text":"<p>Original ONNX model file is converted as following by DXCOM SDK. </p><pre><code>Model dir.\n    \u2514\u2500\u2500 graph.dxnn\n</code></pre> * graph.dxnn : Deepx Single Artifact with NPU Command Data &amp; metadata / Model Parameters"},{"location":"Inference-Guide.html#basic>_flow","title":"Basic Flow","text":""},{"location":"Inference-Guide.html#0>_prepare>_a>_model","title":"0. Prepare a model","text":"<p>You can choose one by 2 way 1) Pick one prebuilt model from ModelZoo 2) Compile a model to DXRT model format (Details are in DXCOM SDK manual)  </p>"},{"location":"Inference-Guide.html#1>_set>_inference>_option","title":"1. Set inference option","text":"<p>Create an <code>dxrt::InferenceOption</code>. (Refer to API Reference) <code>dxrt::InferenceOption</code> represents detailed options for inference engine. - This implementation is deprecated temporarily in current version, and will be ported in next version.  </p>"},{"location":"Inference-Guide.html#2>_load>_model>_to>_inference>_engine","title":"2. Load model to inference engine","text":"<p>Create an <code>dxrt::InferenceEngine</code> from the model directory. Required initialization of HW resources will be done automatically by doing this. If <code>dxrt::InferenceOption</code> is not specified, default inference option is applied. </p><pre><code>auto ie = dxrt::InferenceEngine(\"yolov5s\");\nauto ie = dxrt::InferenceEngine(\"yolov5s\", &amp;option);\n</code></pre>"},{"location":"Inference-Guide.html#3>_connect>_input>_tensors","title":"3. Connect input tensors","text":"<p>Prepare input buffer(s) to perform infernce on. You can easily prepare an input buffer with minimal effort with the code below. </p><pre><code>vector&lt;uint8_t&gt; inputBuf(ie.input_size(), 0);\n</code></pre> Guides for connecting the inference engine to various image sources are provided in dx-app and dx-demo, along with preprocessing examples."},{"location":"Inference-Guide.html#4>_inference","title":"4. Inference","text":""},{"location":"Inference-Guide.html#41>_run>_sync","title":"4.1 Run (Sync)","text":"<p>Run inference engine. <code>outputs</code> is a vector of shared_ptr of <code>dxrt::Tensor</code> returned from <code>dxrt::InferenceEngine::Run()</code>. </p><pre><code>auto outputs = ie.Run(inputBuf.data());\n</code></pre> The <code>dxrt::InferenceEngine::Run()</code> is designed to use a single NPU core. To utilize multiple cores simultaneously, you need to use <code>dxrt::InferenceEngine::RunAsync</code> or threads. Please refer to the following diagram.   <p> </p>"},{"location":"Inference-Guide.html#42>_runasync","title":"4.2 RunAsync","text":"<p>If you want to perform inference in a non-blocking way, you can call <code>dxrt::InferenceEngine::RunAsync</code> as follows. </p><pre><code>auto jobId = ie.RunAsync(inputBuf.data());\n</code></pre> Non-blocking API issues a request ID, and you can wait for the request to complete. <pre><code>auto outputs = ie.Wait(jobId);\n</code></pre> If you don't want to wait to run your application efficiently, you can pipeline requests with callbacks. <pre><code>std::function&lt;int(vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt;, void*)&gt; postProcCallBack = \\\n    [&amp;](vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt; outputs, void *arg)\n    {\n        /* Process output tensors here */\n        ... ...\n        return 0;\n    };\nie.RegisterCallBack(postProcCallBack);\n</code></pre> <p> </p> <p>For more detailed information, please check the API document.  </p>"},{"location":"Inference-Guide.html#5>_process>_output>_tensors","title":"5. Process output tensors","text":"<p>Since inference is done, process output tensors using <code>Tensor</code> APIs and custom post-processing logic. You can find templates and example code in dx-app, dx-demo to help you post-process smoothly. As mentioned before, using callbacks allows for more efficient post-processing.  </p>"},{"location":"Inference-Guide.html#multiple>_device>_inference","title":"Multiple Device Inference","text":"<p>This is not applicable for single-NPU devices. Basically, the inference engine schedules and manages multiple devices in real time. If inference option is set explicitly, inference engine may only use specific devices during real-time inference for the model.  </p>"},{"location":"Inference-Guide.html#device>_tensor>_data>_format","title":"Device Tensor Data Format","text":"<p>Compiled models have tensor shape as NHWC format basically. Input tensor data format of current devices consists of 2 types. For formatter type, [1, 3, 224, 224] will be [1, 224, 224, 3] in device input tensor. For im2col type, [1, 3, 224, 224] will be [1, 224, 224*3+32] in device input tensor.  </p> name Compiled model format Device format Data size Formatter [N, H, W, C] [N, H, W, C] 8bit IM2COL [N, H, W, C] [N, H, align64(W*C)] 8bit <p>Output tensor data format of current devices is aligned NHWC format. For example, [1, 40, 52, 36] will be [1, 52, 36, 40+24] in device output tensor. By using <code>Tensor</code> APIs, post-processing can be done without converting formats.  </p> name Compiled model format Device format aligned NHWC [N, H, W, C] [N, H, W, align64(C)] <p>API to convert from device format to NCHW/NHWC format will be supported soon.  </p>"},{"location":"Inference-Guide.html#runtime>_linking>_error","title":"Runtime Linking Error","text":"<pre><code>yolo: error while loading shared libraries: libprotobuf.so.23:\n      cannot open shared object file: No such file or directory\nor\n\nyolo: error while loading shared libraries: libdxrt_x86_64 &lt;or riscv64/aarch64&gt;.so: \n      cannot open shared object file: No such file or directory\n</code></pre> If you get any library(DXRT lib., openCV, googleprotobuf) linking error while executing applications, please set library path as following.   <ul> <li>dxrt lib. path is <code>build_&lt;CMAKE_SYSTEM_PROCESSOR&gt;/release/lib/</code> in the DXRT package.  </li> <li>openCV lib. path is <code>&lt;CMAKE_INSTALL_PREFIX&gt;/lib</code> in your openCV build repository.  </li> <li>googleprotobuf lib. path is <code>extern/lib/&lt;CMAKE_SYSTEM_PROCESSOR&gt;</code> in the DXRT package. <pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:&lt;lib. path&gt;\n</code></pre></li> </ul>"},{"location":"Inference-Guide.html#profile>_application","title":"Profile application","text":""},{"location":"Inference-Guide.html#1>_gather>_timing>_data>_per>_events","title":"1. Gather timing data per events","text":"<p>You can profile events using <code>Profiler</code> APIs. Please refer to API reference. After application is terminated, <code>profiler.json</code> will be created. </p><pre><code>auto&amp; profiler = dxrt::Profiler::GetInstance();\nprofiler.Start(\"1sec\");\nsleep(1);\nprofiler.End(\"1sec\");\n</code></pre>"},{"location":"Inference-Guide.html#2>_visualize>_profilers>_data","title":"2. Visualize profiler's data","text":"<p>You can visualize events from <code>profiler.json</code> using as following. </p><pre><code>python3 tool/profiler/plot.py --input profiler.json\n</code></pre> Then, generated image file <code>profiler.png</code> will show detailed profiling data.  Please refer to usage of <code>tool/profiler/plot.py</code>. <pre><code>usage: plot.py [-h] [-i INPUT] [-o OUTPUT] [-s START] [-e END] [-g]\n\nDraw timing chart from profiler data.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i INPUT, --input INPUT\n                        Input json file to plot\n  -o OUTPUT, --output OUTPUT\n                        Output image file to save the plot\n  -s START, --start START\n                        Starting point( &gt; 0.0) when the entire interval is 1\n  -e END, --end END     End point( &lt; 1.0) when the entire interval is 1\n  -g, --show_gap        Show time gap between starting points\n</code></pre>"},{"location":"Installation.html","title":"Installation","text":""},{"location":"Installation.html#setup>_build>_environments","title":"Setup build environments","text":"<ul> <li>OS : Ubuntu 18.04, Ubuntu 20.04, Ubuntu 22.04 </li> <li>CMake : 3.14.0 or higher required. Refer to followings. <pre><code>$ sudo apt-get install zlib1g-dev libcurl4-openssl-dev\n$ wget https://cmake.org/files/v3.14/cmake-3.14.0.tar.gz --no-check-certificate\n$ tar xvf cmake-3.14.0.tar.gz\n$ cd cmake-3.14.0\n$ ./bootstrap --system-curl\n$ make -j$(nproc)\n$ sudo make install\n</code></pre></li> <li>Ninja-build <pre><code>$ sudo apt install ninja-build\n</code></pre></li> <li> <p>GCC : 8.0.0 or higher required.  </p> </li> <li> <p>aarch64 gcc </p><pre><code>$ sudo apt-get install gcc-aarch64-linux-gnu g++-aarch64-linux-gnu\n</code></pre> </li> <li>riscv64 gcc <pre><code>$ sudo apt-get install gcc-riscv64-linux-gnu g++-riscv64-linux-gnu\n</code></pre></li> <li>(optional) onnxruntime Required if you need CPU offloading for NN ops that NPU does not support. Refer to following installation example for onnxruntime linux x64 v1.12.0. <pre><code>$ wget https://github.com/microsoft/onnxruntime/releases/download/v1.12.0/onnxruntime-linux-x64-1.12.0.tgz \n$ sudo tar -xvzf onnxruntime-linux-x64-1.12.0.tgz -C /usr/local --strip-components=1\n$ sudo ldconfig \n</code></pre> Refer to onnxruntime releases list  You can use ./install.sh for onnxruntime library installation     <pre><code>./install.sh --onnxruntime  \n</code></pre>   To specify the compilation environment as aarch64, use the --arch option.    <pre><code>./install.sh --arch aarch64 --onnxruntime\n</code></pre></li> </ul>"},{"location":"Installation.html#get>_software>_repository","title":"Get software repository","text":"<p>Three types are provided. * Fetch git repository </p><pre><code>$ git clone git@github.com:DEEPX-AI/dx_rt.git\n</code></pre>"},{"location":"Installation.html#file>_structure","title":"File structure","text":"<pre><code>\u251c\u2500\u2500 cli\n\u251c\u2500\u2500 build.cfg\n\u251c\u2500\u2500 build.sh\n\u251c\u2500\u2500 cmake\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 extern\n\u251c\u2500\u2500 lib\n\u251c\u2500\u2500 python_package\n\u251c\u2500\u2500 sample\n\u251c\u2500\u2500 service\n\u2514\u2500\u2500 test\n</code></pre> <ul> <li>cli : applications</li> <li>build.cfg : C/C++ preprocessor macros to use in application  </li> <li>build.sh : build shell script  </li> <li>cmake : required cmake scripts  </li> <li>docs : markdown files, and doxygen generator for API reference  </li> <li>extern : 3rd party library files  </li> <li>lib : prebuilt dxrt libarary files  </li> <li>sample : sample images for demo applications  </li> <li>service : dxrt service unit file</li> <li>test : dxrt unit test files  </li> </ul>"},{"location":"Installation.html#install>_linux>_device>_driver","title":"Install Linux Device Driver","text":"<p>Please refer to deepx driver git</p>"},{"location":"Installation.html#install>_python>_package","title":"Install python package","text":"<pre><code>$ cd python_package\n$ pip install .\n* You can check dx-engine in the list of Python packages using the command below.\n$ pip list | grep dx\ndx-engine          0.0.1\n</code></pre>"},{"location":"Installation.html#register>_dx-rt>_service","title":"Register DX-RT Service","text":"<p>How to register DX-RT Service </p><pre><code># modify service unit file (check ExecStart path)\n$ vi ./service/dxrt.service\n\n# copy service unit file to system service folder\n$ sudo cp ./service/dxrt.service /etc/systemd/system\n\n# start service as daemon\n$ sudo systemctl start dxrt.service\n</code></pre> Stop the service <pre><code>$ sudo systemctl stop dxrt.service\n</code></pre> <p>Display status of the service </p><pre><code>$ sudo systemctl status dxrt.service\n</code></pre> <p>Restart the service </p><pre><code>$ sudo systemctl restart dxrt.service\n</code></pre> <p>Enable the service to start automatically at boot </p><pre><code>$ sudo systemctl enable dxrt.service\n</code></pre> <p>Disable the service to start automatically at boot </p><pre><code>$ sudo systemctl disable dxrt.service\n</code></pre> <p>Check Logs </p><pre><code>$ sudo journalctl -u dxrt.service\n</code></pre>"},{"location":"Introduction.html","title":"Introduction","text":"<p>DX-RT is DEEPX Runtime SDK for AI inference based on DEEPX devices. It supports pre-built models from DEEPX model zoo, and compiled models by DXCOM(DEEPX Compiler SDK).  </p>"},{"location":"Introduction.html#supported>_devices","title":"Supported devices","text":"<p>DX-RT provides common inference framework based on 2 kinds of inference mode.  </p> <ul> <li>Accelerator mode : inference by PCIe interface  </li> <li>Standalone mode : inference by direct AXI/APB interface  </li> </ul> Device Mode DX_V1 Standalone DX_V3 Standalone DX_M1 Accelerator DX_H1 Accelerator"},{"location":"Introduction.html#resources","title":"Resources","text":""},{"location":"Introduction.html#model>_zoo>_prebuilt>_models","title":"Model zoo (Prebuilt models)","text":"<p>Official modelzoo is in preparation.  </p>"},{"location":"Introduction.html#documents","title":"Documents","text":"<p>Official website documents are in preparation. Currently, documents are provided with limited rights. Please consult with our contact point person. In other way, you can generate documents from repository (using markdown files in <code>docs</code>).  </p> <ul> <li>python&gt;=3.9 is needed. <pre><code># install MkDocs\npip install mkdocs mkdocs-material mkdocs-video pymdown-extensions\n# generate html to directory \"html_docs\"\nmkdocs build\n</code></pre> You can also generate API reference using doxygen. <pre><code># install doxygen\nsudo apt install doxygen graphviz\n# generate API reference html\ncd docs/cpp_api\ndoxygen Doxyfile\n</code></pre></li> </ul>"},{"location":"Test.html","title":"Test","text":""},{"location":"Test.html#google>_unit>_test","title":"Google Unit Test","text":"<ul> <li>Based on googletest framework, test program is provided.  </li> <li>Current version supports build of test program only in DEEPX-internal version. </li> </ul>"},{"location":"Test.html#dxrt>_unit>_test","title":"DXRT Unit Test","text":"<p>Perform DXRT's unit tests. (<code>USE_DXRT_TEST</code> should be <code>ON</code> in cmake/dxrt.cfg.cmake) Please check available test lists as following. </p><pre><code>./bin/dxrt_test --gtest_list_tests\n./bin/dxrt_test -m &lt;model-path&gt;\n</code></pre>"},{"location":"_Footer.html","title":"Footer","text":"<p>Copyright 2022. dxrt team, All rights reserved.</p>"}]}