
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="09_01_C%2B%2B_Tutorials.html">
      
      
        <link rel="next" href="10_01_C%2B%2B_API_Reference.html">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Python Tutorials - DXNN Runtime (DX-RT) User Manual</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+Display:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Noto Sans Display";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#run>_synchronous" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="DXNN Runtime (DX-RT) User Manual" class="md-header__button md-logo" aria-label="DXNN Runtime (DX-RT) User Manual" data-md-component="logo">
      
  <img src="../img/deepx.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            DXNN Runtime (DX-RT) User Manual
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Python Tutorials
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="01_DXNN_Runtime_Overview.html" class="md-tabs__link">
        
  
  
    
  
  DXNN Runtime Overview

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="02_Installation_on_Linux.html" class="md-tabs__link">
        
  
  
    
  
  Installation on Linux

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="03_Installation_on_Windows.html" class="md-tabs__link">
        
  
  
    
  
  Installation on Windows

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="04_Model_Inference.html" class="md-tabs__link">
        
  
  
    
  
  Model Inference

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="05_Command_Line_Interface.html" class="md-tabs__link">
        
  
  
    
  
  Command Line Interface

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="06_Inference_API.html" class="md-tabs__link">
        
  
  
    
  
  Inference API Guide

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="07_Multi_Input_Inference.html" class="md-tabs__link">
        
  
  
    
  
  Multi-input Inference Guide

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="08_Global_Instance.html" class="md-tabs__link">
        
  
  
    
  
  Configuration and DeviceStatus Guide

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="09_01_C%2B%2B_Tutorials.html" class="md-tabs__link">
          
  
  
  Tutorials

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="10_01_C%2B%2B_API_Reference.html" class="md-tabs__link">
          
  
  
  API Reference

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="Appendix_Change_Log.html" class="md-tabs__link">
        
  
  
    
  
  Change Log

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="DXNN Runtime (DX-RT) User Manual" class="md-nav__button md-logo" aria-label="DXNN Runtime (DX-RT) User Manual" data-md-component="logo">
      
  <img src="../img/deepx.png" alt="logo">

    </a>
    DXNN Runtime (DX-RT) User Manual
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="01_DXNN_Runtime_Overview.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DXNN Runtime Overview
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="02_Installation_on_Linux.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation on Linux
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="03_Installation_on_Windows.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation on Windows
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="04_Model_Inference.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Inference
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="05_Command_Line_Interface.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Command Line Interface
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="06_Inference_API.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inference API Guide
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="07_Multi_Input_Inference.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multi-input Inference Guide
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="08_Global_Instance.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Configuration and DeviceStatus Guide
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" checked>
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="09_01_C%2B%2B_Tutorials.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    C++ Tutorials
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Python Tutorials
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="09_02_Python_Tutorials.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Python Tutorials
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#run>_synchronous" class="md-nav__link">
    <span class="md-ellipsis">
      Run (Synchronous)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#runasync>_asynchronous" class="md-nav__link">
    <span class="md-ellipsis">
      RunAsync (Asynchronous)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#run>_batch" class="md-nav__link">
    <span class="md-ellipsis">
      Run (Batch)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inference>_option" class="md-nav__link">
    <span class="md-ellipsis">
      Inference Option
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configuration>_and>_devicestatus" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration and DeviceStatus
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#profiler>_configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Profiler Configuration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-input>_inference" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-input Inference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    API Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="10_01_C%2B%2B_API_Reference.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    C++ API Reference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="10_02_Python_API_Reference.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python API Reference
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="Appendix_Change_Log.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Change Log
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>Python Tutorials</h1>

<h3 id="run&gt;_synchronous">Run (Synchronous)<a class="headerlink" href="#run&gt;_synchronous" title="Permanent link">&para;</a></h3>
<p>The synchronous Run method uses a single NPU core to perform inference in a blocking manner. It can be configured to utilize multiple NPU cores simultaneously by employing threads to run each core independently. (Refer to <strong>Figure</strong> in <strong>Section 5.2. Inference Workflow</strong>)  </p>
<p><strong>Inference Engine Run (Python)</strong>  </p>
<p><code>run_sync_model.py</code></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a># DX-RT importes
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>from dx_engine import InferenceEngine
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>...
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>if __name__ == &quot;__main__&quot;:
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    ...    
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    # create inference engine instance with model
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    ie = InferenceEngine(modelPath)
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    input = [np.zeros(ie.GetInputSize(), dtype=np.uint8)]
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    # inference loop
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    for i in range(loop_count):
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>        # inference synchronously 
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>        # use only one npu core 
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>        outputs = ie.Run(input)
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>        # post processing 
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>        postProcessing(outputs)
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>    exit(0)
</code></pre></div>
<hr />
<h3 id="runasync&gt;_asynchronous">RunAsync (Asynchronous)<a class="headerlink" href="#runasync&gt;_asynchronous" title="Permanent link">&para;</a></h3>
<p>The asynchronous Run mode is a method that performs inference asynchronously while utilizing multiple NPU cores simultaneously. It can be implemented to maximize NPU resources through a callback function or a thread wait mechanism. </p>
<p>Inference Engine RunAsync, Callback, User Argument  </p>
<ul>
<li>the outputs are guaranteed to be valid <strong>only</strong> within this callback function  </li>
<li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li>
<li>inference asynchronously, use all npu cores  </li>
<li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li>
</ul>
<p>The following is an example of asynchronous inference using a callback function. A user argument can be used to synchronize the input with the output of the callback.  </p>
<p><strong>Inference Engine RunAsync, Callback, User Argument (Python)</strong></p>
<p><code>run_async_model.py</code></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>from dx_engine import InferenceEngine
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>...
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>q = queue.Queue()
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>gLoopCount = 0
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>lock = threading.Lock()
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>def onInferenceCallbackFunc(outputs, user_arg):
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>    # the outputs are guaranteed to be valid only within this callback function
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>    # processing this callback functions as quickly as possible is beneficial 
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>    # for improving inference performance
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>    global gLoopCount
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>    # Mutex locks should be properly adjusted 
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>    # to ensure that callback functions are thread-safe.
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>    with lock:
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a>        # user data type casting
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a>        index, loop_count = user_arg
<a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a>
<a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a>
<a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a>        # post processing
<a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a>        #postProcessing(outputs);
<a id="__codelineno-1-26" name="__codelineno-1-26" href="#__codelineno-1-26"></a>
<a id="__codelineno-1-27" name="__codelineno-1-27" href="#__codelineno-1-27"></a>        # something to do
<a id="__codelineno-1-28" name="__codelineno-1-28" href="#__codelineno-1-28"></a>
<a id="__codelineno-1-29" name="__codelineno-1-29" href="#__codelineno-1-29"></a>        print(&quot;Inference output (callback) index=&quot;, index)
<a id="__codelineno-1-30" name="__codelineno-1-30" href="#__codelineno-1-30"></a>
<a id="__codelineno-1-31" name="__codelineno-1-31" href="#__codelineno-1-31"></a>        gLoopCount += 1
<a id="__codelineno-1-32" name="__codelineno-1-32" href="#__codelineno-1-32"></a>        if ( gLoopCount == loop_count ) :
<a id="__codelineno-1-33" name="__codelineno-1-33" href="#__codelineno-1-33"></a>            print(&quot;Complete Callback&quot;)
<a id="__codelineno-1-34" name="__codelineno-1-34" href="#__codelineno-1-34"></a>            q.put(0)
<a id="__codelineno-1-35" name="__codelineno-1-35" href="#__codelineno-1-35"></a>
<a id="__codelineno-1-36" name="__codelineno-1-36" href="#__codelineno-1-36"></a>    return 0
<a id="__codelineno-1-37" name="__codelineno-1-37" href="#__codelineno-1-37"></a>
<a id="__codelineno-1-38" name="__codelineno-1-38" href="#__codelineno-1-38"></a>
<a id="__codelineno-1-39" name="__codelineno-1-39" href="#__codelineno-1-39"></a>if __name__ == &quot;__main__&quot;:
<a id="__codelineno-1-40" name="__codelineno-1-40" href="#__codelineno-1-40"></a>
<a id="__codelineno-1-41" name="__codelineno-1-41" href="#__codelineno-1-41"></a>    ...    
<a id="__codelineno-1-42" name="__codelineno-1-42" href="#__codelineno-1-42"></a>
<a id="__codelineno-1-43" name="__codelineno-1-43" href="#__codelineno-1-43"></a>    # create inference engine instance with model
<a id="__codelineno-1-44" name="__codelineno-1-44" href="#__codelineno-1-44"></a>    ie = InferenceEngine(modelPath)
<a id="__codelineno-1-45" name="__codelineno-1-45" href="#__codelineno-1-45"></a>
<a id="__codelineno-1-46" name="__codelineno-1-46" href="#__codelineno-1-46"></a>    # register call back function
<a id="__codelineno-1-47" name="__codelineno-1-47" href="#__codelineno-1-47"></a>    ie.register_callback(onInferenceCallbackFunc)
<a id="__codelineno-1-48" name="__codelineno-1-48" href="#__codelineno-1-48"></a>
<a id="__codelineno-1-49" name="__codelineno-1-49" href="#__codelineno-1-49"></a>
<a id="__codelineno-1-50" name="__codelineno-1-50" href="#__codelineno-1-50"></a>    input = [np.zeros(ie.GetInputSize(), dtype=np.uint8)]
<a id="__codelineno-1-51" name="__codelineno-1-51" href="#__codelineno-1-51"></a>
<a id="__codelineno-1-52" name="__codelineno-1-52" href="#__codelineno-1-52"></a>    # inference loop
<a id="__codelineno-1-53" name="__codelineno-1-53" href="#__codelineno-1-53"></a>    for i in range(loop_count):
<a id="__codelineno-1-54" name="__codelineno-1-54" href="#__codelineno-1-54"></a>
<a id="__codelineno-1-55" name="__codelineno-1-55" href="#__codelineno-1-55"></a>        # inference asynchronously, use all npu cores
<a id="__codelineno-1-56" name="__codelineno-1-56" href="#__codelineno-1-56"></a>        # if device-load &gt;= max-load-value, this function will block  
<a id="__codelineno-1-57" name="__codelineno-1-57" href="#__codelineno-1-57"></a>        ie.RunAsync(input, user_arg=[i, loop_count])
<a id="__codelineno-1-58" name="__codelineno-1-58" href="#__codelineno-1-58"></a>
<a id="__codelineno-1-59" name="__codelineno-1-59" href="#__codelineno-1-59"></a>        print(&quot;Inference start (async)&quot;, i)
<a id="__codelineno-1-60" name="__codelineno-1-60" href="#__codelineno-1-60"></a>
<a id="__codelineno-1-61" name="__codelineno-1-61" href="#__codelineno-1-61"></a>    exit(q.get())
</code></pre></div>
<p>The following is an example where multiple threads start input and inference, and a single callback processes the output.</p>
<p>Inference Engine RunAsync, Callback, User Argument, Thread  </p>
<ul>
<li>the outputs are guaranteed to be valid <strong>only</strong> within this callback function  </li>
<li>processing this callback functions as quickly as possible is beneficial for improving inference performance  </li>
<li>inference asynchronously, use all npu cores  </li>
<li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li>
</ul>
<p><strong>Inference Engine RunAsync, Callback, User Argument, Thread (Python)</strong></p>
<p><code>run_async_model_thread.py</code>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>from dx_engine import InferenceEngine
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>...
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>THRAD_COUNT = 3
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>total_count = 0
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>q = queue.Queue()
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>lock = threading.Lock()
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>def inferenceThreadFunc(ie, threadIndex, loopCount):
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>    # input
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>    input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>    # inference loop
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a>    for i in range(loopCount):
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a>
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a>        # inference asynchronously, use all npu cores
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#__codelineno-2-20"></a>        # if device-load &gt;= max-load-value, this function will block  
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#__codelineno-2-21"></a>        ie.RunAsync(input,user_arg = [i, loopCount, threadIndex])
<a id="__codelineno-2-22" name="__codelineno-2-22" href="#__codelineno-2-22"></a>
<a id="__codelineno-2-23" name="__codelineno-2-23" href="#__codelineno-2-23"></a>    return 0
<a id="__codelineno-2-24" name="__codelineno-2-24" href="#__codelineno-2-24"></a>
<a id="__codelineno-2-25" name="__codelineno-2-25" href="#__codelineno-2-25"></a>def onInferenceCallbackFunc(outputs, user_arg):
<a id="__codelineno-2-26" name="__codelineno-2-26" href="#__codelineno-2-26"></a>    # the outputs are guaranteed to be valid only within this callback function
<a id="__codelineno-2-27" name="__codelineno-2-27" href="#__codelineno-2-27"></a>    # processing this callback functions as quickly as possible is beneficial 
<a id="__codelineno-2-28" name="__codelineno-2-28" href="#__codelineno-2-28"></a>    # for improving inference performance
<a id="__codelineno-2-29" name="__codelineno-2-29" href="#__codelineno-2-29"></a>
<a id="__codelineno-2-30" name="__codelineno-2-30" href="#__codelineno-2-30"></a>    global total_count
<a id="__codelineno-2-31" name="__codelineno-2-31" href="#__codelineno-2-31"></a>
<a id="__codelineno-2-32" name="__codelineno-2-32" href="#__codelineno-2-32"></a>    # Mutex locks should be properly adjusted 
<a id="__codelineno-2-33" name="__codelineno-2-33" href="#__codelineno-2-33"></a>    # to ensure that callback functions are thread-safe.
<a id="__codelineno-2-34" name="__codelineno-2-34" href="#__codelineno-2-34"></a>    with lock:
<a id="__codelineno-2-35" name="__codelineno-2-35" href="#__codelineno-2-35"></a>        # user data type casting
<a id="__codelineno-2-36" name="__codelineno-2-36" href="#__codelineno-2-36"></a>        index = user_arg[0]
<a id="__codelineno-2-37" name="__codelineno-2-37" href="#__codelineno-2-37"></a>        loop_count = user_arg[1]
<a id="__codelineno-2-38" name="__codelineno-2-38" href="#__codelineno-2-38"></a>        thread_index = user_arg[2]
<a id="__codelineno-2-39" name="__codelineno-2-39" href="#__codelineno-2-39"></a>
<a id="__codelineno-2-40" name="__codelineno-2-40" href="#__codelineno-2-40"></a>        # post processing
<a id="__codelineno-2-41" name="__codelineno-2-41" href="#__codelineno-2-41"></a>        #postProcessing(outputs);
<a id="__codelineno-2-42" name="__codelineno-2-42" href="#__codelineno-2-42"></a>
<a id="__codelineno-2-43" name="__codelineno-2-43" href="#__codelineno-2-43"></a>        # something to do
<a id="__codelineno-2-44" name="__codelineno-2-44" href="#__codelineno-2-44"></a>
<a id="__codelineno-2-45" name="__codelineno-2-45" href="#__codelineno-2-45"></a>        total_count += 1
<a id="__codelineno-2-46" name="__codelineno-2-46" href="#__codelineno-2-46"></a>
<a id="__codelineno-2-47" name="__codelineno-2-47" href="#__codelineno-2-47"></a>        if ( total_count ==  loop_count * THRAD_COUNT) :
<a id="__codelineno-2-48" name="__codelineno-2-48" href="#__codelineno-2-48"></a>            q.put(0)
<a id="__codelineno-2-49" name="__codelineno-2-49" href="#__codelineno-2-49"></a>
<a id="__codelineno-2-50" name="__codelineno-2-50" href="#__codelineno-2-50"></a>    return 0
<a id="__codelineno-2-51" name="__codelineno-2-51" href="#__codelineno-2-51"></a>
<a id="__codelineno-2-52" name="__codelineno-2-52" href="#__codelineno-2-52"></a>
<a id="__codelineno-2-53" name="__codelineno-2-53" href="#__codelineno-2-53"></a>if __name__ == &quot;__main__&quot;:
<a id="__codelineno-2-54" name="__codelineno-2-54" href="#__codelineno-2-54"></a>    ...    
<a id="__codelineno-2-55" name="__codelineno-2-55" href="#__codelineno-2-55"></a>
<a id="__codelineno-2-56" name="__codelineno-2-56" href="#__codelineno-2-56"></a>    # create inference engine instance with model
<a id="__codelineno-2-57" name="__codelineno-2-57" href="#__codelineno-2-57"></a>    ie = InferenceEngine(modelPath)
<a id="__codelineno-2-58" name="__codelineno-2-58" href="#__codelineno-2-58"></a>
<a id="__codelineno-2-59" name="__codelineno-2-59" href="#__codelineno-2-59"></a>    # register call back function
<a id="__codelineno-2-60" name="__codelineno-2-60" href="#__codelineno-2-60"></a>    ie.register_callback(onInferenceCallbackFunc)
<a id="__codelineno-2-61" name="__codelineno-2-61" href="#__codelineno-2-61"></a>
<a id="__codelineno-2-62" name="__codelineno-2-62" href="#__codelineno-2-62"></a>
<a id="__codelineno-2-63" name="__codelineno-2-63" href="#__codelineno-2-63"></a>    t1 = threading.Thread(target=inferenceThreadFunc, args=(ie, 0, loop_count))
<a id="__codelineno-2-64" name="__codelineno-2-64" href="#__codelineno-2-64"></a>    t2 = threading.Thread(target=inferenceThreadFunc, args=(ie, 1, loop_count))
<a id="__codelineno-2-65" name="__codelineno-2-65" href="#__codelineno-2-65"></a>    t3 = threading.Thread(target=inferenceThreadFunc, args=(ie, 2, loop_count))
<a id="__codelineno-2-66" name="__codelineno-2-66" href="#__codelineno-2-66"></a>
<a id="__codelineno-2-67" name="__codelineno-2-67" href="#__codelineno-2-67"></a>    # Start and join
<a id="__codelineno-2-68" name="__codelineno-2-68" href="#__codelineno-2-68"></a>    t1.start()
<a id="__codelineno-2-69" name="__codelineno-2-69" href="#__codelineno-2-69"></a>    t2.start()
<a id="__codelineno-2-70" name="__codelineno-2-70" href="#__codelineno-2-70"></a>    t3.start()
<a id="__codelineno-2-71" name="__codelineno-2-71" href="#__codelineno-2-71"></a>
<a id="__codelineno-2-72" name="__codelineno-2-72" href="#__codelineno-2-72"></a>
<a id="__codelineno-2-73" name="__codelineno-2-73" href="#__codelineno-2-73"></a>    # join
<a id="__codelineno-2-74" name="__codelineno-2-74" href="#__codelineno-2-74"></a>    t1.join()
<a id="__codelineno-2-75" name="__codelineno-2-75" href="#__codelineno-2-75"></a>    t2.join()
<a id="__codelineno-2-76" name="__codelineno-2-76" href="#__codelineno-2-76"></a>    t3.join()
<a id="__codelineno-2-77" name="__codelineno-2-77" href="#__codelineno-2-77"></a>
<a id="__codelineno-2-78" name="__codelineno-2-78" href="#__codelineno-2-78"></a>
<a id="__codelineno-2-79" name="__codelineno-2-79" href="#__codelineno-2-79"></a>    exit(q.get())
</code></pre></div></p>
<p>The following is an example of performing asynchronous inference by creating an inference wait thread. The main thread starts input and inference, and the inference wait thread retrieves the output data corresponding to the input.</p>
<p>Inference Engine RunAsync, Wait  </p>
<ul>
<li>inference asynchronously, use all npu cores  </li>
<li>if <code>device-load &gt;= max-load-value</code>, this function will block  </li>
</ul>
<p><strong>Inference Engine RunAsync, Wait (Python)</strong></p>
<p><code>run_async_model_wait.py</code>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a># DX-RT imports
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>from dx_engine import InferenceEngine
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>...
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>q = queue.Queue()
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>def inferenceThreadFunc(ie, loopCount):
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>    count = 0
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>    while(True):
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a>
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a>        # pop item from queue 
<a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a>        jobId = q.get()
<a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a>
<a id="__codelineno-3-17" name="__codelineno-3-17" href="#__codelineno-3-17"></a>        # waiting for the inference to complete by jobId
<a id="__codelineno-3-18" name="__codelineno-3-18" href="#__codelineno-3-18"></a>        # ownership of the outputs is transferred to the user 
<a id="__codelineno-3-19" name="__codelineno-3-19" href="#__codelineno-3-19"></a>        outputs = ie.Wait(jobId)
<a id="__codelineno-3-20" name="__codelineno-3-20" href="#__codelineno-3-20"></a>
<a id="__codelineno-3-21" name="__codelineno-3-21" href="#__codelineno-3-21"></a>        # post processing
<a id="__codelineno-3-22" name="__codelineno-3-22" href="#__codelineno-3-22"></a>        # postProcessing(outputs);
<a id="__codelineno-3-23" name="__codelineno-3-23" href="#__codelineno-3-23"></a>
<a id="__codelineno-3-24" name="__codelineno-3-24" href="#__codelineno-3-24"></a>        # something to do
<a id="__codelineno-3-25" name="__codelineno-3-25" href="#__codelineno-3-25"></a>
<a id="__codelineno-3-26" name="__codelineno-3-26" href="#__codelineno-3-26"></a>        count += 1
<a id="__codelineno-3-27" name="__codelineno-3-27" href="#__codelineno-3-27"></a>        if ( count &gt;= loopCount ):
<a id="__codelineno-3-28" name="__codelineno-3-28" href="#__codelineno-3-28"></a>            break
<a id="__codelineno-3-29" name="__codelineno-3-29" href="#__codelineno-3-29"></a>
<a id="__codelineno-3-30" name="__codelineno-3-30" href="#__codelineno-3-30"></a>    return 0
<a id="__codelineno-3-31" name="__codelineno-3-31" href="#__codelineno-3-31"></a>
<a id="__codelineno-3-32" name="__codelineno-3-32" href="#__codelineno-3-32"></a>
<a id="__codelineno-3-33" name="__codelineno-3-33" href="#__codelineno-3-33"></a>if __name__ == &quot;__main__&quot;:
<a id="__codelineno-3-34" name="__codelineno-3-34" href="#__codelineno-3-34"></a>    ...
<a id="__codelineno-3-35" name="__codelineno-3-35" href="#__codelineno-3-35"></a>
<a id="__codelineno-3-36" name="__codelineno-3-36" href="#__codelineno-3-36"></a>    # create inference engine instance with model
<a id="__codelineno-3-37" name="__codelineno-3-37" href="#__codelineno-3-37"></a>    with InferenceEngine(modelPath) as ie:
<a id="__codelineno-3-38" name="__codelineno-3-38" href="#__codelineno-3-38"></a>
<a id="__codelineno-3-39" name="__codelineno-3-39" href="#__codelineno-3-39"></a>        # do not register call back function
<a id="__codelineno-3-40" name="__codelineno-3-40" href="#__codelineno-3-40"></a>        # ie.register_callback(onInferenceCallbackFunc)
<a id="__codelineno-3-41" name="__codelineno-3-41" href="#__codelineno-3-41"></a>
<a id="__codelineno-3-42" name="__codelineno-3-42" href="#__codelineno-3-42"></a>        t1 = threading.Thread(target=inferenceThreadFunc, args=(ie, loop_count))
<a id="__codelineno-3-43" name="__codelineno-3-43" href="#__codelineno-3-43"></a>
<a id="__codelineno-3-44" name="__codelineno-3-44" href="#__codelineno-3-44"></a>        t1.start()
<a id="__codelineno-3-45" name="__codelineno-3-45" href="#__codelineno-3-45"></a>
<a id="__codelineno-3-46" name="__codelineno-3-46" href="#__codelineno-3-46"></a>        input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]
<a id="__codelineno-3-47" name="__codelineno-3-47" href="#__codelineno-3-47"></a>
<a id="__codelineno-3-48" name="__codelineno-3-48" href="#__codelineno-3-48"></a>        # inference loop
<a id="__codelineno-3-49" name="__codelineno-3-49" href="#__codelineno-3-49"></a>        for i in range(loop_count):
<a id="__codelineno-3-50" name="__codelineno-3-50" href="#__codelineno-3-50"></a>
<a id="__codelineno-3-51" name="__codelineno-3-51" href="#__codelineno-3-51"></a>            # inference asynchronously, use all npu cores
<a id="__codelineno-3-52" name="__codelineno-3-52" href="#__codelineno-3-52"></a>            # if device-load &gt;= max-load-value, this function will block  
<a id="__codelineno-3-53" name="__codelineno-3-53" href="#__codelineno-3-53"></a>            jobId = ie.run_async(input, user_arg=0)
<a id="__codelineno-3-54" name="__codelineno-3-54" href="#__codelineno-3-54"></a>
<a id="__codelineno-3-55" name="__codelineno-3-55" href="#__codelineno-3-55"></a>            q.put(jobId)
<a id="__codelineno-3-56" name="__codelineno-3-56" href="#__codelineno-3-56"></a>
<a id="__codelineno-3-57" name="__codelineno-3-57" href="#__codelineno-3-57"></a>        t1.join()
<a id="__codelineno-3-58" name="__codelineno-3-58" href="#__codelineno-3-58"></a>
<a id="__codelineno-3-59" name="__codelineno-3-59" href="#__codelineno-3-59"></a>    exit(0)
</code></pre></div></p>
<hr />
<h3 id="run&gt;_batch">Run (Batch)<a class="headerlink" href="#run&gt;_batch" title="Permanent link">&para;</a></h3>
<p>The following is an example of batch inference with multiple inputs and multiple outputs.</p>
<p><code>run_batch_model.py</code></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>import numpy as np
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>import sys
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>from dx_engine import InferenceEngine
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>from dx_engine import InferenceOption
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a>if __name__ == &quot;__main__&quot;:
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a>    ...
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a>    # create inference engine instance with model
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a>    with InferenceEngine(modelPath) as ie:
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a>
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a>        input_buffers = []
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a>        output_buffers = []
<a id="__codelineno-4-15" name="__codelineno-4-15" href="#__codelineno-4-15"></a>        index = 0
<a id="__codelineno-4-16" name="__codelineno-4-16" href="#__codelineno-4-16"></a>        for b in range(batch_count):
<a id="__codelineno-4-17" name="__codelineno-4-17" href="#__codelineno-4-17"></a>            input_buffers.append([np.array([np.random.randint(0, 255)],  dtype=np.uint8)])
<a id="__codelineno-4-18" name="__codelineno-4-18" href="#__codelineno-4-18"></a>            output_buffers.append([np.zeros(ie.get_output_size(), dtype=np.uint8)])
<a id="__codelineno-4-19" name="__codelineno-4-19" href="#__codelineno-4-19"></a>            index = index + 1
<a id="__codelineno-4-20" name="__codelineno-4-20" href="#__codelineno-4-20"></a>
<a id="__codelineno-4-21" name="__codelineno-4-21" href="#__codelineno-4-21"></a>        # inference loop
<a id="__codelineno-4-22" name="__codelineno-4-22" href="#__codelineno-4-22"></a>        for i in range(loop_count):
<a id="__codelineno-4-23" name="__codelineno-4-23" href="#__codelineno-4-23"></a>
<a id="__codelineno-4-24" name="__codelineno-4-24" href="#__codelineno-4-24"></a>            # batch inference
<a id="__codelineno-4-25" name="__codelineno-4-25" href="#__codelineno-4-25"></a>            # It operates asynchronously internally 
<a id="__codelineno-4-26" name="__codelineno-4-26" href="#__codelineno-4-26"></a>            # for the specified number of batches and returns the results
<a id="__codelineno-4-27" name="__codelineno-4-27" href="#__codelineno-4-27"></a>            results = ie.run_batch(input_buffers, output_buffers)
<a id="__codelineno-4-28" name="__codelineno-4-28" href="#__codelineno-4-28"></a>
<a id="__codelineno-4-29" name="__codelineno-4-29" href="#__codelineno-4-29"></a>            # post processing 
<a id="__codelineno-4-30" name="__codelineno-4-30" href="#__codelineno-4-30"></a>
<a id="__codelineno-4-31" name="__codelineno-4-31" href="#__codelineno-4-31"></a>    exit(0)
</code></pre></div>
<hr />
<h3 id="inference&gt;_option">Inference Option<a class="headerlink" href="#inference&gt;_option" title="Permanent link">&para;</a></h3>
<p>The following inference options allow you to specify an NPU core for performing inference.</p>
<p>Inference Engine Run, Inference Option  </p>
<ul>
<li>
<dl>
<dt>select devices</dt>
<dd>default device is <code>[]</code>  </dd>
<dd>Choose the device to utilize  (ex. <code>[0, 2]</code>)  </dd>
</dl>
</li>
<li>
<dl>
<dt>select bound option per device</dt>
<dd><code>InferenceOption.BOUND_OPTION.NPU_ALL</code>  </dd>
<dd><code>InferenceOption.BOUND_OPTION.NPU_0</code>  </dd>
<dd><code>InferenceOption.BOUND_OPTION.NPU_1</code>  </dd>
<dd><code>InferenceOption.BOUND_OPTION.NPU_2</code> </dd>
<dd><code>InferenceOption.BOUND_OPTION.NPU_01</code>  </dd>
<dd><code>InferenceOption.BOUND_OPTION.NPU_12</code>  </dd>
<dd><code>InferenceOption.BOUND_OPTION.NPU_02</code>  </dd>
</dl>
</li>
<li>
<dl>
<dt>use onnx runtime library (<code>ORT</code>)</dt>
<dd><code>set_use_ort / get_use_ort</code>  </dd>
</dl>
</li>
</ul>
<p>NPU_ALL / NPU_0 / NPU_1 / NPU_2
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a># DX-RT imports
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>from dx_engine import InferenceEngine, InferenceOption
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>...
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a>if __name__ == &quot;__main__&quot;:
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>    ...
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a>    # inference option
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>    option = InferenceOption()
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a>
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a>    print(&quot;Inference Options:&quot;)
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a>
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a>    # select devices
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a>    option.devices = [0]
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#__codelineno-5-15"></a>
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#__codelineno-5-16"></a>    # NPU bound opion (NPU_ALL or NPU_0 or NPU_1 or NPU_2)
<a id="__codelineno-5-17" name="__codelineno-5-17" href="#__codelineno-5-17"></a>    option.bound_option = InferenceOption.BOUND_OPTION.NPU_ALL
<a id="__codelineno-5-18" name="__codelineno-5-18" href="#__codelineno-5-18"></a>
<a id="__codelineno-5-19" name="__codelineno-5-19" href="#__codelineno-5-19"></a>    # use ONNX Runtime (True or False)
<a id="__codelineno-5-20" name="__codelineno-5-20" href="#__codelineno-5-20"></a>    option.use_ort = False
<a id="__codelineno-5-21" name="__codelineno-5-21" href="#__codelineno-5-21"></a>
<a id="__codelineno-5-22" name="__codelineno-5-22" href="#__codelineno-5-22"></a>    # create inference engine instance with model
<a id="__codelineno-5-23" name="__codelineno-5-23" href="#__codelineno-5-23"></a>    with InferenceEngine(modelPath, option) as ie:
<a id="__codelineno-5-24" name="__codelineno-5-24" href="#__codelineno-5-24"></a>
<a id="__codelineno-5-25" name="__codelineno-5-25" href="#__codelineno-5-25"></a>        input = [np.zeros(ie.get_input_size(), dtype=np.uint8)]
<a id="__codelineno-5-26" name="__codelineno-5-26" href="#__codelineno-5-26"></a>
<a id="__codelineno-5-27" name="__codelineno-5-27" href="#__codelineno-5-27"></a>        # inference loop
<a id="__codelineno-5-28" name="__codelineno-5-28" href="#__codelineno-5-28"></a>        for i in range(loop_count):
<a id="__codelineno-5-29" name="__codelineno-5-29" href="#__codelineno-5-29"></a>
<a id="__codelineno-5-30" name="__codelineno-5-30" href="#__codelineno-5-30"></a>            # inference synchronously 
<a id="__codelineno-5-31" name="__codelineno-5-31" href="#__codelineno-5-31"></a>            # use only one npu core 
<a id="__codelineno-5-32" name="__codelineno-5-32" href="#__codelineno-5-32"></a>            # ownership of the outputs is transferred to the user 
<a id="__codelineno-5-33" name="__codelineno-5-33" href="#__codelineno-5-33"></a>            outputs = ie.run(input)
<a id="__codelineno-5-34" name="__codelineno-5-34" href="#__codelineno-5-34"></a>
<a id="__codelineno-5-35" name="__codelineno-5-35" href="#__codelineno-5-35"></a>            # post processing 
<a id="__codelineno-5-36" name="__codelineno-5-36" href="#__codelineno-5-36"></a>            #postProcessing(outputs)
<a id="__codelineno-5-37" name="__codelineno-5-37" href="#__codelineno-5-37"></a>            print(&quot;Inference outputs &quot;, i)
<a id="__codelineno-5-38" name="__codelineno-5-38" href="#__codelineno-5-38"></a>
<a id="__codelineno-5-39" name="__codelineno-5-39" href="#__codelineno-5-39"></a>    exit(0)
</code></pre></div></p>
<hr />
<h3 id="configuration&gt;_and&gt;_devicestatus">Configuration and DeviceStatus<a class="headerlink" href="#configuration&gt;_and&gt;_devicestatus" title="Permanent link">&para;</a></h3>
<p>This guide explains how to use the <code>Configuration</code> class to set up the inference engine and the <code>DeviceStatus</code> class to monitor hardware status.</p>
<h4 id="engine&gt;_configuration">Engine Configuration ⚙️<a class="headerlink" href="#engine&gt;_configuration" title="Permanent link">&para;</a></h4>
<p>The <code>Configuration</code> class allows you to set engine parameters and retrieve version information before running inference.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="c1"># Create a configuration object</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="n">config</span> <span class="o">=</span> <span class="n">Configuration</span><span class="p">()</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="c1"># Enable options like showing model details or profiling information</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a><span class="n">config</span><span class="o">.</span><span class="n">set_enable</span><span class="p">(</span><span class="n">Configuration</span><span class="o">.</span><span class="n">ITEM</span><span class="o">.</span><span class="n">SHOW_MODEL_INFO</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a><span class="n">config</span><span class="o">.</span><span class="n">set_enable</span><span class="p">(</span><span class="n">Configuration</span><span class="o">.</span><span class="n">ITEM</span><span class="o">.</span><span class="n">SHOW_PROFILE</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a><span class="c1"># Retrieve version information</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Runtime framework version: &#39;</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">get_version</span><span class="p">())</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Device driver version: &#39;</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">get_driver_version</span><span class="p">())</span>
</code></pre></div>
<ul>
<li><strong><code>Configuration()</code></strong>: Creates an object to manage engine settings.</li>
<li><strong><code>config.set_enable(...)</code></strong>: Turns specific engine features on or off. In this case, it enables printing model information and performance profiles upon loading.</li>
<li><strong><code>config.get_version()</code></strong>: Fetches read-only information, such as software and driver versions.</li>
</ul>
<h4 id="querying&gt;_device&gt;_status">Querying Device Status 🖥️<a class="headerlink" href="#querying&gt;_device&gt;_status" title="Permanent link">&para;</a></h4>
<p>The <code>DeviceStatus</code> class is used to get the real-time operational status of the NPU hardware, such as temperature and clock speed. This is typically done after inference is complete to check the hardware's state.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="c1"># Get the number of available devices</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="n">device_count</span> <span class="o">=</span> <span class="n">DeviceStatus</span><span class="o">.</span><span class="n">get_device_count</span><span class="p">()</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="c1"># Loop through each device</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">device_count</span><span class="p">):</span>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a>    <span class="c1"># Get a status snapshot for the current device</span>
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a>    <span class="n">device_status</span> <span class="o">=</span> <span class="n">DeviceStatus</span><span class="o">.</span><span class="n">get_current_status</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a>    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Device </span><span class="si">{</span><span class="n">device_status</span><span class="o">.</span><span class="n">get_id</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a>
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a>    <span class="c1"># Loop through each NPU core to get its metrics</span>
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#__codelineno-7-11"></a>    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="c1"># Assuming 3 cores for this example</span>
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#__codelineno-7-12"></a>        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#__codelineno-7-13"></a>            <span class="sa">f</span><span class="s1">&#39;   NPU Core </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s1"> &#39;</span>
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#__codelineno-7-14"></a>            <span class="sa">f</span><span class="s1">&#39;Temperature: </span><span class="si">{</span><span class="n">device_status</span><span class="o">.</span><span class="n">get_temperature</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="si">}</span><span class="s1"> &#39;</span>
<a id="__codelineno-7-15" name="__codelineno-7-15" href="#__codelineno-7-15"></a>            <span class="sa">f</span><span class="s1">&#39;Voltage: </span><span class="si">{</span><span class="n">device_status</span><span class="o">.</span><span class="n">get_npu_voltage</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="si">}</span><span class="s1"> &#39;</span>
<a id="__codelineno-7-16" name="__codelineno-7-16" href="#__codelineno-7-16"></a>            <span class="sa">f</span><span class="s1">&#39;Clock: </span><span class="si">{</span><span class="n">device_status</span><span class="o">.</span><span class="n">get_npu_clock</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span>
<a id="__codelineno-7-17" name="__codelineno-7-17" href="#__codelineno-7-17"></a>        <span class="p">)</span>
</code></pre></div>
<ul>
<li><strong><code>DeviceStatus.get_device_count()</code></strong>: A static method that returns the number of connected DEEPX devices.</li>
<li><strong><code>DeviceStatus.get_current_status(i)</code></strong>: Returns a status object containing a <strong>snapshot</strong> of the hardware metrics for device <code>i</code> at that moment.</li>
<li><strong><code>device_status.get_temperature(c)</code></strong>: An instance method that returns the temperature (in Celsius) for a specific NPU core <code>c</code>. The <code>get_npu_voltage</code> and <code>get_npu_clock</code> methods work similarly.</li>
</ul>
<hr />
<h3 id="profiler&gt;_configuration">Profiler Configuration<a class="headerlink" href="#profiler&gt;_configuration" title="Permanent link">&para;</a></h3>
<p>This guide provides a simple, code-focused manual on how to configure the profiler using the DXRT Python wrapper. The profiler is a powerful tool for analyzing the performance of each layer within your model.</p>
<p>Configuration is managed through an instance of the <code>Configuration</code> class.</p>
<h4 id="enabling&gt;_the&gt;_profiler">Enabling the Profiler<a class="headerlink" href="#enabling&gt;_the&gt;_profiler" title="Permanent link">&para;</a></h4>
<p>Before you can use any profiler features, you must first create a <code>Configuration</code> object and <strong>enable</strong> the profiler. This is the essential first step.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="c1"># Create a Configuration instance</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="n">config</span> <span class="o">=</span> <span class="n">Configuration</span><span class="p">()</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="c1"># Enable the profiler feature</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a><span class="n">config</span><span class="o">.</span><span class="n">set_enable</span><span class="p">(</span><span class="n">Configuration</span><span class="o">.</span><span class="n">ITEM</span><span class="o">.</span><span class="n">PROFILER</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><strong><code>config = Configuration()</code></strong>: Creates the object that controls system-wide settings for the runtime.</li>
<li><strong><code>set_enable()</code></strong>: This method activates or deactivates a specific DXRT feature.</li>
<li><strong><code>Configuration.ITEM.PROFILER</code></strong>: Specifies that the target feature is the profiler.</li>
<li><strong><code>True</code></strong>: Enables the profiler. Set to <code>False</code> to disable it.</li>
</ul>
<h4 id="configuration&gt;_options">Configuration Options<a class="headerlink" href="#configuration&gt;_options" title="Permanent link">&para;</a></h4>
<p>Once enabled, you can set specific attributes for the profiler's behavior using the same <code>config</code> object.</p>
<h5 id="displaying&gt;_profiler&gt;_data&gt;_in&gt;_the&gt;_console">Displaying Profiler Data in the Console<a class="headerlink" href="#displaying&gt;_profiler&gt;_data&gt;_in&gt;_the&gt;_console" title="Permanent link">&para;</a></h5>
<p>To see the profiling results printed directly to your console after the inference runs, use the <code>PROFILER_SHOW_DATA</code> attribute.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="c1"># Configure the profiler to print its report to the console</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="n">config</span><span class="o">.</span><span class="n">set_attribute</span><span class="p">(</span><span class="n">Configuration</span><span class="o">.</span><span class="n">ITEM</span><span class="o">.</span><span class="n">PROFILER</span><span class="p">,</span>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>                     <span class="n">Configuration</span><span class="o">.</span><span class="n">ATTRIBUTE</span><span class="o">.</span><span class="n">PROFILER_SHOW_DATA</span><span class="p">,</span> <span class="s2">&quot;ON&quot;</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><strong><code>set_attribute()</code></strong>: Sets a specific property for a DXRT feature.</li>
<li><strong><code>PROFILER_SHOW_DATA</code></strong>: The attribute to control console output.</li>
<li><strong><code>"ON"</code></strong>: A string value to enable this attribute. Use <code>"OFF"</code> to disable it.</li>
</ul>
<h5 id="saving&gt;_profiler&gt;_data&gt;_to&gt;_a&gt;_file">Saving Profiler Data to a File<a class="headerlink" href="#saving&gt;_profiler&gt;_data&gt;_to&gt;_a&gt;_file" title="Permanent link">&para;</a></h5>
<p>To save the profiling report to a file for later analysis, use the <code>PROFILER_SAVE_DATA</code> attribute. The resulting report is generated in the same folder with the name <strong><code>profiler.json</code></strong>. 📄</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="c1"># Configure the profiler to save its report to a file</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="n">config</span><span class="o">.</span><span class="n">set_attribute</span><span class="p">(</span><span class="n">Configuration</span><span class="o">.</span><span class="n">ITEM</span><span class="o">.</span><span class="n">PROFILER</span><span class="p">,</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>                     <span class="n">Configuration</span><span class="o">.</span><span class="n">ATTRIBUTE</span><span class="o">.</span><span class="n">PROFILER_SAVE_DATA</span><span class="p">,</span> <span class="s2">&quot;ON&quot;</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><strong><code>PROFILER_SAVE_DATA</code></strong>: The attribute to control file output.</li>
<li><strong><code>"ON"</code></strong>: A string value to enable file saving. Use <code>"OFF"</code> to disable it.</li>
</ul>
<h4 id="complete&gt;_code&gt;_example">Complete Code Example<a class="headerlink" href="#complete&gt;_code&gt;_example" title="Permanent link">&para;</a></h4>
<p>Here is a complete example showing how to apply all the configurations at the start of your script. These settings are applied globally, and any <code>InferenceEngine</code> instance created afterward will automatically use them.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>    <span class="c1"># Step 1: Create a Configuration instance and enable the profiler</span>
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a>    <span class="n">config</span> <span class="o">=</span> <span class="n">Configuration</span><span class="p">()</span>
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a>    <span class="n">config</span><span class="o">.</span><span class="n">set_enable</span><span class="p">(</span><span class="n">Configuration</span><span class="o">.</span><span class="n">ITEM</span><span class="o">.</span><span class="n">PROFILER</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a>
<a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a>    <span class="c1"># Step 2: Set attributes to show data in console and save to a file</span>
<a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a>    <span class="n">config</span><span class="o">.</span><span class="n">set_attribute</span><span class="p">(</span><span class="n">Configuration</span><span class="o">.</span><span class="n">ITEM</span><span class="o">.</span><span class="n">PROFILER</span><span class="p">,</span>
<a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a>                         <span class="n">Configuration</span><span class="o">.</span><span class="n">ATTRIBUTE</span><span class="o">.</span><span class="n">PROFILER_SHOW_DATA</span><span class="p">,</span> <span class="s2">&quot;ON&quot;</span><span class="p">)</span>
<a id="__codelineno-11-10" name="__codelineno-11-10" href="#__codelineno-11-10"></a>    <span class="n">config</span><span class="o">.</span><span class="n">set_attribute</span><span class="p">(</span><span class="n">Configuration</span><span class="o">.</span><span class="n">ITEM</span><span class="o">.</span><span class="n">PROFILER</span><span class="p">,</span>
<a id="__codelineno-11-11" name="__codelineno-11-11" href="#__codelineno-11-11"></a>                         <span class="n">Configuration</span><span class="o">.</span><span class="n">ATTRIBUTE</span><span class="o">.</span><span class="n">PROFILER_SAVE_DATA</span><span class="p">,</span> <span class="s2">&quot;ON&quot;</span><span class="p">)</span>
<a id="__codelineno-11-12" name="__codelineno-11-12" href="#__codelineno-11-12"></a>
<a id="__codelineno-11-13" name="__codelineno-11-13" href="#__codelineno-11-13"></a>    <span class="c1"># The configuration is now active.</span>
<a id="__codelineno-11-14" name="__codelineno-11-14" href="#__codelineno-11-14"></a>    <span class="c1"># ...</span>
<a id="__codelineno-11-15" name="__codelineno-11-15" href="#__codelineno-11-15"></a>
<a id="__codelineno-11-16" name="__codelineno-11-16" href="#__codelineno-11-16"></a>    <span class="c1"># Create an inference engine instance that will now be profiled</span>
<a id="__codelineno-11-17" name="__codelineno-11-17" href="#__codelineno-11-17"></a>    <span class="k">with</span> <span class="n">InferenceEngine</span><span class="p">(</span><span class="n">modelPath</span><span class="p">)</span> <span class="k">as</span> <span class="n">ie</span><span class="p">:</span>
<a id="__codelineno-11-18" name="__codelineno-11-18" href="#__codelineno-11-18"></a>        <span class="c1"># ... register callback and run inference loop ...</span>
</code></pre></div>
<hr />
<h3 id="multi-input&gt;_inference">Multi-input Inference<a class="headerlink" href="#multi-input&gt;_inference" title="Permanent link">&para;</a></h3>
<p>This guide explains various methods for performing inference on multi-input models using the <code>InferenceEngine</code>. The examples cover different input formats, synchronous and asynchronous execution, and batch processing.</p>
<h4 id="model&gt;_information">Model Information<a class="headerlink" href="#model&gt;_information" title="Permanent link">&para;</a></h4>
<p>Before running inference, it's useful to inspect the model's properties. The <code>print_model_info</code> function in the example script shows how to query the inference engine for details about the model's input and output tensors.</p>
<ul>
<li><strong><code>ie.is_multi_input_model()</code></strong>: Checks if the loaded model has multiple inputs.</li>
<li><strong><code>ie.get_input_tensor_count()</code></strong>: Gets the number of input tensors.</li>
<li><strong><code>ie.get_input_tensor_names()</code></strong>: Retrieves the names of all input tensors.</li>
<li><strong><code>ie.get_input_tensor_sizes()</code></strong>: Gets the size (in bytes) of each input tensor.</li>
<li><strong><code>ie.get_output_tensor_names()</code> / <code>ie.get_output_tensor_sizes()</code></strong>: Provide similar information for output tensors.</li>
</ul>
<!-- end list -->

<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">print_model_info</span><span class="p">(</span><span class="n">ie</span><span class="p">:</span> <span class="n">InferenceEngine</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>    <span class="k">if</span> <span class="n">ie</span><span class="o">.</span><span class="n">is_multi_input_model</span><span class="p">():</span>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor count: </span><span class="si">{</span><span class="n">ie</span><span class="o">.</span><span class="n">get_input_tensor_count</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>        <span class="n">input_names</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">get_input_tensor_names</span><span class="p">()</span>
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a>        <span class="n">input_sizes</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">get_input_tensor_sizes</span><span class="p">()</span>
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_names</span><span class="p">):</span>
<a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">input_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> bytes&quot;</span><span class="p">)</span>
</code></pre></div>
<h4 id="synchronous&gt;_single&gt;_inference">Synchronous Single Inference<a class="headerlink" href="#synchronous&gt;_single&gt;_inference" title="Permanent link">&para;</a></h4>
<p>These examples demonstrate different ways to run a single inference request synchronously.</p>
<h5 id="input&gt;_formats">Input Formats<a class="headerlink" href="#input&gt;_formats" title="Permanent link">&para;</a></h5>
<h6 id="a&gt;_dictionary&gt;_format&gt;_dictstr&gt;_npndarray">A. Dictionary Format (<code>Dict[str, np.ndarray]</code>)<a class="headerlink" href="#a&gt;_dictionary&gt;_format&gt;_dictstr&gt;_npndarray" title="Permanent link">&para;</a></h6>
<p>This is the most robust method. You provide a dictionary where keys are the tensor names and values are the <code>numpy</code> arrays. This format is not sensitive to the order of tensors.</p>
<ul>
<li><strong>API</strong>: <code>ie.run_multi_input(input_tensors)</code></li>
<li><strong>Use Case</strong>: Recommended for clarity and to avoid errors from tensor reordering.</li>
</ul>
<!-- end list -->

<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="c1"># Create input data</span>
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a><span class="n">input_names</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">get_input_tensor_names</span><span class="p">()</span>
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a><span class="n">input_sizes</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">get_input_tensor_sizes</span><span class="p">()</span>
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a><span class="n">input_tensors</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">create_dummy_input</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_names</span><span class="p">,</span> <span class="n">input_sizes</span><span class="p">)}</span>
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>
<a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a><span class="c1"># Run inference</span>
<a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a><span class="n">outputs</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">run_multi_input</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">)</span>
</code></pre></div>
<h6 id="b&gt;_list&gt;_format&gt;_listnpndarray">B. List Format (<code>List[np.ndarray]</code>)<a class="headerlink" href="#b&gt;_list&gt;_format&gt;_listnpndarray" title="Permanent link">&para;</a></h6>
<p>You provide a list of <code>numpy</code> arrays. The order of arrays in the list <strong>must</strong> match the order returned by <code>ie.get_input_tensor_names()</code>.</p>
<ul>
<li><strong>API</strong>: <code>ie.run(input_list)</code></li>
<li><strong>Use Case</strong>: When tensor order is known and fixed. Can be slightly more performant than the dictionary-based approach due to less overhead.</li>
</ul>
<!-- end list -->

<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="c1"># Create input data in the correct order</span>
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a><span class="n">input_sizes</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">get_input_tensor_sizes</span><span class="p">()</span>
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">create_dummy_input</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">input_sizes</span><span class="p">]</span>
<a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a>
<a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a><span class="c1"># Run inference</span>
<a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a><span class="n">outputs</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">input_list</span><span class="p">)</span>
</code></pre></div>
<h6 id="c&gt;_auto-split&gt;_concatenated&gt;_buffer">C. Auto-Split Concatenated Buffer<a class="headerlink" href="#c&gt;_auto-split&gt;_concatenated&gt;_buffer" title="Permanent link">&para;</a></h6>
<p>You provide a single, contiguous <code>numpy</code> array containing all input data concatenated together. The engine automatically splits this buffer into the correct tensor inputs based on their sizes. The concatenation order <strong>must</strong> match the order from <code>ie.get_input_tensor_names()</code>.</p>
<ul>
<li><strong>API</strong>: <code>ie.run(concatenated_input)</code></li>
<li><strong>Use Case</strong>: Efficient when input data is already in a single block or when interfacing with systems that provide data this way.</li>
</ul>
<!-- end list -->

<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="c1"># Create a single buffer with all input data concatenated</span>
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a><span class="n">total_input_size</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">get_input_size</span><span class="p">()</span>
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a><span class="n">concatenated_input</span> <span class="o">=</span> <span class="n">create_dummy_input</span><span class="p">(</span><span class="n">total_input_size</span><span class="p">)</span>
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>
<a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a><span class="c1"># Run inference</span>
<a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a><span class="n">outputs</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">concatenated_input</span><span class="p">)</span>
</code></pre></div>
<h5 id="output&gt;_buffer&gt;_management">Output Buffer Management<a class="headerlink" href="#output&gt;_buffer&gt;_management" title="Permanent link">&para;</a></h5>
<p>For each synchronous method, you can either let the engine allocate output memory automatically or provide pre-allocated buffers for performance gains.</p>
<ul>
<li>
<p><strong>Auto-Allocated Output (No Buffer Provided)</strong>: Simpler to use. The engine returns a new list of <code>numpy</code> arrays.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="c1"># Engine allocates and manages output memory</span>
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a><span class="n">outputs</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">run_multi_input</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<p><strong>User-Provided Output Buffers</strong>: More performant as it avoids repeated memory allocations. The user is responsible for creating a list of <code>numpy</code> arrays with the correct sizes.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="c1"># User creates the output buffers</span>
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a><span class="n">output_sizes</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">get_output_tensor_sizes</span><span class="p">()</span>
<a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a><span class="n">output_buffers</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">output_sizes</span><span class="p">]</span>
<a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a>
<a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a><span class="c1"># Run inference, placing results in the provided buffers</span>
<a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a><span class="n">outputs</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">run_multi_input</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">,</span> <span class="n">output_buffers</span><span class="o">=</span><span class="n">output_buffers</span><span class="p">)</span>
</code></pre></div>
</li>
</ul>
<h4 id="synchronous&gt;_batch&gt;_inference">Synchronous Batch Inference<a class="headerlink" href="#synchronous&gt;_batch&gt;_inference" title="Permanent link">&para;</a></h4>
<p>For processing multiple inputs at once to maximize throughput, you can use the batch inference capabilities of the <code>run</code> method. This is more efficient than running single inferences in a loop.</p>
<h6 id="a&gt;_explicit&gt;_batch&gt;_format&gt;_listlistnpndarray">A. Explicit Batch Format (<code>List[List[np.ndarray]]</code>)<a class="headerlink" href="#a&gt;_explicit&gt;_batch&gt;_format&gt;_listlistnpndarray" title="Permanent link">&para;</a></h6>
<p>This is the clearest way to represent a batch. The input is a list of lists, where the outer list represents the batch and each inner list contains all input tensors for a single sample.</p>
<ul>
<li><strong>API</strong>: <code>ie.run(batch_inputs, output_buffers=...)</code></li>
<li><strong>Input</strong>: A <code>List[List[np.ndarray]]</code>.</li>
<li><strong>Output</strong>: A <code>List[List[np.ndarray]]</code>.</li>
</ul>
<!-- end list -->

<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3</span>
<a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a><span class="n">input_sizes</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">get_input_tensor_sizes</span><span class="p">()</span>
<a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a><span class="n">batch_inputs</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
<a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a>    <span class="n">sample_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">create_dummy_input</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">input_sizes</span><span class="p">]</span>
<a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a>    <span class="n">batch_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample_inputs</span><span class="p">)</span>
<a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a>
<a id="__codelineno-18-8" name="__codelineno-18-8" href="#__codelineno-18-8"></a><span class="c1"># Output buffers must also match the batch structure</span>
<a id="__codelineno-18-9" name="__codelineno-18-9" href="#__codelineno-18-9"></a><span class="c1"># ... create batch_outputs ...</span>
<a id="__codelineno-18-10" name="__codelineno-18-10" href="#__codelineno-18-10"></a>
<a id="__codelineno-18-11" name="__codelineno-18-11" href="#__codelineno-18-11"></a><span class="c1"># Run batch inference</span>
<a id="__codelineno-18-12" name="__codelineno-18-12" href="#__codelineno-18-12"></a><span class="n">results</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">batch_inputs</span><span class="p">,</span> <span class="n">output_buffers</span><span class="o">=</span><span class="n">batch_outputs</span><span class="p">)</span>
</code></pre></div>
<h6 id="b&gt;_flattened&gt;_batch&gt;_format&gt;_listnpndarray">B. Flattened Batch Format (<code>List[np.ndarray]</code>)<a class="headerlink" href="#b&gt;_flattened&gt;_batch&gt;_format&gt;_listnpndarray" title="Permanent link">&para;</a></h6>
<p>As a convenience, the API can also accept a single "flattened" list of <code>numpy</code> arrays. The total number of arrays must be a multiple of the model's input tensor count. The engine will automatically group them into batches.</p>
<ul>
<li><strong>API</strong>: <code>ie.run(flattened_inputs, output_buffers=...)</code></li>
<li><strong>Input</strong>: A <code>List[np.ndarray]</code> containing <code>batch_size * num_input_tensors</code> arrays.</li>
<li><strong>Output</strong>: The result is still returned in the explicit batch format (<code>List[List[np.ndarray]]</code>).</li>
</ul>
<!-- end list -->

<div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3</span>
<a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a><span class="n">input_sizes</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">get_input_tensor_sizes</span><span class="p">()</span>
<a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a><span class="n">flattened_inputs</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
<a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a>    <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">input_sizes</span><span class="p">:</span>
<a id="__codelineno-19-6" name="__codelineno-19-6" href="#__codelineno-19-6"></a>        <span class="n">flattened_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">create_dummy_input</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
<a id="__codelineno-19-7" name="__codelineno-19-7" href="#__codelineno-19-7"></a>
<a id="__codelineno-19-8" name="__codelineno-19-8" href="#__codelineno-19-8"></a><span class="c1"># ... create flattened_output_buffers ...</span>
<a id="__codelineno-19-9" name="__codelineno-19-9" href="#__codelineno-19-9"></a>
<a id="__codelineno-19-10" name="__codelineno-19-10" href="#__codelineno-19-10"></a><span class="c1"># Run batch inference</span>
<a id="__codelineno-19-11" name="__codelineno-19-11" href="#__codelineno-19-11"></a><span class="n">results</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">flattened_inputs</span><span class="p">,</span> <span class="n">output_buffers</span><span class="o">=</span><span class="n">flattened_output_buffers</span><span class="p">)</span>
</code></pre></div>
<h4 id="asynchronous&gt;_inference">Asynchronous Inference<a class="headerlink" href="#asynchronous&gt;_inference" title="Permanent link">&para;</a></h4>
<p>Asynchronous APIs allow you to submit inference requests without blocking the calling thread. The results are returned later via a callback function. This is ideal for applications that need to remain responsive.</p>
<ul>
<li><strong>APIs</strong>:<ul>
<li><code>ie.run_async_multi_input(input_tensors, user_arg=...)</code></li>
<li><code>ie.run_async(input_data, user_arg=...)</code></li>
</ul>
</li>
<li><strong>Callback Registration</strong>: <code>ie.register_callback(callback_function)</code></li>
</ul>
<p>The <code>AsyncInferenceHandler</code> class in the example demonstrates how to manage state across multiple asynchronous calls.</p>
<ul>
<li><strong>Register a Callback</strong>: Provide a function that the engine will call upon completion of each async request. The callback receives the output arrays and a <code>user_arg</code> for context.</li>
<li><strong>Submit Requests</strong>: Call an <code>run_async</code> variant. This call returns immediately with a job ID.</li>
<li><strong>Process in Callback</strong>: The callback function is executed in a separate worker thread. Here, you can process the results. It's crucial to ensure thread safety (e.g., using a <code>threading.Lock</code>) if you modify shared data.</li>
</ul>
<!-- end list -->

<div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a><span class="c1"># 1. Create a handler and register its callback method</span>
<a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a><span class="n">handler</span> <span class="o">=</span> <span class="n">AsyncInferenceHandler</span><span class="p">(</span><span class="n">async_count</span><span class="p">)</span>
<a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a><span class="n">ie</span><span class="o">.</span><span class="n">register_callback</span><span class="p">(</span><span class="n">handler</span><span class="o">.</span><span class="n">callback</span><span class="p">)</span>
<a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a>
<a id="__codelineno-20-5" name="__codelineno-20-5" href="#__codelineno-20-5"></a><span class="c1"># 2. Submit multiple async requests in a loop</span>
<a id="__codelineno-20-6" name="__codelineno-20-6" href="#__codelineno-20-6"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">async_count</span><span class="p">):</span>
<a id="__codelineno-20-7" name="__codelineno-20-7" href="#__codelineno-20-7"></a>    <span class="n">user_arg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;async_sample_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span>
<a id="__codelineno-20-8" name="__codelineno-20-8" href="#__codelineno-20-8"></a>    <span class="c1"># Each call is non-blocking</span>
<a id="__codelineno-20-9" name="__codelineno-20-9" href="#__codelineno-20-9"></a>    <span class="n">job_id</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">run_async_multi_input</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">,</span> <span class="n">user_arg</span><span class="o">=</span><span class="n">user_arg</span><span class="p">)</span>
<a id="__codelineno-20-10" name="__codelineno-20-10" href="#__codelineno-20-10"></a>
<a id="__codelineno-20-11" name="__codelineno-20-11" href="#__codelineno-20-11"></a><span class="c1"># 3. Wait for all callbacks to complete</span>
<a id="__codelineno-20-12" name="__codelineno-20-12" href="#__codelineno-20-12"></a><span class="n">handler</span><span class="o">.</span><span class="n">wait_for_completion</span><span class="p">()</span>
<a id="__codelineno-20-13" name="__codelineno-20-13" href="#__codelineno-20-13"></a>
<a id="__codelineno-20-14" name="__codelineno-20-14" href="#__codelineno-20-14"></a><span class="c1"># 4. Clear the callback when done</span>
<a id="__codelineno-20-15" name="__codelineno-20-15" href="#__codelineno-20-15"></a><span class="n">ie</span><span class="o">.</span><span class="n">register_callback</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
<hr />
<h3 id="examples">Examples<a class="headerlink" href="#examples" title="Permanent link">&para;</a></h3>
<p>The examples provided earlier are actual code samples that can be executed. Please refer to them for practical use. (<code>examples/python</code>)  </p>
<ul>
<li>
<dl>
<dt><code>run_async_model.py</code></dt>
<dd>A performance-optimized example using a callback function  </dd>
</dl>
</li>
<li>
<dl>
<dt><code>run_async_model_thread.py</code></dt>
<dd>An example using a single inference engine, callback function, and thread  </dd>
<dd>Usage method when there is a single AI model and multiple inputs  </dd>
</dl>
</li>
<li>
<dl>
<dt><code>run_async_model_wait.py</code></dt>
<dd>An example using threads and waits  </dd>
</dl>
</li>
<li>
<dl>
<dt><code>run_async_model_conf.py</code></dt>
<dd>An example using configuration and device status  </dd>
</dl>
</li>
<li>
<dl>
<dt><code>run_async_model_profiler.py</code></dt>
<dd>An example using profiler configuration </dd>
</dl>
</li>
<li>
<dl>
<dt><code>run_sync_model.py</code></dt>
<dd>An example using a single thread  </dd>
</dl>
</li>
<li>
<dl>
<dt><code>run_sync_model_thread.py</code></dt>
<dd>An example running an inference engine on multiple threads  </dd>
</dl>
</li>
<li>
<dl>
<dt><code>run_sync_model_bound.py</code></dt>
<dd>An example of specifying an NPU using the bound option  </dd>
</dl>
</li>
<li>
<dl>
<dt><code>multi_input_model_inference.py</code></dt>
<dd>An example of using multi-input model inference  </dd>
</dl>
</li>
</ul>
<hr />












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      ⓒ Copyright 2025 <a href="http://deepx.ai">DEEPX</a>. All Rights Reserved.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/company/deepx-corporation/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["toc.integrate", "content.code.annotate", "navigation.tabs", "navigation.tabs.sticky", "content.tabs.link", "content.code.copy"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>