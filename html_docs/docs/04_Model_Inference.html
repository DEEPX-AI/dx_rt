
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="03_Installation_on_Windows.html">
      
      
        <link rel="next" href="05_Command_Line_Interface.html">
      
      
      <link rel="icon" href="../img/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Model Inference - DXNN Runtime (DX-RT) User Manual</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+Display:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Noto Sans Display";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#model>_file>_format" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="DXNN Runtime (DX-RT) User Manual" class="md-header__button md-logo" aria-label="DXNN Runtime (DX-RT) User Manual" data-md-component="logo">
      
  <img src="../img/deepx.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            DXNN Runtime (DX-RT) User Manual
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Model Inference
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="01_DXNN_Runtime_Overview.html" class="md-tabs__link">
        
  
  
    
  
  DXNN Runtime Overview

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="02_Installation_on_Linux.html" class="md-tabs__link">
        
  
  
    
  
  Installation on Linux

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="03_Installation_on_Windows.html" class="md-tabs__link">
        
  
  
    
  
  Installation on Windows

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="04_Model_Inference.html" class="md-tabs__link">
        
  
  
    
  
  Model Inference

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="05_Command_Line_Interface.html" class="md-tabs__link">
        
  
  
    
  
  Command Line Interface

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="06_01_C%2B%2B_Tutorials.html" class="md-tabs__link">
          
  
  
  Tutorials

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="07_01_C%2B%2B_API_Reference.html" class="md-tabs__link">
          
  
  
  API Reference

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="Appendix_Change_Log.html" class="md-tabs__link">
        
  
  
    
  
  Change Log

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="DXNN Runtime (DX-RT) User Manual" class="md-nav__button md-logo" aria-label="DXNN Runtime (DX-RT) User Manual" data-md-component="logo">
      
  <img src="../img/deepx.png" alt="logo">

    </a>
    DXNN Runtime (DX-RT) User Manual
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="01_DXNN_Runtime_Overview.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DXNN Runtime Overview
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="02_Installation_on_Linux.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation on Linux
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="03_Installation_on_Windows.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation on Windows
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Model Inference
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="04_Model_Inference.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Model Inference
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#model>_file>_format" class="md-nav__link">
    <span class="md-ellipsis">
      Model File Format
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inference>_workflow" class="md-nav__link">
    <span class="md-ellipsis">
      Inference Workflow
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Inference Workflow">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prepare>_the>_model" class="md-nav__link">
    <span class="md-ellipsis">
      Prepare the Model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure>_inference>_options" class="md-nav__link">
    <span class="md-ellipsis">
      Configure Inference Options
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load>_the>_model>_into>_the>_inference>_engine" class="md-nav__link">
    <span class="md-ellipsis">
      Load the Model into the Inference Engine
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#connect>_input>_tensors" class="md-nav__link">
    <span class="md-ellipsis">
      Connect Input Tensors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    <span class="md-ellipsis">
      Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#process>_output>_tensors" class="md-nav__link">
    <span class="md-ellipsis">
      Process Output Tensors
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multiple>_device>_inference" class="md-nav__link">
    <span class="md-ellipsis">
      Multiple Device Inference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data>_format>_of>_device>_tensor" class="md-nav__link">
    <span class="md-ellipsis">
      Data Format of Device Tensor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#profile>_application" class="md-nav__link">
    <span class="md-ellipsis">
      Profile Application
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Profile Application">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gather>_timing>_data>_per>_event" class="md-nav__link">
    <span class="md-ellipsis">
      Gather Timing Data per Event
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#visualize>_profiler>_data" class="md-nav__link">
    <span class="md-ellipsis">
      Visualize Profiler Data
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how>_to>_create>_an>_application>_using>_dx-rt" class="md-nav__link">
    <span class="md-ellipsis">
      How To Create an Application Using DX-RT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optional>_improving>_cpu>_task>_throughput>_with>_dxrt>_dynamic>_cpu>_thread" class="md-nav__link">
    <span class="md-ellipsis">
      (Optional) Improving CPU Task Throughput with DXRT_DYNAMIC_CPU_THREAD
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="05_Command_Line_Interface.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Command Line Interface
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="06_01_C%2B%2B_Tutorials.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    C++ Tutorials
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="06_02_Python_Tutorials.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python Tutorials
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    API Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="07_01_C%2B%2B_API_Reference.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    C++ API Reference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="07_02_Python_API_Reference.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python API Reference
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="Appendix_Change_Log.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Change Log
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>Model Inference</h1>

<h2 id="model&gt;_file&gt;_format">Model File Format<a class="headerlink" href="#model&gt;_file&gt;_format" title="Permanent link">&para;</a></h2>
<p>The original ONNX model is converted by <strong>DX-COM</strong> into the following structure.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>Model dir.
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    └── graph.dxnn
</code></pre></div>
<ul>
<li>
<dl>
<dt><code>graph.dxnn</code></dt>
<dd>A unified DEEPX artifact that contains  NPU command data, model metadata, model parameters.  </dd>
</dl>
</li>
</ul>
<p>This file is used directly for inference on DEEPX hardware or simulator.  </p>
<hr />
<h2 id="inference&gt;_workflow">Inference Workflow<a class="headerlink" href="#inference&gt;_workflow" title="Permanent link">&para;</a></h2>
<p>Here the inference workflow using the DXNN Runtime as follows.  </p>
<div class="center-text">
<p align="center">
<img src="./../resources/04_02_Inference_Workflow.png" alt="Inference Workflow" width="800px">  
<br>
Figure. Inference Workflow  
<br><br>
</p>
</div>

<ul>
<li><strong>1</strong>. Compiled Model and optional InferenceOption are provided to initialize the InferenceEngine.  </li>
<li><strong>2</strong>. Pre-processed Input Tensors are passed to the InferenceEngine for inference.  </li>
<li><strong>3</strong>. The InferenceEngine produces Output Tensors as a result of the inference.  </li>
<li><strong>4</strong>. These outputs are then passed to the Post-Processing stage for interpretation or further action.  </li>
</ul>
<h3 id="prepare&gt;_the&gt;_model">Prepare the Model<a class="headerlink" href="#prepare&gt;_the&gt;_model" title="Permanent link">&para;</a></h3>
<p>Choose one of the following options.  </p>
<ul>
<li>Use a pre-built model from <strong>DX ModelZoo</strong>  </li>
<li>Compile an ONNX model into the <strong>DX-RT</strong> format using <strong>DX-COM</strong> (Refer to the <strong>DX-COM User Guide</strong> for details.)  </li>
</ul>
<h3 id="configure&gt;_inference&gt;_options">Configure Inference Options<a class="headerlink" href="#configure&gt;_inference&gt;_options" title="Permanent link">&para;</a></h3>
<p>Create a <code>dxrt::InferenceOption</code> object to configure runtime settings for the inference engine.  </p>
<p><strong>Note.</strong> This option is temporarily unsupported in the current version, and will be available in the next release.</p>
<h3 id="load&gt;_the&gt;_model&gt;_into&gt;_the&gt;_inference&gt;_engine">Load the Model into the Inference Engine<a class="headerlink" href="#load&gt;_the&gt;_model&gt;_into&gt;_the&gt;_inference&gt;_engine" title="Permanent link">&para;</a></h3>
<p>Create a <code>dxrt::InferenceEngine</code> instance using the path to the compiled model directory. Hardware resources are automatically initialized during this step.  </p>
<p>If <code>dxrt::InferenceEngine</code> is <strong>not</strong> provided, a default option is applied.  </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>auto ie = dxrt::InferenceEngine(&quot;yolov5s.dxnn&quot;);
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>auto ie = dxrt::InferenceEngine(&quot;yolov5s.dxnn&quot;, &amp;option);
</code></pre></div>
<h3 id="connect&gt;_input&gt;_tensors">Connect Input Tensors<a class="headerlink" href="#connect&gt;_input&gt;_tensors" title="Permanent link">&para;</a></h3>
<p>Prepare input buffers for inference.  </p>
<p>The following example shows how to initialize the buffer with the appropriate size.  </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>std::vector&lt;uint8_t&gt; inputBuf(ie.GetInputSize(), 0);  
</code></pre></div>
<p>Refer to <strong>DX-APP User Guide</strong> for practical examples on connecting inference engines to image sources such as cameras or video, along with the preprocessing routines. </p>
<h3 id="inference">Inference<a class="headerlink" href="#inference" title="Permanent link">&para;</a></h3>
<p><strong>DX-RT</strong> provides both synchronous and asynchronous execution modes for flexible inference handling.  </p>
<p><strong>1. Run - Synchronous Execution</strong><br />
Use the <code>dxrt::InferenceEngine::Run()</code> method for blocking, single-core inference.  </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>auto outputs = ie.Run(inputBuf.data());
</code></pre></div>
<ul>
<li>This method processes input and output on the same thread.  </li>
<li>This method is suitable for simple and sequential workloads.  </li>
</ul>
<p><strong>2. Run - Asynchronous Execution</strong>  </p>
<p><strong>a.</strong> With <code>Wait()</code>  </p>
<p>Use <code>RunAsync()</code> to perform the inference in non-blocking mode, and retrieve results later with <code>Wait()</code>.  </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>auto jobId = ie.RunAsync(inputBuf.data());
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>auto outputs = ie.Wait(jobId);
</code></pre></div>
<ul>
<li>This method is ideal for parallel workloads where inference can run in the background.  </li>
<li>This method is continuously executed while waiting for the result.  </li>
</ul>
<p><strong>b.</strong> With Callback  </p>
<p>Use a callback function to handle output as soon as inference completes. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>std::function&lt;int(vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt;, void*)&gt; postProcCallBack = \
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>    [&amp;](vector&lt;shared_ptr&lt;dxrt::Tensor&gt;&gt; outputs, void *arg)
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>    {
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>        /* Process output tensors here */
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a>        ... ...
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>        return 0;
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a>    };
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a>ie.RegisterCallback(postProcCallBack)
</code></pre></div>
<ul>
<li>The callback is triggered by a background thread after inference.  </li>
<li>You can pass a custom argument to track input/output pairs.</li>
</ul>
<p><strong>Note.</strong> Output data is <strong>only</strong> valid within the callback scope.  </p>
<h3 id="process&gt;_output&gt;_tensors">Process Output Tensors<a class="headerlink" href="#process&gt;_output&gt;_tensors" title="Permanent link">&para;</a></h3>
<p>Once inference is complete, the output tensors are processed using Tensor APIs and custom post-processing logic. You can find the templates and example code in <strong>DX-APP</strong> to help you implement post-process smoothly.<br />
As noted earlier, using callbacks allows for more efficient and real-time post-processing.  </p>
<hr />
<h2 id="multiple&gt;_device&gt;_inference">Multiple Device Inference<a class="headerlink" href="#multiple&gt;_device&gt;_inference" title="Permanent link">&para;</a></h2>
<p>This feature is <strong>not</strong> applicable to single-NPU devices. Basically, the inference engine schedules and manages multiple devices in real time.<br />
If the inference option is explicitly set, the inference engine may <strong>only</strong> use specific devices during real-time inference for the model.  </p>
<hr />
<h2 id="data&gt;_format&gt;_of&gt;_device&gt;_tensor">Data Format of Device Tensor<a class="headerlink" href="#data&gt;_format&gt;_of&gt;_device&gt;_tensor" title="Permanent link">&para;</a></h2>
<p>Compiled models use the <strong>NHWC</strong> format by default.  </p>
<p>However, the input tensor formats on the device side may vary depending on the hardware’s processing type.  </p>
<p><strong>Input Tensor Formats</strong> </p>
<table>
<thead>
<tr>
<th><strong>Type</strong></th>
<th><strong>Compiled Model Format</strong></th>
<th><strong>Device Format</strong></th>
<th><strong>Data Size</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Formatter</code></td>
<td><code>[N, H, W, C]</code></td>
<td><code>[N, H, W, C]</code></td>
<td>8-bit</td>
</tr>
<tr>
<td><code>IM2COL</code></td>
<td><code>[N, H, W, C]</code></td>
<td><code>[N, H, align64(W*C)]</code></td>
<td>8-bit</td>
</tr>
</tbody>
</table>
<ul>
<li>Formatter Type Example: <code>[1, 3, 224, 224] (NCHW) -&gt; [1, 224, 224, 3] (NHWC)</code>  </li>
<li>IM2COL Type Example: <code>[1, 3, 224, 224] (NCHW) -&gt; [1, 224, 224*3+32] (NH, aligned width x channel)</code>  </li>
</ul>
<p><strong>Output Tensor Formats</strong> </p>
<p>The output tensor format is also aligned with the NHWC format, but with padding applied for alignment.</p>
<table>
<thead>
<tr>
<th><strong>Type</strong></th>
<th><strong>Compiled Model Format</strong></th>
<th><strong>Device Format</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Aligned NHWC</code></td>
<td><code>[N, H, W, C]</code></td>
<td><code>[N, H, W, align64(C)]</code></td>
</tr>
</tbody>
</table>
<ul>
<li>Output Example: <code>[1, 40, 52, 36] (NCHW) -&gt; [1, 52, 36, 40+24]</code>
   (Channel size is aligned for optimal memory access.)  </li>
</ul>
<p>Post-processing can be performed directly without converting formats.<br />
API to convert from device format to <strong>NCHW/NHWC</strong> format will be supported in the next release.  </p>
<hr />
<h2 id="profile&gt;_application">Profile Application<a class="headerlink" href="#profile&gt;_application" title="Permanent link">&para;</a></h2>
<h3 id="gather&gt;_timing&gt;_data&gt;_per&gt;_event">Gather Timing Data per Event<a class="headerlink" href="#gather&gt;_timing&gt;_data&gt;_per&gt;_event" title="Permanent link">&para;</a></h3>
<p>You can profile events within your application using the Profiler APIs. Please refer to <strong>Section 8. API reference</strong>.  </p>
<p>Here is a basic usage example. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>auto&amp; profiler = dxrt::Profiler::GetInstance();
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>profiler.Start(&quot;1sec&quot;);
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>sleep(1);
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>profiler.End(&quot;1sec&quot;);
</code></pre></div>
<p>After the application is finished, <code>profiler.json</code> is created in the working directory.</p>
<h3 id="visualize&gt;_profiler&gt;_data">Visualize Profiler Data<a class="headerlink" href="#visualize&gt;_profiler&gt;_data" title="Permanent link">&para;</a></h3>
<p>You can visualize the profiling results using the following Python script.  </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>python3 tool/profiler/plot.py --input profiler.json
</code></pre></div>
<p>This generates an image file named <code>profiler.png</code>, providing a detailed view of runtime event timing for performance analysis. </p>
<div class="center-text">
<p align="center">
<img src="./../resources/04_05_03_DX-RT_Profiling_Report.png" alt="DX-RT Profiling Report" width="700px">  
<br>
Figure. DX-RT Profiling Report  
<br><br>
</p>
</div>

<p><strong>Script Usage:</strong> <code>tool/profiler/plot.py</code>  </p>
<p>Use this script to draw a timing chart from profiling data generated by <strong>DX-RT</strong>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>usage: plot.py [-h] [-i INPUT] [-o OUTPUT] [-s START] [-e END] [-g]
</code></pre></div>
<p>Optional Arguments  </p>
<ul>
<li><code>-h, --help</code>: Show help message and exit  </li>
<li><code>-i INPUT, --input INPUT</code>: Input <code>.json</code> file to visualize (e.g., <code>profiler.json</code>)  </li>
<li><code>-o OUTPUT, --output OUTPUT</code>: Output image file name to save (e.g., profiler.png)  </li>
<li><code>-s START, --start START</code>: Starting position (normalized, &gt; 0.0) within the time interval [0.0-1.0]  </li>
<li><code>-e END, --end END</code>: End position (normalized, &lt; 1.0) within the time interval [0.0-1.0]  </li>
<li><code>-g, --show_gap</code>: Show time gaps between the start point of each event  </li>
</ul>
<p>Please refer to usage of <code>tool/profiler/plot.py</code>.  </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>usage: plot.py [-h] [-i INPUT] [-o OUTPUT] [-s START] [-e END] [-g]
</code></pre></div>
<hr />
<h2 id="how&gt;_to&gt;_create&gt;_an&gt;_application&gt;_using&gt;_dx-rt">How To Create an Application Using DX-RT<a class="headerlink" href="#how&gt;_to&gt;_create&gt;_an&gt;_application&gt;_using&gt;_dx-rt" title="Permanent link">&para;</a></h2>
<p>This guide provides step-by-step instructions for creating a new CMake project using the <strong>DX-RT</strong> library.</p>
<p><strong>1. Build the DX-RT Library</strong> <br />
Before starting, make sure the <strong>DX-RT</strong> library is already built.  </p>
<p>Refer to <strong>Chapter 2. Installation on Linus</strong> and <strong>Chapter 3. Installation on Windows</strong> for detailed build instructions. </p>
<p><strong>2. Create a New CMake Project</strong> <br />
Create a project directory and an initial <code>CMakeLists.txt</code> file.<br />
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>mkdir MyProject
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>cd MyProject
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>touch CMakeLists.txt
</code></pre></div></p>
<p><strong>3. “Hello World” with DX-RT API</strong><br />
Create a simple source file (<code>main.cpp</code>) that uses a <strong>DX-RT</strong> API.  </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>#include &quot;dxrt/dxrt_api.h&quot;
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>using namespace std;
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a>int main(int argc, char *argv[])
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a>{
<a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a> auto&amp; devices = dxrt::CheckDevices();
<a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a> cout &lt;&lt; &quot;hello, world&quot; &lt;&lt; endl;
<a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a> return 0;
<a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a>}
</code></pre></div>
<p><strong>4. Modify CMakeLists.txt</strong><br />
Edit the <code>CMakeLists.txt</code> file as follows.  </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>cmake_minimum_required(VERSION 3.14)
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>project(app_template)
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>set(CMAKE_CXX_STANDARD_REQUIRED &quot;ON&quot;)
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a>set(CMAKE_CXX_STANDARD &quot;14&quot;)
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a>
<a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a># Set the DX-RT library installation path (adjust as needed)
<a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a>set(DXRT_LIB_PATH &quot;/usr/local/lib&quot;) 
<a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a>
<a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a># Locate the DX-RT library
<a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a>find_library(DXRT_LIBRARY REQUIRED NAMES dxrt_${CMAKE_SYSTEM_PROCESSOR} PATHS $
<a id="__codelineno-12-12" name="__codelineno-12-12" href="#__codelineno-12-12"></a>{DXRT_LIB_PATH})
<a id="__codelineno-12-13" name="__codelineno-12-13" href="#__codelineno-12-13"></a>
<a id="__codelineno-12-14" name="__codelineno-12-14" href="#__codelineno-12-14"></a># Add executable and link libraries
<a id="__codelineno-12-15" name="__codelineno-12-15" href="#__codelineno-12-15"></a>add_executable(HelloWorld main.cpp)
<a id="__codelineno-12-16" name="__codelineno-12-16" href="#__codelineno-12-16"></a>target_link_libraries(HelloWorld PRIVATE ${DXRT_LIBRARY} protobuf)
</code></pre></div>
<p>Replace <code>/usr/local/lib</code> with the actual path where the <strong>DX-RT</strong> library is installed.</p>
<p><strong>5. Build the Project</strong><br />
Compile your project using the following commands.<br />
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>mkdir build
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>cd build
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>cmake ..
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a>make
</code></pre></div></p>
<p><strong>6. Run the Executable</strong><br />
After a successful build, run the generated executable.   </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>./HelloWorld
</code></pre></div>
<p>You now successfully create and build a CMake project using the <strong>DX-RT</strong> library. </p>
<hr />
<h2 id="optional&gt;_improving&gt;_cpu&gt;_task&gt;_throughput&gt;_with&gt;_dxrt&gt;_dynamic&gt;_cpu&gt;_thread">(Optional) Improving CPU Task Throughput with DXRT_DYNAMIC_CPU_THREAD<a class="headerlink" href="#optional&gt;_improving&gt;_cpu&gt;_task&gt;_throughput&gt;_with&gt;_dxrt&gt;_dynamic&gt;_cpu&gt;_thread" title="Permanent link">&para;</a></h2>
<p>The USE_ORT option allows for enabling ONNX Runtime to handle operations that are not supported by the NPU. 
When this option is active, the model's CPU tasks are executed via ONNX Runtime. </p>
<p>To mitigate potential bottlenecks in these CPU tasks, especially under varying Host CPU conditions, an optional dynamic multi-threading feature is provided. This feature monitors the input queue load to identify CPU task congestion. If a high load is detected, it dynamically increases the number of threads allocated to CPU tasks, thereby improving their throughput. This dynamic CPU threading can be enabled by setting the DXRT_DYNAMIC_CPU_THREAD=ON environment variable (e.g., export DXRT_DYNAMIC_CPU_THREAD=ON). </p>
<p>Additionally, if the system observes that CPU tasks are experiencing significant load, it will display a message: "To improve FPS, set: 'export DXRT_DYNAMIC_CPU_THREAD=ON'", recommending the activation of this feature for better performance.</p>
<p>Warning: Enabling the DXRT_DYNAMIC_CPU_THREAD=ON option does not always guarantee an FPS increase; its effectiveness can vary depending on the specific workload and system conditions.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      ⓒ Copyright 2025 <a href="http://deepx.ai">DEEPX</a>. All Rights Reserved.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/company/deepx-corporation/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["toc.integrate", "content.code.annotate", "navigation.tabs", "navigation.tabs.sticky", "content.tabs.link", "content.code.copy"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>