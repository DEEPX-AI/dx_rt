<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>DXRT: dxrt::InferenceEngine Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">DXRT
   </div>
   <div id="projectbrief">DEEPX Runtime SDK for AI inference on DEEPX devices.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacedxrt.html">dxrt</a></li><li class="navelem"><a class="el" href="classdxrt_1_1InferenceEngine.html">InferenceEngine</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classdxrt_1_1InferenceEngine-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">dxrt::InferenceEngine Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>This class abstracts runtime inference executor for user's compiled model.  
 <a href="classdxrt_1_1InferenceEngine.html#details">More...</a></p>

<p><code>#include &quot;<a class="el" href="dxrt__api_8h_source.html">dxrt/dxrt_api.h</a>&quot;</code></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:aed622c0fa566620899c9e7e26011bacd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aed622c0fa566620899c9e7e26011bacd">InferenceEngine</a> (const std::string &amp;modelPath, <a class="el" href="structdxrt_1_1InferenceOption.html">InferenceOption</a> &amp;option=<a class="el" href="namespacedxrt.html#aab5060680ba2f567f47f8868cd8b1f00">DefaultInferenceOption</a>)</td></tr>
<tr class="memdesc:aed622c0fa566620899c9e7e26011bacd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Perform the task of loading the model and configuring the NPU to run.  <a href="classdxrt_1_1InferenceEngine.html#aed622c0fa566620899c9e7e26011bacd">More...</a><br /></td></tr>
<tr class="separator:aed622c0fa566620899c9e7e26011bacd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a88cea5324cbeb436034adb60cf31050f"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a88cea5324cbeb436034adb60cf31050f">Run</a> (void *inputPtr, void *userArg=nullptr, void *outputPtr=nullptr)</td></tr>
<tr class="memdesc:a88cea5324cbeb436034adb60cf31050f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Run inference engine using specific input pointer Synchronously.  <a href="classdxrt_1_1InferenceEngine.html#a88cea5324cbeb436034adb60cf31050f">More...</a><br /></td></tr>
<tr class="separator:a88cea5324cbeb436034adb60cf31050f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af3006c9a7183f7e42d1528aa8a6fea24"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#af3006c9a7183f7e42d1528aa8a6fea24">RunAsync</a> (void *inputPtr, void *userArg=nullptr, void *outputPtr=nullptr)</td></tr>
<tr class="memdesc:af3006c9a7183f7e42d1528aa8a6fea24"><td class="mdescLeft">&#160;</td><td class="mdescRight">Non-blocking call to request asynchronous inference by input pointer, and get job ID from inference engine.  <a href="classdxrt_1_1InferenceEngine.html#af3006c9a7183f7e42d1528aa8a6fea24">More...</a><br /></td></tr>
<tr class="separator:af3006c9a7183f7e42d1528aa8a6fea24"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab0c6629f048341d62bfa3a400c90e827"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ab0c6629f048341d62bfa3a400c90e827">RunBenchMark</a> (int num, void *inputPtr=nullptr)</td></tr>
<tr class="memdesc:ab0c6629f048341d62bfa3a400c90e827"><td class="mdescLeft">&#160;</td><td class="mdescRight">run benchmark with loop n times  <a href="classdxrt_1_1InferenceEngine.html#ab0c6629f048341d62bfa3a400c90e827">More...</a><br /></td></tr>
<tr class="separator:ab0c6629f048341d62bfa3a400c90e827"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aef4c9502a5d5a9d256d3899391376aba"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aef4c9502a5d5a9d256d3899391376aba">ValidateDevice</a> (void *inputPtr, int deviceId=0)</td></tr>
<tr class="memdesc:aef4c9502a5d5a9d256d3899391376aba"><td class="mdescLeft">&#160;</td><td class="mdescRight">Validate inference of a specific NPU device connected to the host. This function runs a validation process using the provided input data on the specified NPU device. It can be used to ensure that the NPU device is operational and can process inference tasks correctly.  <a href="classdxrt_1_1InferenceEngine.html#aef4c9502a5d5a9d256d3899391376aba">More...</a><br /></td></tr>
<tr class="separator:aef4c9502a5d5a9d256d3899391376aba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3d3fc75faa79470d9a3bdbdc61561a3a"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a3d3fc75faa79470d9a3bdbdc61561a3a">RegisterCallBack</a> (std::function&lt; int(TensorPtrs &amp;<a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a>, void *userArg)&gt; callbackFunc)</td></tr>
<tr class="memdesc:a3d3fc75faa79470d9a3bdbdc61561a3a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Register user callback function to be called by inference completion.  <a href="classdxrt_1_1InferenceEngine.html#a3d3fc75faa79470d9a3bdbdc61561a3a">More...</a><br /></td></tr>
<tr class="separator:a3d3fc75faa79470d9a3bdbdc61561a3a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2c6f130d722950153287d4cf1efd5a5d"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a2c6f130d722950153287d4cf1efd5a5d">Wait</a> (int jobId)</td></tr>
<tr class="memdesc:a2c6f130d722950153287d4cf1efd5a5d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Wait until an request is complete and returns output.  <a href="classdxrt_1_1InferenceEngine.html#a2c6f130d722950153287d4cf1efd5a5d">More...</a><br /></td></tr>
<tr class="separator:a2c6f130d722950153287d4cf1efd5a5d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afbee8b031fdd6c0a3916ed00d08c9cd8"><td class="memItemLeft" align="right" valign="top">Tensors&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#afbee8b031fdd6c0a3916ed00d08c9cd8">inputs</a> (void *ptr=nullptr, uint64_t phyAddr=0)</td></tr>
<tr class="memdesc:afbee8b031fdd6c0a3916ed00d08c9cd8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get input tensor.  <a href="classdxrt_1_1InferenceEngine.html#afbee8b031fdd6c0a3916ed00d08c9cd8">More...</a><br /></td></tr>
<tr class="separator:afbee8b031fdd6c0a3916ed00d08c9cd8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0893b724d6eed939556f1315ce05829d"><td class="memItemLeft" align="right" valign="top">Tensors&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> (void *ptr=nullptr, uint64_t phyAddr=0)</td></tr>
<tr class="memdesc:a0893b724d6eed939556f1315ce05829d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get output tensor.  <a href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">More...</a><br /></td></tr>
<tr class="separator:a0893b724d6eed939556f1315ce05829d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a96c4e5953a0daf4a1c886002c052a6ba"><td class="memItemLeft" align="right" valign="top">uint64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a96c4e5953a0daf4a1c886002c052a6ba">input_size</a> ()</td></tr>
<tr class="memdesc:a96c4e5953a0daf4a1c886002c052a6ba"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get total size of input tensors.  <a href="classdxrt_1_1InferenceEngine.html#a96c4e5953a0daf4a1c886002c052a6ba">More...</a><br /></td></tr>
<tr class="separator:a96c4e5953a0daf4a1c886002c052a6ba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a93e1d7b460b7f8656e725ed708de507b"><td class="memItemLeft" align="right" valign="top">uint64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a93e1d7b460b7f8656e725ed708de507b">output_size</a> ()</td></tr>
<tr class="memdesc:a93e1d7b460b7f8656e725ed708de507b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get total size of output tensors.  <a href="classdxrt_1_1InferenceEngine.html#a93e1d7b460b7f8656e725ed708de507b">More...</a><br /></td></tr>
<tr class="separator:a93e1d7b460b7f8656e725ed708de507b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3fb16094508bd367cc4971238f002765"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a3fb16094508bd367cc4971238f002765">name</a> ()</td></tr>
<tr class="memdesc:a3fb16094508bd367cc4971238f002765"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get model name.  <a href="classdxrt_1_1InferenceEngine.html#a3fb16094508bd367cc4971238f002765">More...</a><br /></td></tr>
<tr class="separator:a3fb16094508bd367cc4971238f002765"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a22087ed8a6839bfc1c4a6b9c21629236"><td class="memItemLeft" align="right" valign="top">std::vector&lt; string &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a22087ed8a6839bfc1c4a6b9c21629236">task_order</a> ()</td></tr>
<tr class="memdesc:a22087ed8a6839bfc1c4a6b9c21629236"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get model task order.  <a href="classdxrt_1_1InferenceEngine.html#a22087ed8a6839bfc1c4a6b9c21629236">More...</a><br /></td></tr>
<tr class="separator:a22087ed8a6839bfc1c4a6b9c21629236"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a990ca720e7f8a28ac5394206eb6f9491"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a990ca720e7f8a28ac5394206eb6f9491">latency</a> ()</td></tr>
<tr class="memdesc:a990ca720e7f8a28ac5394206eb6f9491"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get recent latency.  <a href="classdxrt_1_1InferenceEngine.html#a990ca720e7f8a28ac5394206eb6f9491">More...</a><br /></td></tr>
<tr class="separator:a990ca720e7f8a28ac5394206eb6f9491"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad2f3b616e0e996b66ef7df37dc16a85"><td class="memItemLeft" align="right" valign="top">uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aad2f3b616e0e996b66ef7df37dc16a85">inference_time</a> ()</td></tr>
<tr class="memdesc:aad2f3b616e0e996b66ef7df37dc16a85"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get recent inference time.  <a href="classdxrt_1_1InferenceEngine.html#aad2f3b616e0e996b66ef7df37dc16a85">More...</a><br /></td></tr>
<tr class="separator:aad2f3b616e0e996b66ef7df37dc16a85"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0b38cd5001ec327cbdc5b00a7143d0e2"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a0b38cd5001ec327cbdc5b00a7143d0e2">get_num_tails</a> ()</td></tr>
<tr class="memdesc:a0b38cd5001ec327cbdc5b00a7143d0e2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the number of tail tasks in the model.  <a href="classdxrt_1_1InferenceEngine.html#a0b38cd5001ec327cbdc5b00a7143d0e2">More...</a><br /></td></tr>
<tr class="separator:a0b38cd5001ec327cbdc5b00a7143d0e2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adb7b0c6f4a8411823d7eaed57cedd36d"><td class="memItemLeft" align="right" valign="top">string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#adb7b0c6f4a8411823d7eaed57cedd36d">get_compile_type</a> ()</td></tr>
<tr class="memdesc:adb7b0c6f4a8411823d7eaed57cedd36d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the compile type of the model.  <a href="classdxrt_1_1InferenceEngine.html#adb7b0c6f4a8411823d7eaed57cedd36d">More...</a><br /></td></tr>
<tr class="separator:adb7b0c6f4a8411823d7eaed57cedd36d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>This class abstracts runtime inference executor for user's compiled model. </p>
<p>After user loads compiled model to <a class="el" href="classdxrt_1_1InferenceEngine.html" title="This class abstracts runtime inference executor for user&#39;s compiled model.">InferenceEngine</a>, real-time device tasks will be scheduled by internal runtime libraries. It supports both inference mode (synchronous/asynchronous) according to user's request. </p><div class="fragment"><div class="line"><span class="comment">// Use default inference option</span></div>
<div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><span class="keyword">auto</span> ie = <a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a>(modelPath, <span class="keyword">nullptr</span>);</div>
</div><!-- fragment --> <div class="fragment"><div class="line"><span class="comment">// Use a new inference option</span></div>
<div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><a class="code" href="structdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a> option;</div>
<div class="line">option.<a class="code" href="structdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">devices</a> = {0,1,3};  <span class="comment">//use only 0,1,3 device</span></div>
<div class="line"><span class="keyword">auto</span> ie = <a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a>(modelPath, option);</div>
</div><!-- fragment --> </div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="aed622c0fa566620899c9e7e26011bacd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aed622c0fa566620899c9e7e26011bacd">&#9670;&nbsp;</a></span>InferenceEngine()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">dxrt::InferenceEngine::InferenceEngine </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>modelPath</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="structdxrt_1_1InferenceOption.html">InferenceOption</a> &amp;&#160;</td>
          <td class="paramname"><em>option</em> = <code><a class="el" href="namespacedxrt.html#aab5060680ba2f567f47f8868cd8b1f00">DefaultInferenceOption</a></code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Perform the task of loading the model and configuring the NPU to run. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">modelPath</td><td>model path </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">option</td><td>device and npu core options <div class="fragment"><div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a>(modelPath) ie(modelPath);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
<div class="line"> </div>
<div class="line"><a class="code" href="structdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a> op;</div>
<div class="line">op.<a class="code" href="structdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">devices</a>.push_back(0); </div>
<div class="line">op.<a class="code" href="structdxrt_1_1InferenceOption.html#a9fd270a5eeeb1a14ce5f133c16c30d55">boundOption</a> = dxrt::InferenceOption::BOUND_OPTION::NPU_0; <span class="comment">// NPU_0 only</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a> ie(modelPath, op);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
</div><!-- fragment --> </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="adb7b0c6f4a8411823d7eaed57cedd36d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adb7b0c6f4a8411823d7eaed57cedd36d">&#9670;&nbsp;</a></span>get_compile_type()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">string dxrt::InferenceEngine::get_compile_type </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the compile type of the model. </p>
<dl class="section return"><dt>Returns</dt><dd>The compile type of the model. </dd></dl>

</div>
</div>
<a id="a0b38cd5001ec327cbdc5b00a7143d0e2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0b38cd5001ec327cbdc5b00a7143d0e2">&#9670;&nbsp;</a></span>get_num_tails()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::get_num_tails </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the number of tail tasks in the model. </p>
<dl class="section return"><dt>Returns</dt><dd>The number of tasks that have no subsequent tasks.</dd></dl>
<p>Tail tasks are those which do not have any tasks following them in the model's task chain. This function provides the count of such tail tasks. </p>

</div>
</div>
<a id="aad2f3b616e0e996b66ef7df37dc16a85"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aad2f3b616e0e996b66ef7df37dc16a85">&#9670;&nbsp;</a></span>inference_time()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">uint32_t dxrt::InferenceEngine::inference_time </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get recent inference time. </p>
<dl class="section return"><dt>Returns</dt><dd>inference time (microseconds) </dd></dl>

</div>
</div>
<a id="a96c4e5953a0daf4a1c886002c052a6ba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a96c4e5953a0daf4a1c886002c052a6ba">&#9670;&nbsp;</a></span>input_size()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">uint64_t dxrt::InferenceEngine::input_size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get total size of input tensors. </p>
<dl class="section return"><dt>Returns</dt><dd>input size of one inference in bytes </dd></dl>

</div>
</div>
<a id="afbee8b031fdd6c0a3916ed00d08c9cd8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afbee8b031fdd6c0a3916ed00d08c9cd8">&#9670;&nbsp;</a></span>inputs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensors dxrt::InferenceEngine::inputs </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>ptr</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint64_t&#160;</td>
          <td class="paramname"><em>phyAddr</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">ptr</td><td>pointer to virtual address </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">phyAddr</td><td>pointer to physical address </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>if ptr is null, input memory area in engine is returned </dd>
<dd>
if ptr and phyAddr is given, inputs tensors that contains output addresses </dd></dl>

</div>
</div>
<a id="a990ca720e7f8a28ac5394206eb6f9491"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a990ca720e7f8a28ac5394206eb6f9491">&#9670;&nbsp;</a></span>latency()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::latency </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get recent latency. </p>
<dl class="section return"><dt>Returns</dt><dd>latency (microseconds) </dd></dl>

</div>
</div>
<a id="a3fb16094508bd367cc4971238f002765"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3fb16094508bd367cc4971238f002765">&#9670;&nbsp;</a></span>name()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::string dxrt::InferenceEngine::name </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get model name. </p>
<dl class="section return"><dt>Returns</dt><dd>model name </dd></dl>

</div>
</div>
<a id="a93e1d7b460b7f8656e725ed708de507b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a93e1d7b460b7f8656e725ed708de507b">&#9670;&nbsp;</a></span>output_size()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">uint64_t dxrt::InferenceEngine::output_size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get total size of output tensors. </p>
<dl class="section return"><dt>Returns</dt><dd>output size of one inference in bytes </dd></dl>

</div>
</div>
<a id="a0893b724d6eed939556f1315ce05829d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0893b724d6eed939556f1315ce05829d">&#9670;&nbsp;</a></span>outputs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensors dxrt::InferenceEngine::outputs </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>ptr</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint64_t&#160;</td>
          <td class="paramname"><em>phyAddr</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get output tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">ptr</td><td>pointer to virtual address </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">phyAddr</td><td>pointer to physical address </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>if ptr is null, output memory area in engine is returned </dd>
<dd>
if ptr and phyAddr is given, outputs tensors that contains output addresses </dd></dl>

</div>
</div>
<a id="a3d3fc75faa79470d9a3bdbdc61561a3a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3d3fc75faa79470d9a3bdbdc61561a3a">&#9670;&nbsp;</a></span>RegisterCallBack()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void dxrt::InferenceEngine::RegisterCallBack </td>
          <td>(</td>
          <td class="paramtype">std::function&lt; int(TensorPtrs &amp;<a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a>, void *userArg)&gt;&#160;</td>
          <td class="paramname"><em>callbackFunc</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Register user callback function to be called by inference completion. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">callbackFunc</td><td>Function which is called when inference is complete, it gets outputs and user_arg ptr </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">outputs</td><td>output tensors data </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">userArg</td><td>userArg given by <a class="el" href="classdxrt_1_1InferenceEngine.html#a88cea5324cbeb436034adb60cf31050f" title="Run inference engine using specific input pointer Synchronously.">Run()</a>; </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a88cea5324cbeb436034adb60cf31050f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a88cea5324cbeb436034adb60cf31050f">&#9670;&nbsp;</a></span>Run()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::Run </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userArg</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>outputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Run inference engine using specific input pointer Synchronously. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>input data pointer to run inference </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArg</td><td>user-defined arguments as a void pointer(e.g. original frame data, metadata about input, ... ) </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputPtr</td><td>pointer to output data, if it is nullptr, output data is stored in buffer inside DXRT. <div class="fragment"><div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a> ie(modelPath);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
</div><!-- fragment --> </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>output tensors as vector of smart pointer instances </dd></dl>

</div>
</div>
<a id="af3006c9a7183f7e42d1528aa8a6fea24"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af3006c9a7183f7e42d1528aa8a6fea24">&#9670;&nbsp;</a></span>RunAsync()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::RunAsync </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userArg</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>outputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Non-blocking call to request asynchronous inference by input pointer, and get job ID from inference engine. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>input data pointer to run inference </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArg</td><td>user-defined arguments as a void pointer(e.g. original frame data, metadata about input, ... ) </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputPtr</td><td>pointer to output data, if it is nullptr, output data area is allocated by DXRT. <div class="fragment"><div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model&quot;</span></div>
<div class="line"><a class="code" href="structdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a> option;</div>
<div class="line">option.<a class="code" href="structdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">devices</a> = {0,1,3};  <span class="comment">//use only 0,1,3 device</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a> ie(modelPath, option);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
</div><!-- fragment --> </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>job id that can be used to wait() function </dd></dl>

</div>
</div>
<a id="ab0c6629f048341d62bfa3a400c90e827"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab0c6629f048341d62bfa3a400c90e827">&#9670;&nbsp;</a></span>RunBenchMark()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">float dxrt::InferenceEngine::RunBenchMark </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>run benchmark with loop n times </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">num</td><td>number of inferences </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>input data pointer to run inference </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>average fps </dd></dl>

</div>
</div>
<a id="a22087ed8a6839bfc1c4a6b9c21629236"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a22087ed8a6839bfc1c4a6b9c21629236">&#9670;&nbsp;</a></span>task_order()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;string&gt; dxrt::InferenceEngine::task_order </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get model task order. </p>
<dl class="section return"><dt>Returns</dt><dd>task order </dd></dl>

</div>
</div>
<a id="aef4c9502a5d5a9d256d3899391376aba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aef4c9502a5d5a9d256d3899391376aba">&#9670;&nbsp;</a></span>ValidateDevice()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::ValidateDevice </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>deviceId</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Validate inference of a specific NPU device connected to the host. This function runs a validation process using the provided input data on the specified NPU device. It can be used to ensure that the NPU device is operational and can process inference tasks correctly. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>Pointer to the input data used for validation. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">deviceId</td><td>ID of the NPU device to validate. Default is 0 (first device). </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Output tensors as a vector of smart pointer instances, representing the validation results. </dd></dl>

</div>
</div>
<a id="a2c6f130d722950153287d4cf1efd5a5d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2c6f130d722950153287d4cf1efd5a5d">&#9670;&nbsp;</a></span>Wait()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::Wait </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>jobId</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Wait until an request is complete and returns output. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">jobId</td><td>job Id returned by <a class="el" href="classdxrt_1_1InferenceEngine.html#af3006c9a7183f7e42d1528aa8a6fea24" title="Non-blocking call to request asynchronous inference by input pointer, and get job ID from inference e...">RunAsync()</a> </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>output tensors as vector of smart pointer instances </dd></dl>

</div>
</div>
</div><!-- contents -->
<div class="ttc" id="astructdxrt_1_1InferenceOption_html"><div class="ttname"><a href="structdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a></div><div class="ttdoc">This struct specifies inference options applied to dxrt::InferenceEngine.</div><div class="ttdef"><b>Definition:</b> inference_option.h:14</div></div>
<div class="ttc" id="astructdxrt_1_1InferenceOption_html_a9fd270a5eeeb1a14ce5f133c16c30d55"><div class="ttname"><a href="structdxrt_1_1InferenceOption.html#a9fd270a5eeeb1a14ce5f133c16c30d55">dxrt::InferenceOption::boundOption</a></div><div class="ttdeci">uint32_t boundOption</div><div class="ttdoc">Select the NPU core inside the device.</div><div class="ttdef"><b>Definition:</b> inference_option.h:31</div></div>
<div class="ttc" id="astructdxrt_1_1InferenceOption_html_ae98837ff566fe2b089a5b1bf07feb48c"><div class="ttname"><a href="structdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">dxrt::InferenceOption::devices</a></div><div class="ttdeci">std::vector&lt; int &gt; devices</div><div class="ttdoc">device ID list to use</div><div class="ttdef"><b>Definition:</b> inference_option.h:26</div></div>
<div class="ttc" id="aclassdxrt_1_1InferenceEngine_html_a0893b724d6eed939556f1315ce05829d"><div class="ttname"><a href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">dxrt::InferenceEngine::outputs</a></div><div class="ttdeci">Tensors outputs(void *ptr=nullptr, uint64_t phyAddr=0)</div><div class="ttdoc">Get output tensor.</div></div>
<div class="ttc" id="aclassdxrt_1_1InferenceEngine_html"><div class="ttname"><a href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a></div><div class="ttdoc">This class abstracts runtime inference executor for user's compiled model.</div><div class="ttdef"><b>Definition:</b> inference_engine.h:60</div></div>
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
