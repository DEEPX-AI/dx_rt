<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>DXRT: dxrt::InferenceEngine Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">DXRT
   </div>
   <div id="projectbrief">DEEPX Runtime SDK for AI inference on DEEPX devices.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacedxrt.html">dxrt</a></li><li class="navelem"><a class="el" href="classdxrt_1_1InferenceEngine.html">InferenceEngine</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classdxrt_1_1InferenceEngine-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">dxrt::InferenceEngine Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>This class abstracts runtime inference executor for user's compiled model.  
 <a href="classdxrt_1_1InferenceEngine.html#details">More...</a></p>

<p><code>#include &quot;<a class="el" href="dxrt__api_8h_source.html">dxrt/dxrt_api.h</a>&quot;</code></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:aed622c0fa566620899c9e7e26011bacd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aed622c0fa566620899c9e7e26011bacd">InferenceEngine</a> (const std::string &amp;modelPath, <a class="el" href="classdxrt_1_1InferenceOption.html">InferenceOption</a> &amp;option=<a class="el" href="namespacedxrt.html#aab5060680ba2f567f47f8868cd8b1f00">DefaultInferenceOption</a>)</td></tr>
<tr class="memdesc:aed622c0fa566620899c9e7e26011bacd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Perform the task of loading the model and configuring the NPU to run.  <a href="classdxrt_1_1InferenceEngine.html#aed622c0fa566620899c9e7e26011bacd">More...</a><br /></td></tr>
<tr class="separator:aed622c0fa566620899c9e7e26011bacd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a88cea5324cbeb436034adb60cf31050f"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a88cea5324cbeb436034adb60cf31050f">Run</a> (void *inputPtr, void *userArg=nullptr, void *outputPtr=nullptr)</td></tr>
<tr class="memdesc:a88cea5324cbeb436034adb60cf31050f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Run inference engine using specific input pointer Synchronously.  <a href="classdxrt_1_1InferenceEngine.html#a88cea5324cbeb436034adb60cf31050f">More...</a><br /></td></tr>
<tr class="separator:a88cea5324cbeb436034adb60cf31050f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab54d666cc65cef4d6ae2fe4c300d7ba1"><td class="memItemLeft" align="right" valign="top">std::vector&lt; TensorPtrs &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ab54d666cc65cef4d6ae2fe4c300d7ba1">Run</a> (const std::vector&lt; void * &gt; &amp;inputBuffers, const std::vector&lt; void * &gt; &amp;outputBuffers, const std::vector&lt; void * &gt; &amp;userArgs={})</td></tr>
<tr class="memdesc:ab54d666cc65cef4d6ae2fe4c300d7ba1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Runs the inference engine using a specific input pointer vector.  <a href="classdxrt_1_1InferenceEngine.html#ab54d666cc65cef4d6ae2fe4c300d7ba1">More...</a><br /></td></tr>
<tr class="separator:ab54d666cc65cef4d6ae2fe4c300d7ba1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af3006c9a7183f7e42d1528aa8a6fea24"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#af3006c9a7183f7e42d1528aa8a6fea24">RunAsync</a> (void *inputPtr, void *userArg=nullptr, void *outputPtr=nullptr)</td></tr>
<tr class="memdesc:af3006c9a7183f7e42d1528aa8a6fea24"><td class="mdescLeft">&#160;</td><td class="mdescRight">Non-blocking call to request asynchronous inference by input pointer, and get job ID from inference engine.  <a href="classdxrt_1_1InferenceEngine.html#af3006c9a7183f7e42d1528aa8a6fea24">More...</a><br /></td></tr>
<tr class="separator:af3006c9a7183f7e42d1528aa8a6fea24"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac9c8166a8c588860ea4a5fc29d743eb5"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ac9c8166a8c588860ea4a5fc29d743eb5">RunAsync</a> (const std::vector&lt; void * &gt; &amp;inputPtrs, void *userArg=nullptr, void *outputPtr=nullptr)</td></tr>
<tr class="memdesc:ac9c8166a8c588860ea4a5fc29d743eb5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Non-blocking call to request asynchronous inference with automatic multi-input detection. This function automatically detects whether the input should be interpreted as multi-input single inference or batch inference based on the model requirements and input count.  <a href="classdxrt_1_1InferenceEngine.html#ac9c8166a8c588860ea4a5fc29d743eb5">More...</a><br /></td></tr>
<tr class="separator:ac9c8166a8c588860ea4a5fc29d743eb5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6a5b11e470a9dfeca6c60f644726ea59"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a6a5b11e470a9dfeca6c60f644726ea59">RunAsyncMultiInput</a> (const std::map&lt; std::string, void * &gt; &amp;inputTensors, void *userArg=nullptr, void *outputPtr=nullptr)</td></tr>
<tr class="memdesc:a6a5b11e470a9dfeca6c60f644726ea59"><td class="mdescLeft">&#160;</td><td class="mdescRight">Run async inference with multiple input tensors for multi-input models.  <a href="classdxrt_1_1InferenceEngine.html#a6a5b11e470a9dfeca6c60f644726ea59">More...</a><br /></td></tr>
<tr class="separator:a6a5b11e470a9dfeca6c60f644726ea59"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8a61dde8d97418d77e5bfd348011517f"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a8a61dde8d97418d77e5bfd348011517f">RunAsyncMultiInput</a> (const std::vector&lt; void * &gt; &amp;inputPtrs, void *userArg=nullptr, void *outputPtr=nullptr)</td></tr>
<tr class="memdesc:a8a61dde8d97418d77e5bfd348011517f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Run async inference with multiple input tensors (vector format) for multi-input models.  <a href="classdxrt_1_1InferenceEngine.html#a8a61dde8d97418d77e5bfd348011517f">More...</a><br /></td></tr>
<tr class="separator:a8a61dde8d97418d77e5bfd348011517f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab0c6629f048341d62bfa3a400c90e827"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ab0c6629f048341d62bfa3a400c90e827">RunBenchMark</a> (int num, void *inputPtr=nullptr)</td></tr>
<tr class="memdesc:ab0c6629f048341d62bfa3a400c90e827"><td class="mdescLeft">&#160;</td><td class="mdescRight">run benchmark with loop n times (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#ab0c6629f048341d62bfa3a400c90e827">More...</a><br /></td></tr>
<tr class="separator:ab0c6629f048341d62bfa3a400c90e827"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3846a2f47ea39933136e589aad036eb0"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a3846a2f47ea39933136e589aad036eb0">RunBenchmark</a> (int num, void *inputPtr=nullptr)</td></tr>
<tr class="memdesc:a3846a2f47ea39933136e589aad036eb0"><td class="mdescLeft">&#160;</td><td class="mdescRight">run benchmark with loop n times  <a href="classdxrt_1_1InferenceEngine.html#a3846a2f47ea39933136e589aad036eb0">More...</a><br /></td></tr>
<tr class="separator:a3846a2f47ea39933136e589aad036eb0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aef4c9502a5d5a9d256d3899391376aba"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aef4c9502a5d5a9d256d3899391376aba">ValidateDevice</a> (void *inputPtr, int deviceId=0)</td></tr>
<tr class="memdesc:aef4c9502a5d5a9d256d3899391376aba"><td class="mdescLeft">&#160;</td><td class="mdescRight">Validate inference of a specific NPU device connected to the host. This function runs a validation process using the provided input data on the specified NPU device. It can be used to ensure that the NPU device is operational and can process inference tasks correctly.  <a href="classdxrt_1_1InferenceEngine.html#aef4c9502a5d5a9d256d3899391376aba">More...</a><br /></td></tr>
<tr class="separator:aef4c9502a5d5a9d256d3899391376aba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2cbf5e6220f5be2cd9132f2570129faf"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a2cbf5e6220f5be2cd9132f2570129faf">ValidateDevice</a> (const std::vector&lt; void * &gt; &amp;inputPtrs, int deviceId=0)</td></tr>
<tr class="memdesc:a2cbf5e6220f5be2cd9132f2570129faf"><td class="mdescLeft">&#160;</td><td class="mdescRight">Validate inference of a specific NPU device with automatic multi-input detection. This function automatically detects whether the input should be interpreted as multi-input based on the model requirements and input count.  <a href="classdxrt_1_1InferenceEngine.html#a2cbf5e6220f5be2cd9132f2570129faf">More...</a><br /></td></tr>
<tr class="separator:a2cbf5e6220f5be2cd9132f2570129faf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5b6d78e715a91712548b905d2c730a0c"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a5b6d78e715a91712548b905d2c730a0c">ValidateDeviceMultiInput</a> (const std::map&lt; std::string, void * &gt; &amp;inputTensors, int deviceId=0)</td></tr>
<tr class="memdesc:a5b6d78e715a91712548b905d2c730a0c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Validate NPU device with multiple input tensors for multi-input models.  <a href="classdxrt_1_1InferenceEngine.html#a5b6d78e715a91712548b905d2c730a0c">More...</a><br /></td></tr>
<tr class="separator:a5b6d78e715a91712548b905d2c730a0c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af252439b3ba07ea017792c4fe6d82c7a"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#af252439b3ba07ea017792c4fe6d82c7a">ValidateDeviceMultiInput</a> (const std::vector&lt; void * &gt; &amp;inputPtrs, int deviceId=0)</td></tr>
<tr class="memdesc:af252439b3ba07ea017792c4fe6d82c7a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Validate NPU device with multiple input tensors (vector format) for multi-input models.  <a href="classdxrt_1_1InferenceEngine.html#af252439b3ba07ea017792c4fe6d82c7a">More...</a><br /></td></tr>
<tr class="separator:af252439b3ba07ea017792c4fe6d82c7a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3d3fc75faa79470d9a3bdbdc61561a3a"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a3d3fc75faa79470d9a3bdbdc61561a3a">RegisterCallBack</a> (std::function&lt; int(TensorPtrs &amp;<a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a>, void *userArg)&gt; callbackFunc)</td></tr>
<tr class="memdesc:a3d3fc75faa79470d9a3bdbdc61561a3a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Register user callback function to be called by inference completion. (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a3d3fc75faa79470d9a3bdbdc61561a3a">More...</a><br /></td></tr>
<tr class="separator:a3d3fc75faa79470d9a3bdbdc61561a3a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1d8b33fa324a48b43433c401bd54280a"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a1d8b33fa324a48b43433c401bd54280a">RegisterCallback</a> (std::function&lt; int(TensorPtrs &amp;<a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a>, void *userArg)&gt; callbackFunc)</td></tr>
<tr class="memdesc:a1d8b33fa324a48b43433c401bd54280a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Register user callback function to be called by inference completion.  <a href="classdxrt_1_1InferenceEngine.html#a1d8b33fa324a48b43433c401bd54280a">More...</a><br /></td></tr>
<tr class="separator:a1d8b33fa324a48b43433c401bd54280a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2c6f130d722950153287d4cf1efd5a5d"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a2c6f130d722950153287d4cf1efd5a5d">Wait</a> (int jobId)</td></tr>
<tr class="memdesc:a2c6f130d722950153287d4cf1efd5a5d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Wait until an request is complete and returns output.  <a href="classdxrt_1_1InferenceEngine.html#a2c6f130d722950153287d4cf1efd5a5d">More...</a><br /></td></tr>
<tr class="separator:a2c6f130d722950153287d4cf1efd5a5d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afbee8b031fdd6c0a3916ed00d08c9cd8"><td class="memItemLeft" align="right" valign="top">Tensors&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#afbee8b031fdd6c0a3916ed00d08c9cd8">inputs</a> (void *ptr=nullptr, uint64_t phyAddr=0)</td></tr>
<tr class="memdesc:afbee8b031fdd6c0a3916ed00d08c9cd8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get input tensor (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#afbee8b031fdd6c0a3916ed00d08c9cd8">More...</a><br /></td></tr>
<tr class="separator:afbee8b031fdd6c0a3916ed00d08c9cd8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abe536ab666125335e08764fa4916c157"><td class="memItemLeft" align="right" valign="top">Tensors&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#abe536ab666125335e08764fa4916c157">GetInputs</a> (void *ptr=nullptr, uint64_t phyAddr=0)</td></tr>
<tr class="memdesc:abe536ab666125335e08764fa4916c157"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get input tensor.  <a href="classdxrt_1_1InferenceEngine.html#abe536ab666125335e08764fa4916c157">More...</a><br /></td></tr>
<tr class="separator:abe536ab666125335e08764fa4916c157"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af81b48e8ccd95953dd9bc483e5ec678b"><td class="memItemLeft" align="right" valign="top">std::vector&lt; Tensors &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#af81b48e8ccd95953dd9bc483e5ec678b">inputs</a> (int devId)</td></tr>
<tr class="memdesc:af81b48e8ccd95953dd9bc483e5ec678b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get input tensor (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#af81b48e8ccd95953dd9bc483e5ec678b">More...</a><br /></td></tr>
<tr class="separator:af81b48e8ccd95953dd9bc483e5ec678b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adb0043cbbf3dfa50eba37d03b9e7d48e"><td class="memItemLeft" align="right" valign="top">std::vector&lt; Tensors &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#adb0043cbbf3dfa50eba37d03b9e7d48e">GetInputs</a> (int devId)</td></tr>
<tr class="memdesc:adb0043cbbf3dfa50eba37d03b9e7d48e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get input tensor.  <a href="classdxrt_1_1InferenceEngine.html#adb0043cbbf3dfa50eba37d03b9e7d48e">More...</a><br /></td></tr>
<tr class="separator:adb0043cbbf3dfa50eba37d03b9e7d48e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0893b724d6eed939556f1315ce05829d"><td class="memItemLeft" align="right" valign="top">Tensors&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> (void *ptr=nullptr, uint64_t phyAddr=0)</td></tr>
<tr class="memdesc:a0893b724d6eed939556f1315ce05829d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get output tensor (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">More...</a><br /></td></tr>
<tr class="separator:a0893b724d6eed939556f1315ce05829d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a411b78e6c9560119f9a39c30ee1021af"><td class="memItemLeft" align="right" valign="top">Tensors&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a411b78e6c9560119f9a39c30ee1021af">GetOutputs</a> (void *ptr=nullptr, uint64_t phyAddr=0)</td></tr>
<tr class="memdesc:a411b78e6c9560119f9a39c30ee1021af"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get output tensor.  <a href="classdxrt_1_1InferenceEngine.html#a411b78e6c9560119f9a39c30ee1021af">More...</a><br /></td></tr>
<tr class="separator:a411b78e6c9560119f9a39c30ee1021af"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a96c4e5953a0daf4a1c886002c052a6ba"><td class="memItemLeft" align="right" valign="top">uint64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a96c4e5953a0daf4a1c886002c052a6ba">input_size</a> ()</td></tr>
<tr class="memdesc:a96c4e5953a0daf4a1c886002c052a6ba"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get total size of input tensors (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a96c4e5953a0daf4a1c886002c052a6ba">More...</a><br /></td></tr>
<tr class="separator:a96c4e5953a0daf4a1c886002c052a6ba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad2c11689c4839b5dbb3571107a7b3a7c"><td class="memItemLeft" align="right" valign="top">uint64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ad2c11689c4839b5dbb3571107a7b3a7c">GetInputSize</a> ()</td></tr>
<tr class="memdesc:ad2c11689c4839b5dbb3571107a7b3a7c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get total size of input tensors.  <a href="classdxrt_1_1InferenceEngine.html#ad2c11689c4839b5dbb3571107a7b3a7c">More...</a><br /></td></tr>
<tr class="separator:ad2c11689c4839b5dbb3571107a7b3a7c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a62b2716dbe26bbca8dac24d4954f8c17"><td class="memItemLeft" align="right" valign="top">std::vector&lt; uint64_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a62b2716dbe26bbca8dac24d4954f8c17">GetInputTensorSizes</a> ()</td></tr>
<tr class="memdesc:a62b2716dbe26bbca8dac24d4954f8c17"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get individual input tensor sizes for multi-input models.  <a href="classdxrt_1_1InferenceEngine.html#a62b2716dbe26bbca8dac24d4954f8c17">More...</a><br /></td></tr>
<tr class="separator:a62b2716dbe26bbca8dac24d4954f8c17"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aee8bc8dd933efeed07de606fcf2fb161"><td class="memItemLeft" align="right" valign="top">std::vector&lt; uint64_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aee8bc8dd933efeed07de606fcf2fb161">GetOutputTensorSizes</a> ()</td></tr>
<tr class="memdesc:aee8bc8dd933efeed07de606fcf2fb161"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get individual output tensor sizes for multi-output models.  <a href="classdxrt_1_1InferenceEngine.html#aee8bc8dd933efeed07de606fcf2fb161">More...</a><br /></td></tr>
<tr class="separator:aee8bc8dd933efeed07de606fcf2fb161"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a93e1d7b460b7f8656e725ed708de507b"><td class="memItemLeft" align="right" valign="top">uint64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a93e1d7b460b7f8656e725ed708de507b">output_size</a> ()</td></tr>
<tr class="memdesc:a93e1d7b460b7f8656e725ed708de507b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get total size of output tensors (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a93e1d7b460b7f8656e725ed708de507b">More...</a><br /></td></tr>
<tr class="separator:a93e1d7b460b7f8656e725ed708de507b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a82af5685f6b43643c3ddf47bf0a4ade8"><td class="memItemLeft" align="right" valign="top">uint64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a82af5685f6b43643c3ddf47bf0a4ade8">GetOutputSize</a> ()</td></tr>
<tr class="memdesc:a82af5685f6b43643c3ddf47bf0a4ade8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get total size of output tensors.  <a href="classdxrt_1_1InferenceEngine.html#a82af5685f6b43643c3ddf47bf0a4ade8">More...</a><br /></td></tr>
<tr class="separator:a82af5685f6b43643c3ddf47bf0a4ade8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3fb16094508bd367cc4971238f002765"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a3fb16094508bd367cc4971238f002765">name</a> ()</td></tr>
<tr class="memdesc:a3fb16094508bd367cc4971238f002765"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get model name (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a3fb16094508bd367cc4971238f002765">More...</a><br /></td></tr>
<tr class="separator:a3fb16094508bd367cc4971238f002765"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7ed90b0eaa55ce7c097cc7163009c66e"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a7ed90b0eaa55ce7c097cc7163009c66e">GetModelName</a> ()</td></tr>
<tr class="memdesc:a7ed90b0eaa55ce7c097cc7163009c66e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get model name.  <a href="classdxrt_1_1InferenceEngine.html#a7ed90b0eaa55ce7c097cc7163009c66e">More...</a><br /></td></tr>
<tr class="separator:a7ed90b0eaa55ce7c097cc7163009c66e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a578a0098f9b49902fa4c81d95945baa4"><td class="memItemLeft" align="right" valign="top">std::vector&lt; std::string &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a578a0098f9b49902fa4c81d95945baa4">task_order</a> ()</td></tr>
<tr class="memdesc:a578a0098f9b49902fa4c81d95945baa4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get model task order (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a578a0098f9b49902fa4c81d95945baa4">More...</a><br /></td></tr>
<tr class="separator:a578a0098f9b49902fa4c81d95945baa4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8b759a7bb098d710e1c3052170057143"><td class="memItemLeft" align="right" valign="top">std::vector&lt; std::string &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a8b759a7bb098d710e1c3052170057143">GetTaskOrder</a> ()</td></tr>
<tr class="memdesc:a8b759a7bb098d710e1c3052170057143"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get model task order.  <a href="classdxrt_1_1InferenceEngine.html#a8b759a7bb098d710e1c3052170057143">More...</a><br /></td></tr>
<tr class="separator:a8b759a7bb098d710e1c3052170057143"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a990ca720e7f8a28ac5394206eb6f9491"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a990ca720e7f8a28ac5394206eb6f9491">latency</a> ()</td></tr>
<tr class="memdesc:a990ca720e7f8a28ac5394206eb6f9491"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latest latency (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a990ca720e7f8a28ac5394206eb6f9491">More...</a><br /></td></tr>
<tr class="separator:a990ca720e7f8a28ac5394206eb6f9491"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3758aeb28d7ed75216770a5fd5dd9619"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a3758aeb28d7ed75216770a5fd5dd9619">GetLatency</a> ()</td></tr>
<tr class="memdesc:a3758aeb28d7ed75216770a5fd5dd9619"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latest latency.  <a href="classdxrt_1_1InferenceEngine.html#a3758aeb28d7ed75216770a5fd5dd9619">More...</a><br /></td></tr>
<tr class="separator:a3758aeb28d7ed75216770a5fd5dd9619"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad2f3b616e0e996b66ef7df37dc16a85"><td class="memItemLeft" align="right" valign="top">uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aad2f3b616e0e996b66ef7df37dc16a85">inference_time</a> ()</td></tr>
<tr class="memdesc:aad2f3b616e0e996b66ef7df37dc16a85"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latest inference time (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#aad2f3b616e0e996b66ef7df37dc16a85">More...</a><br /></td></tr>
<tr class="separator:aad2f3b616e0e996b66ef7df37dc16a85"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1ee64321b5d6c50cb95be282080f326d"><td class="memItemLeft" align="right" valign="top">uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a1ee64321b5d6c50cb95be282080f326d">GetNpuInferenceTime</a> ()</td></tr>
<tr class="memdesc:a1ee64321b5d6c50cb95be282080f326d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latest inference time.  <a href="classdxrt_1_1InferenceEngine.html#a1ee64321b5d6c50cb95be282080f326d">More...</a><br /></td></tr>
<tr class="separator:a1ee64321b5d6c50cb95be282080f326d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a331862cfde13a009b3e4133678a1c597"><td class="memItemLeft" align="right" valign="top">std::vector&lt; int &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a331862cfde13a009b3e4133678a1c597">GetLatencyVector</a> ()</td></tr>
<tr class="memdesc:a331862cfde13a009b3e4133678a1c597"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get recent Latency.  <a href="classdxrt_1_1InferenceEngine.html#a331862cfde13a009b3e4133678a1c597">More...</a><br /></td></tr>
<tr class="separator:a331862cfde13a009b3e4133678a1c597"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad79d35e5a285878517eb784439737a9"><td class="memItemLeft" align="right" valign="top">std::vector&lt; uint32_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aad79d35e5a285878517eb784439737a9">GetNpuInferenceTimeVector</a> ()</td></tr>
<tr class="memdesc:aad79d35e5a285878517eb784439737a9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get recent inference time.  <a href="classdxrt_1_1InferenceEngine.html#aad79d35e5a285878517eb784439737a9">More...</a><br /></td></tr>
<tr class="separator:aad79d35e5a285878517eb784439737a9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1e81cce5b5db047c17ba40b69bab9a60"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a1e81cce5b5db047c17ba40b69bab9a60">GetLatencyMean</a> ()</td></tr>
<tr class="memdesc:a1e81cce5b5db047c17ba40b69bab9a60"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latency Mean.  <a href="classdxrt_1_1InferenceEngine.html#a1e81cce5b5db047c17ba40b69bab9a60">More...</a><br /></td></tr>
<tr class="separator:a1e81cce5b5db047c17ba40b69bab9a60"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0d46e7a44cd5975c5a9c3a76be684dc3"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a0d46e7a44cd5975c5a9c3a76be684dc3">GetNpuInferenceTimeMean</a> ()</td></tr>
<tr class="memdesc:a0d46e7a44cd5975c5a9c3a76be684dc3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get inference time Mean.  <a href="classdxrt_1_1InferenceEngine.html#a0d46e7a44cd5975c5a9c3a76be684dc3">More...</a><br /></td></tr>
<tr class="separator:a0d46e7a44cd5975c5a9c3a76be684dc3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae8bec4ff8e4e0fb6fa680c04f84b642d"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ae8bec4ff8e4e0fb6fa680c04f84b642d">GetLatencyStdDev</a> ()</td></tr>
<tr class="memdesc:ae8bec4ff8e4e0fb6fa680c04f84b642d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latency Standard Deviation.  <a href="classdxrt_1_1InferenceEngine.html#ae8bec4ff8e4e0fb6fa680c04f84b642d">More...</a><br /></td></tr>
<tr class="separator:ae8bec4ff8e4e0fb6fa680c04f84b642d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4d552cb23986c189b1002a3e0b9915a9"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a4d552cb23986c189b1002a3e0b9915a9">GetNpuInferenceTimeStdDev</a> ()</td></tr>
<tr class="memdesc:a4d552cb23986c189b1002a3e0b9915a9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get inference time Standard Deviation.  <a href="classdxrt_1_1InferenceEngine.html#a4d552cb23986c189b1002a3e0b9915a9">More...</a><br /></td></tr>
<tr class="separator:a4d552cb23986c189b1002a3e0b9915a9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1259543ba3f8fc52efb503e40570f95e"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a1259543ba3f8fc52efb503e40570f95e">GetLatencyCnt</a> ()</td></tr>
<tr class="memdesc:a1259543ba3f8fc52efb503e40570f95e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get latency Count.  <a href="classdxrt_1_1InferenceEngine.html#a1259543ba3f8fc52efb503e40570f95e">More...</a><br /></td></tr>
<tr class="separator:a1259543ba3f8fc52efb503e40570f95e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad7a32a700fa89a0213b815d57f8fc1c2"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ad7a32a700fa89a0213b815d57f8fc1c2">GetNpuInferenceTimeCnt</a> ()</td></tr>
<tr class="memdesc:ad7a32a700fa89a0213b815d57f8fc1c2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get inference time Count.  <a href="classdxrt_1_1InferenceEngine.html#ad7a32a700fa89a0213b815d57f8fc1c2">More...</a><br /></td></tr>
<tr class="separator:ad7a32a700fa89a0213b815d57f8fc1c2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a34dfe51c8f5289bd54ff96be8d64bc12"><td class="memItemLeft" align="right" valign="top">std::vector&lt; TensorPtrs &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a34dfe51c8f5289bd54ff96be8d64bc12">get_outputs</a> ()</td></tr>
<tr class="memdesc:a34dfe51c8f5289bd54ff96be8d64bc12"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get output tensors of all tasks (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a34dfe51c8f5289bd54ff96be8d64bc12">More...</a><br /></td></tr>
<tr class="separator:a34dfe51c8f5289bd54ff96be8d64bc12"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae936aff8999d075130a531f3de60125a"><td class="memItemLeft" align="right" valign="top">std::vector&lt; TensorPtrs &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ae936aff8999d075130a531f3de60125a">GetAllTaskOutputs</a> ()</td></tr>
<tr class="memdesc:ae936aff8999d075130a531f3de60125a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get output tensors of all tasks.  <a href="classdxrt_1_1InferenceEngine.html#ae936aff8999d075130a531f3de60125a">More...</a><br /></td></tr>
<tr class="separator:ae936aff8999d075130a531f3de60125a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa8ab6719ae0efdf0f5c678e3401ab57d"><td class="memItemLeft" align="right" valign="top">std::vector&lt; uint8_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aa8ab6719ae0efdf0f5c678e3401ab57d">bitmatch_mask</a> (int index)</td></tr>
<tr class="separator:aa8ab6719ae0efdf0f5c678e3401ab57d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0b38cd5001ec327cbdc5b00a7143d0e2"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a0b38cd5001ec327cbdc5b00a7143d0e2">get_num_tails</a> ()</td></tr>
<tr class="memdesc:a0b38cd5001ec327cbdc5b00a7143d0e2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the number of tail tasks in the model. (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a0b38cd5001ec327cbdc5b00a7143d0e2">More...</a><br /></td></tr>
<tr class="separator:a0b38cd5001ec327cbdc5b00a7143d0e2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a110445fe04396f2d2ea309c18c3bb930"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a110445fe04396f2d2ea309c18c3bb930">GetNumTailTasks</a> ()</td></tr>
<tr class="memdesc:a110445fe04396f2d2ea309c18c3bb930"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the number of tail tasks in the model.  <a href="classdxrt_1_1InferenceEngine.html#a110445fe04396f2d2ea309c18c3bb930">More...</a><br /></td></tr>
<tr class="separator:a110445fe04396f2d2ea309c18c3bb930"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a265ff0d27c6bac98c0c641c0e5f8f6ba"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a265ff0d27c6bac98c0c641c0e5f8f6ba">get_compile_type</a> ()</td></tr>
<tr class="memdesc:a265ff0d27c6bac98c0c641c0e5f8f6ba"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the compile type of the model. (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a265ff0d27c6bac98c0c641c0e5f8f6ba">More...</a><br /></td></tr>
<tr class="separator:a265ff0d27c6bac98c0c641c0e5f8f6ba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9308cfc747e8d56119febfd2b48b876c"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a9308cfc747e8d56119febfd2b48b876c">GetCompileType</a> ()</td></tr>
<tr class="memdesc:a9308cfc747e8d56119febfd2b48b876c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the compile type of the model.  <a href="classdxrt_1_1InferenceEngine.html#a9308cfc747e8d56119febfd2b48b876c">More...</a><br /></td></tr>
<tr class="separator:a9308cfc747e8d56119febfd2b48b876c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1b3771241a7424353e97a0c81a28d875"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a1b3771241a7424353e97a0c81a28d875">GetModelVersion</a> ()</td></tr>
<tr class="memdesc:a1b3771241a7424353e97a0c81a28d875"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the DXNN file format version of the model.  <a href="classdxrt_1_1InferenceEngine.html#a1b3771241a7424353e97a0c81a28d875">More...</a><br /></td></tr>
<tr class="separator:a1b3771241a7424353e97a0c81a28d875"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9aeb596b415843e7e318c47938444259"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a9aeb596b415843e7e318c47938444259">is_PPU</a> ()</td></tr>
<tr class="memdesc:a9aeb596b415843e7e318c47938444259"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns whether the model is using PPU. (Legacy API)  <a href="classdxrt_1_1InferenceEngine.html#a9aeb596b415843e7e318c47938444259">More...</a><br /></td></tr>
<tr class="separator:a9aeb596b415843e7e318c47938444259"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab6fd915af259467afe1681098b315dca"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ab6fd915af259467afe1681098b315dca">IsPPU</a> ()</td></tr>
<tr class="memdesc:ab6fd915af259467afe1681098b315dca"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns whether the model is using PPU.  <a href="classdxrt_1_1InferenceEngine.html#ab6fd915af259467afe1681098b315dca">More...</a><br /></td></tr>
<tr class="separator:ab6fd915af259467afe1681098b315dca"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a498ec2af6a8ebdd4e8edd26cba1faf24"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a498ec2af6a8ebdd4e8edd26cba1faf24">IsOrtConfigured</a> ()</td></tr>
<tr class="memdesc:a498ec2af6a8ebdd4e8edd26cba1faf24"><td class="mdescLeft">&#160;</td><td class="mdescRight">Checks whether ORT (ONNX Runtime) is configured.  <a href="classdxrt_1_1InferenceEngine.html#a498ec2af6a8ebdd4e8edd26cba1faf24">More...</a><br /></td></tr>
<tr class="separator:a498ec2af6a8ebdd4e8edd26cba1faf24"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a896b05b9798d0ae33b3ba25e82e9df4c"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a896b05b9798d0ae33b3ba25e82e9df4c">IsMultiInputModel</a> () const</td></tr>
<tr class="memdesc:a896b05b9798d0ae33b3ba25e82e9df4c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns whether the model has multiple input tensors.  <a href="classdxrt_1_1InferenceEngine.html#a896b05b9798d0ae33b3ba25e82e9df4c">More...</a><br /></td></tr>
<tr class="separator:a896b05b9798d0ae33b3ba25e82e9df4c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab7e4e88fea3dfcc9c276531f7674c20b"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#ab7e4e88fea3dfcc9c276531f7674c20b">GetInputTensorCount</a> () const</td></tr>
<tr class="memdesc:ab7e4e88fea3dfcc9c276531f7674c20b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the number of input tensors required by the model.  <a href="classdxrt_1_1InferenceEngine.html#ab7e4e88fea3dfcc9c276531f7674c20b">More...</a><br /></td></tr>
<tr class="separator:ab7e4e88fea3dfcc9c276531f7674c20b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a41f090eff09ddb3e3963666eb507d7dc"><td class="memItemLeft" align="right" valign="top">std::vector&lt; std::string &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a41f090eff09ddb3e3963666eb507d7dc">GetInputTensorNames</a> () const</td></tr>
<tr class="memdesc:a41f090eff09ddb3e3963666eb507d7dc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the names of all input tensors in the model.  <a href="classdxrt_1_1InferenceEngine.html#a41f090eff09ddb3e3963666eb507d7dc">More...</a><br /></td></tr>
<tr class="separator:a41f090eff09ddb3e3963666eb507d7dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1edebbe4f7b03f042a550e822ac16284"><td class="memItemLeft" align="right" valign="top">std::vector&lt; std::string &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a1edebbe4f7b03f042a550e822ac16284">GetOutputTensorNames</a> () const</td></tr>
<tr class="memdesc:a1edebbe4f7b03f042a550e822ac16284"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the names of all output tensors in the model.  <a href="classdxrt_1_1InferenceEngine.html#a1edebbe4f7b03f042a550e822ac16284">More...</a><br /></td></tr>
<tr class="separator:a1edebbe4f7b03f042a550e822ac16284"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a937480e5d622b1ddcfdd430ed2ba0476"><td class="memItemLeft" align="right" valign="top">std::map&lt; std::string, std::string &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a937480e5d622b1ddcfdd430ed2ba0476">GetInputTensorToTaskMapping</a> () const</td></tr>
<tr class="memdesc:a937480e5d622b1ddcfdd430ed2ba0476"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the mapping from input tensor names to their target tasks.  <a href="classdxrt_1_1InferenceEngine.html#a937480e5d622b1ddcfdd430ed2ba0476">More...</a><br /></td></tr>
<tr class="separator:a937480e5d622b1ddcfdd430ed2ba0476"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af3feefee6373ecfe7fe5b9b84501a723"><td class="memItemLeft" align="right" valign="top"><a id="af3feefee6373ecfe7fe5b9b84501a723"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#af3feefee6373ecfe7fe5b9b84501a723">Dispose</a> ()</td></tr>
<tr class="memdesc:af3feefee6373ecfe7fe5b9b84501a723"><td class="mdescLeft">&#160;</td><td class="mdescRight">Resource deallocation and cleanup. <br /></td></tr>
<tr class="separator:af3feefee6373ecfe7fe5b9b84501a723"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aff7eaeecfb9fbf7aa1ea8e27d0572b92"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aff7eaeecfb9fbf7aa1ea8e27d0572b92">RunMultiInput</a> (const std::map&lt; std::string, void * &gt; &amp;inputTensors, void *userArg=nullptr, void *outputPtr=nullptr)</td></tr>
<tr class="memdesc:aff7eaeecfb9fbf7aa1ea8e27d0572b92"><td class="mdescLeft">&#160;</td><td class="mdescRight">Run inference engine with multiple input tensors for multi-input models.  <a href="classdxrt_1_1InferenceEngine.html#aff7eaeecfb9fbf7aa1ea8e27d0572b92">More...</a><br /></td></tr>
<tr class="separator:aff7eaeecfb9fbf7aa1ea8e27d0572b92"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4b5c6a8aa5db938b8b50b7d9c844b367"><td class="memItemLeft" align="right" valign="top">TensorPtrs&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a4b5c6a8aa5db938b8b50b7d9c844b367">RunMultiInput</a> (const std::vector&lt; void * &gt; &amp;inputPtrs, void *userArg=nullptr, void *outputPtr=nullptr)</td></tr>
<tr class="memdesc:a4b5c6a8aa5db938b8b50b7d9c844b367"><td class="mdescLeft">&#160;</td><td class="mdescRight">Run inference engine with multiple input tensors (vector format) for multi-input models.  <a href="classdxrt_1_1InferenceEngine.html#a4b5c6a8aa5db938b8b50b7d9c844b367">More...</a><br /></td></tr>
<tr class="separator:a4b5c6a8aa5db938b8b50b7d9c844b367"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa227ad7501e794726b9af17e88418cf1"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aa227ad7501e794726b9af17e88418cf1">supportsTensorCentricOffsets</a> () const</td></tr>
<tr class="memdesc:aa227ad7501e794726b9af17e88418cf1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Check if tensor-centric offset calculation is supported.  <a href="classdxrt_1_1InferenceEngine.html#aa227ad7501e794726b9af17e88418cf1">More...</a><br /></td></tr>
<tr class="separator:aa227ad7501e794726b9af17e88418cf1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2338c50d4d24b081179372bfea9d5f2c"><td class="memItemLeft" align="right" valign="top">uint64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#a2338c50d4d24b081179372bfea9d5f2c">getTensorOffset</a> (const std::string &amp;tensorName) const</td></tr>
<tr class="memdesc:a2338c50d4d24b081179372bfea9d5f2c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get the offset of a tensor in the final output buffer.  <a href="classdxrt_1_1InferenceEngine.html#a2338c50d4d24b081179372bfea9d5f2c">More...</a><br /></td></tr>
<tr class="separator:a2338c50d4d24b081179372bfea9d5f2c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa861889fd3d562742950053868077086"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classdxrt_1_1InferenceEngine.html#aa861889fd3d562742950053868077086">GetOutputTensorOffset</a> (const std::string &amp;tensorName) const</td></tr>
<tr class="memdesc:aa861889fd3d562742950053868077086"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the offset for a specific output tensor in the final output buffer.  <a href="classdxrt_1_1InferenceEngine.html#aa861889fd3d562742950053868077086">More...</a><br /></td></tr>
<tr class="separator:aa861889fd3d562742950053868077086"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>This class abstracts runtime inference executor for user's compiled model. </p>
<p>After user loads compiled model to <a class="el" href="classdxrt_1_1InferenceEngine.html" title="This class abstracts runtime inference executor for user&#39;s compiled model.">InferenceEngine</a>, real-time device tasks will be scheduled by internal runtime libraries. It supports both inference mode (synchronous/asynchronous) according to user's request. </p><div class="fragment"><div class="line"><span class="comment">// Use default inference option</span></div>
<div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><span class="keyword">auto</span> ie = <a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a>(modelPath, <span class="keyword">nullptr</span>);</div>
</div><!-- fragment --> <div class="fragment"><div class="line"><span class="comment">// Use a new inference option</span></div>
<div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a> option;</div>
<div class="line">option.<a class="code" href="classdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">devices</a> = {0,1,3};  <span class="comment">//use only 0,1,3 device</span></div>
<div class="line"><span class="keyword">auto</span> ie = <a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a>(modelPath, option);</div>
</div><!-- fragment --> </div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="aed622c0fa566620899c9e7e26011bacd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aed622c0fa566620899c9e7e26011bacd">&#9670;&nbsp;</a></span>InferenceEngine()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">dxrt::InferenceEngine::InferenceEngine </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>modelPath</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classdxrt_1_1InferenceOption.html">InferenceOption</a> &amp;&#160;</td>
          <td class="paramname"><em>option</em> = <code><a class="el" href="namespacedxrt.html#aab5060680ba2f567f47f8868cd8b1f00">DefaultInferenceOption</a></code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">explicit</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Perform the task of loading the model and configuring the NPU to run. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">modelPath</td><td>model path </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">option</td><td>device and npu core options <div class="fragment"><div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a>(modelPath) ie(modelPath);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
<div class="line"> </div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a> op;</div>
<div class="line">op.<a class="code" href="classdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">devices</a>.push_back(0); </div>
<div class="line">op.<a class="code" href="classdxrt_1_1InferenceOption.html#a9fd270a5eeeb1a14ce5f133c16c30d55">boundOption</a> = dxrt::InferenceOption::BOUND_OPTION::NPU_0; <span class="comment">// NPU_0 only</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a> ie(modelPath, op);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
</div><!-- fragment --> </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="aa8ab6719ae0efdf0f5c678e3401ab57d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa8ab6719ae0efdf0f5c678e3401ab57d">&#9670;&nbsp;</a></span>bitmatch_mask()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;uint8_t&gt; dxrt::InferenceEngine::bitmatch_mask </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>index</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000013">Deprecated:</a></b></dt><dd>Use GetBitmatchMask() instead. </dd></dl>

</div>
</div>
<a id="a265ff0d27c6bac98c0c641c0e5f8f6ba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a265ff0d27c6bac98c0c641c0e5f8f6ba">&#9670;&nbsp;</a></span>get_compile_type()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::string dxrt::InferenceEngine::get_compile_type </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the compile type of the model. (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000015">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a9308cfc747e8d56119febfd2b48b876c" title="Returns the compile type of the model.">GetCompileType()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>The compile type of the model. </dd></dl>

</div>
</div>
<a id="a0b38cd5001ec327cbdc5b00a7143d0e2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0b38cd5001ec327cbdc5b00a7143d0e2">&#9670;&nbsp;</a></span>get_num_tails()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::get_num_tails </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the number of tail tasks in the model. (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000014">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a110445fe04396f2d2ea309c18c3bb930" title="Returns the number of tail tasks in the model.">GetNumTailTasks()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>The number of tasks that have no subsequent tasks.</dd></dl>
<p>Tail tasks are those which do not have any tasks following them in the model's task chain. This function provides the count of such tail tasks. </p>

</div>
</div>
<a id="a34dfe51c8f5289bd54ff96be8d64bc12"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a34dfe51c8f5289bd54ff96be8d64bc12">&#9670;&nbsp;</a></span>get_outputs()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;TensorPtrs&gt; dxrt::InferenceEngine::get_outputs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get output tensors of all tasks (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000012">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#ae936aff8999d075130a531f3de60125a" title="Get output tensors of all tasks.">GetAllTaskOutputs()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>the output of all tasks as a vector of smart pointer instance vectors. <br  />
 </dd></dl>

</div>
</div>
<a id="ae936aff8999d075130a531f3de60125a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae936aff8999d075130a531f3de60125a">&#9670;&nbsp;</a></span>GetAllTaskOutputs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;TensorPtrs&gt; dxrt::InferenceEngine::GetAllTaskOutputs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get output tensors of all tasks. </p>
<dl class="section return"><dt>Returns</dt><dd>the output of all Tasks as a vector of smart pointer instance vectors. <br  />
 </dd></dl>

</div>
</div>
<a id="a9308cfc747e8d56119febfd2b48b876c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9308cfc747e8d56119febfd2b48b876c">&#9670;&nbsp;</a></span>GetCompileType()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::string dxrt::InferenceEngine::GetCompileType </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the compile type of the model. </p>
<dl class="section return"><dt>Returns</dt><dd>The compile type of the model. </dd></dl>

</div>
</div>
<a id="adb0043cbbf3dfa50eba37d03b9e7d48e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adb0043cbbf3dfa50eba37d03b9e7d48e">&#9670;&nbsp;</a></span>GetInputs() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;Tensors&gt; dxrt::InferenceEngine::GetInputs </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>devId</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">devId</td><td>device id </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>vector of input tensors </dd></dl>

</div>
</div>
<a id="abe536ab666125335e08764fa4916c157"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abe536ab666125335e08764fa4916c157">&#9670;&nbsp;</a></span>GetInputs() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensors dxrt::InferenceEngine::GetInputs </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>ptr</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint64_t&#160;</td>
          <td class="paramname"><em>phyAddr</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">ptr</td><td>pointer to virtual address </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">phyAddr</td><td>pointer to physical address </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>if ptr is null, input memory area in engine is returned </dd>
<dd>
if ptr and phyAddr is given, inputs tensors that contains output addresses </dd></dl>

</div>
</div>
<a id="ad2c11689c4839b5dbb3571107a7b3a7c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad2c11689c4839b5dbb3571107a7b3a7c">&#9670;&nbsp;</a></span>GetInputSize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">uint64_t dxrt::InferenceEngine::GetInputSize </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get total size of input tensors. </p>
<dl class="section return"><dt>Returns</dt><dd>Input size of one inference in bytes </dd></dl>

</div>
</div>
<a id="ab7e4e88fea3dfcc9c276531f7674c20b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab7e4e88fea3dfcc9c276531f7674c20b">&#9670;&nbsp;</a></span>GetInputTensorCount()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::GetInputTensorCount </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the number of input tensors required by the model. </p>
<dl class="section return"><dt>Returns</dt><dd>The number of input tensors. </dd></dl>

</div>
</div>
<a id="a41f090eff09ddb3e3963666eb507d7dc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a41f090eff09ddb3e3963666eb507d7dc">&#9670;&nbsp;</a></span>GetInputTensorNames()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;std::string&gt; dxrt::InferenceEngine::GetInputTensorNames </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the names of all input tensors in the model. </p>
<dl class="section return"><dt>Returns</dt><dd>Vector of input tensor names in the order they should be provided. </dd></dl>

</div>
</div>
<a id="a62b2716dbe26bbca8dac24d4954f8c17"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a62b2716dbe26bbca8dac24d4954f8c17">&#9670;&nbsp;</a></span>GetInputTensorSizes()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;uint64_t&gt; dxrt::InferenceEngine::GetInputTensorSizes </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get individual input tensor sizes for multi-input models. </p>
<dl class="section return"><dt>Returns</dt><dd>Vector of input tensor sizes in bytes, in the order specified by <a class="el" href="classdxrt_1_1InferenceEngine.html#a41f090eff09ddb3e3963666eb507d7dc" title="Returns the names of all input tensors in the model.">GetInputTensorNames()</a> </dd></dl>

</div>
</div>
<a id="a937480e5d622b1ddcfdd430ed2ba0476"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a937480e5d622b1ddcfdd430ed2ba0476">&#9670;&nbsp;</a></span>GetInputTensorToTaskMapping()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::map&lt;std::string, std::string&gt; dxrt::InferenceEngine::GetInputTensorToTaskMapping </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the mapping from input tensor names to their target tasks. </p>
<dl class="section return"><dt>Returns</dt><dd>Map where key is tensor name and value is task name. </dd></dl>

</div>
</div>
<a id="a3758aeb28d7ed75216770a5fd5dd9619"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3758aeb28d7ed75216770a5fd5dd9619">&#9670;&nbsp;</a></span>GetLatency()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::GetLatency </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get latest latency. </p>
<dl class="section return"><dt>Returns</dt><dd>latency (microseconds) </dd></dl>

</div>
</div>
<a id="a1259543ba3f8fc52efb503e40570f95e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1259543ba3f8fc52efb503e40570f95e">&#9670;&nbsp;</a></span>GetLatencyCnt()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::GetLatencyCnt </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get latency Count. </p>
<dl class="section return"><dt>Returns</dt><dd>latency Count </dd></dl>

</div>
</div>
<a id="a1e81cce5b5db047c17ba40b69bab9a60"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1e81cce5b5db047c17ba40b69bab9a60">&#9670;&nbsp;</a></span>GetLatencyMean()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double dxrt::InferenceEngine::GetLatencyMean </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get latency Mean. </p>
<dl class="section return"><dt>Returns</dt><dd>latency Mean (microseconds) </dd></dl>

</div>
</div>
<a id="ae8bec4ff8e4e0fb6fa680c04f84b642d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae8bec4ff8e4e0fb6fa680c04f84b642d">&#9670;&nbsp;</a></span>GetLatencyStdDev()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double dxrt::InferenceEngine::GetLatencyStdDev </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get latency Standard Deviation. </p>
<dl class="section return"><dt>Returns</dt><dd>latency Standard Deviation </dd></dl>

</div>
</div>
<a id="a331862cfde13a009b3e4133678a1c597"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a331862cfde13a009b3e4133678a1c597">&#9670;&nbsp;</a></span>GetLatencyVector()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;int&gt; dxrt::InferenceEngine::GetLatencyVector </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get recent Latency. </p>
<dl class="section return"><dt>Returns</dt><dd>latency (microseconds) </dd></dl>

</div>
</div>
<a id="a7ed90b0eaa55ce7c097cc7163009c66e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7ed90b0eaa55ce7c097cc7163009c66e">&#9670;&nbsp;</a></span>GetModelName()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::string dxrt::InferenceEngine::GetModelName </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get model name. </p>
<dl class="section return"><dt>Returns</dt><dd>model name </dd></dl>

</div>
</div>
<a id="a1b3771241a7424353e97a0c81a28d875"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1b3771241a7424353e97a0c81a28d875">&#9670;&nbsp;</a></span>GetModelVersion()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::string dxrt::InferenceEngine::GetModelVersion </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the DXNN file format version of the model. </p>
<dl class="section return"><dt>Returns</dt><dd>The DXNN file format version as a string. </dd></dl>

</div>
</div>
<a id="a1ee64321b5d6c50cb95be282080f326d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1ee64321b5d6c50cb95be282080f326d">&#9670;&nbsp;</a></span>GetNpuInferenceTime()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">uint32_t dxrt::InferenceEngine::GetNpuInferenceTime </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get latest inference time. </p>
<dl class="section return"><dt>Returns</dt><dd>inference time (microseconds) </dd></dl>

</div>
</div>
<a id="ad7a32a700fa89a0213b815d57f8fc1c2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad7a32a700fa89a0213b815d57f8fc1c2">&#9670;&nbsp;</a></span>GetNpuInferenceTimeCnt()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::GetNpuInferenceTimeCnt </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get inference time Count. </p>
<dl class="section return"><dt>Returns</dt><dd>inference time Count </dd></dl>

</div>
</div>
<a id="a0d46e7a44cd5975c5a9c3a76be684dc3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0d46e7a44cd5975c5a9c3a76be684dc3">&#9670;&nbsp;</a></span>GetNpuInferenceTimeMean()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double dxrt::InferenceEngine::GetNpuInferenceTimeMean </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get inference time Mean. </p>
<dl class="section return"><dt>Returns</dt><dd>inference time Mean (microseconds) </dd></dl>

</div>
</div>
<a id="a4d552cb23986c189b1002a3e0b9915a9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4d552cb23986c189b1002a3e0b9915a9">&#9670;&nbsp;</a></span>GetNpuInferenceTimeStdDev()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double dxrt::InferenceEngine::GetNpuInferenceTimeStdDev </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get inference time Standard Deviation. </p>
<dl class="section return"><dt>Returns</dt><dd>inference time Standard Deviation </dd></dl>

</div>
</div>
<a id="aad79d35e5a285878517eb784439737a9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aad79d35e5a285878517eb784439737a9">&#9670;&nbsp;</a></span>GetNpuInferenceTimeVector()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;uint32_t&gt; dxrt::InferenceEngine::GetNpuInferenceTimeVector </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get recent inference time. </p>
<dl class="section return"><dt>Returns</dt><dd>inference time (microseconds) </dd></dl>

</div>
</div>
<a id="a110445fe04396f2d2ea309c18c3bb930"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a110445fe04396f2d2ea309c18c3bb930">&#9670;&nbsp;</a></span>GetNumTailTasks()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::GetNumTailTasks </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the number of tail tasks in the model. </p>
<dl class="section return"><dt>Returns</dt><dd>The number of tasks that have no subsequent tasks.</dd></dl>
<p>Tail tasks are those which do not have any tasks following them in the model's task chain. This function provides the count of such tail tasks. </p>

</div>
</div>
<a id="a411b78e6c9560119f9a39c30ee1021af"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a411b78e6c9560119f9a39c30ee1021af">&#9670;&nbsp;</a></span>GetOutputs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensors dxrt::InferenceEngine::GetOutputs </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>ptr</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint64_t&#160;</td>
          <td class="paramname"><em>phyAddr</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get output tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">ptr</td><td>pointer to virtual address </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">phyAddr</td><td>pointer to physical address </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>if ptr is null, output memory area in engine is returned </dd>
<dd>
if ptr and phyAddr is given, outputs tensors that contains output addresses </dd></dl>

</div>
</div>
<a id="a82af5685f6b43643c3ddf47bf0a4ade8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a82af5685f6b43643c3ddf47bf0a4ade8">&#9670;&nbsp;</a></span>GetOutputSize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">uint64_t dxrt::InferenceEngine::GetOutputSize </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get total size of output tensors. </p>
<dl class="section return"><dt>Returns</dt><dd>Output size of one inference in bytes </dd></dl>

</div>
</div>
<a id="a1edebbe4f7b03f042a550e822ac16284"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1edebbe4f7b03f042a550e822ac16284">&#9670;&nbsp;</a></span>GetOutputTensorNames()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;std::string&gt; dxrt::InferenceEngine::GetOutputTensorNames </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the names of all output tensors in the model. </p>
<dl class="section return"><dt>Returns</dt><dd>Vector of output tensor names in the order they are produced. </dd></dl>

</div>
</div>
<a id="aa861889fd3d562742950053868077086"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa861889fd3d562742950053868077086">&#9670;&nbsp;</a></span>GetOutputTensorOffset()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">size_t dxrt::InferenceEngine::GetOutputTensorOffset </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>tensorName</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the offset for a specific output tensor in the final output buffer. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorName</td><td>Name of the output tensor </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Offset in bytes for the tensor in the output buffer </dd></dl>

</div>
</div>
<a id="aee8bc8dd933efeed07de606fcf2fb161"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aee8bc8dd933efeed07de606fcf2fb161">&#9670;&nbsp;</a></span>GetOutputTensorSizes()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;uint64_t&gt; dxrt::InferenceEngine::GetOutputTensorSizes </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get individual output tensor sizes for multi-output models. </p>
<dl class="section return"><dt>Returns</dt><dd>Vector of output tensor sizes in bytes, in the order specified by <a class="el" href="classdxrt_1_1InferenceEngine.html#a1edebbe4f7b03f042a550e822ac16284" title="Returns the names of all output tensors in the model.">GetOutputTensorNames()</a> </dd></dl>

</div>
</div>
<a id="a8b759a7bb098d710e1c3052170057143"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8b759a7bb098d710e1c3052170057143">&#9670;&nbsp;</a></span>GetTaskOrder()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;std::string&gt; dxrt::InferenceEngine::GetTaskOrder </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get model task order. </p>
<dl class="section return"><dt>Returns</dt><dd>task order </dd></dl>

</div>
</div>
<a id="a2338c50d4d24b081179372bfea9d5f2c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2338c50d4d24b081179372bfea9d5f2c">&#9670;&nbsp;</a></span>getTensorOffset()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">uint64_t dxrt::InferenceEngine::getTensorOffset </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>tensorName</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get the offset of a tensor in the final output buffer. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">tensorName</td><td>Name of the tensor </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Offset in bytes, or 0 if tensor not found </dd></dl>

</div>
</div>
<a id="aad2f3b616e0e996b66ef7df37dc16a85"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aad2f3b616e0e996b66ef7df37dc16a85">&#9670;&nbsp;</a></span>inference_time()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">uint32_t dxrt::InferenceEngine::inference_time </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get latest inference time (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000011">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a1ee64321b5d6c50cb95be282080f326d" title="Get latest inference time.">GetNpuInferenceTime()</a> instead. <br  />
 </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>inference time (microseconds) </dd></dl>

</div>
</div>
<a id="a96c4e5953a0daf4a1c886002c052a6ba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a96c4e5953a0daf4a1c886002c052a6ba">&#9670;&nbsp;</a></span>input_size()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">uint64_t dxrt::InferenceEngine::input_size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get total size of input tensors (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000006">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#ad2c11689c4839b5dbb3571107a7b3a7c" title="Get total size of input tensors.">GetInputSize()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>Input size of one inference in bytes </dd></dl>

</div>
</div>
<a id="af81b48e8ccd95953dd9bc483e5ec678b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af81b48e8ccd95953dd9bc483e5ec678b">&#9670;&nbsp;</a></span>inputs() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;Tensors&gt; dxrt::InferenceEngine::inputs </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>devId</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get input tensor (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000004">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#abe536ab666125335e08764fa4916c157" title="Get input tensor.">GetInputs()</a> instead. </dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">devId</td><td>device id </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>vector of input tensors </dd></dl>

</div>
</div>
<a id="afbee8b031fdd6c0a3916ed00d08c9cd8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afbee8b031fdd6c0a3916ed00d08c9cd8">&#9670;&nbsp;</a></span>inputs() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">Tensors dxrt::InferenceEngine::inputs </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>ptr</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint64_t&#160;</td>
          <td class="paramname"><em>phyAddr</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get input tensor (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000003">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#abe536ab666125335e08764fa4916c157" title="Get input tensor.">GetInputs()</a> instead. </dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">ptr</td><td>pointer to virtual address </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">phyAddr</td><td>pointer to physical address </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>if ptr is null, input memory area in engine is returned </dd>
<dd>
if ptr and phyAddr is given, inputs tensors that contains output addresses </dd></dl>

</div>
</div>
<a id="a9aeb596b415843e7e318c47938444259"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9aeb596b415843e7e318c47938444259">&#9670;&nbsp;</a></span>is_PPU()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool dxrt::InferenceEngine::is_PPU </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns whether the model is using PPU. (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000016">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#ab6fd915af259467afe1681098b315dca" title="Returns whether the model is using PPU.">IsPPU()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>whether the model is using PPU. </dd></dl>

</div>
</div>
<a id="a896b05b9798d0ae33b3ba25e82e9df4c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a896b05b9798d0ae33b3ba25e82e9df4c">&#9670;&nbsp;</a></span>IsMultiInputModel()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool dxrt::InferenceEngine::IsMultiInputModel </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns whether the model has multiple input tensors. </p>
<dl class="section return"><dt>Returns</dt><dd>true if the model has multiple input tensors, false otherwise. </dd></dl>

</div>
</div>
<a id="a498ec2af6a8ebdd4e8edd26cba1faf24"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a498ec2af6a8ebdd4e8edd26cba1faf24">&#9670;&nbsp;</a></span>IsOrtConfigured()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool dxrt::InferenceEngine::IsOrtConfigured </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Checks whether ORT (ONNX Runtime) is configured. </p>
<dl class="section return"><dt>Returns</dt><dd>true if ORT is configured, false otherwise. </dd></dl>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">InvalidArgumentException</td><td>If USE_ORT is OFF but the useORT option is set to true. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ab6fd915af259467afe1681098b315dca"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab6fd915af259467afe1681098b315dca">&#9670;&nbsp;</a></span>IsPPU()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool dxrt::InferenceEngine::IsPPU </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns whether the model is using PPU. </p>
<dl class="section return"><dt>Returns</dt><dd>whether the model is using PPU. </dd></dl>

</div>
</div>
<a id="a990ca720e7f8a28ac5394206eb6f9491"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a990ca720e7f8a28ac5394206eb6f9491">&#9670;&nbsp;</a></span>latency()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::latency </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get latest latency (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000010">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a3758aeb28d7ed75216770a5fd5dd9619" title="Get latest latency.">GetLatency()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>latency (microseconds) </dd></dl>

</div>
</div>
<a id="a3fb16094508bd367cc4971238f002765"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3fb16094508bd367cc4971238f002765">&#9670;&nbsp;</a></span>name()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::string dxrt::InferenceEngine::name </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get model name (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000008">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a7ed90b0eaa55ce7c097cc7163009c66e" title="Get model name.">GetModelName()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>model name </dd></dl>

</div>
</div>
<a id="a93e1d7b460b7f8656e725ed708de507b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a93e1d7b460b7f8656e725ed708de507b">&#9670;&nbsp;</a></span>output_size()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">uint64_t dxrt::InferenceEngine::output_size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get total size of output tensors (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000007">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a82af5685f6b43643c3ddf47bf0a4ade8" title="Get total size of output tensors.">GetOutputSize()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>Output size of one inference in bytes </dd></dl>

</div>
</div>
<a id="a0893b724d6eed939556f1315ce05829d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0893b724d6eed939556f1315ce05829d">&#9670;&nbsp;</a></span>outputs()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">Tensors dxrt::InferenceEngine::outputs </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>ptr</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint64_t&#160;</td>
          <td class="paramname"><em>phyAddr</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get output tensor (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000005">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a411b78e6c9560119f9a39c30ee1021af" title="Get output tensor.">GetOutputs()</a> instead. </dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">ptr</td><td>pointer to virtual address </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">phyAddr</td><td>pointer to physical address </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>if ptr is null, output memory area in engine is returned </dd>
<dd>
if ptr and phyAddr is given, outputs tensors that contains output addresses </dd></dl>

</div>
</div>
<a id="a3d3fc75faa79470d9a3bdbdc61561a3a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3d3fc75faa79470d9a3bdbdc61561a3a">&#9670;&nbsp;</a></span>RegisterCallBack()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void dxrt::InferenceEngine::RegisterCallBack </td>
          <td>(</td>
          <td class="paramtype">std::function&lt; int(TensorPtrs &amp;<a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a>, void *userArg)&gt;&#160;</td>
          <td class="paramname"><em>callbackFunc</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Register user callback function to be called by inference completion. (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000002">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a1d8b33fa324a48b43433c401bd54280a" title="Register user callback function to be called by inference completion.">RegisterCallback()</a> instead. </dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">callbackFunc</td><td>Function which is called when inference is complete, it gets outputs and user_arg ptr </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">outputs</td><td>output tensors data </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">userArg</td><td>userArg given by <a class="el" href="classdxrt_1_1InferenceEngine.html#a88cea5324cbeb436034adb60cf31050f" title="Run inference engine using specific input pointer Synchronously.">Run()</a>; </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a1d8b33fa324a48b43433c401bd54280a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1d8b33fa324a48b43433c401bd54280a">&#9670;&nbsp;</a></span>RegisterCallback()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void dxrt::InferenceEngine::RegisterCallback </td>
          <td>(</td>
          <td class="paramtype">std::function&lt; int(TensorPtrs &amp;<a class="el" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a>, void *userArg)&gt;&#160;</td>
          <td class="paramname"><em>callbackFunc</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Register user callback function to be called by inference completion. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">callbackFunc</td><td>Function which is called when inference is complete, it gets outputs and user_arg ptr </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">outputs</td><td>output tensors data </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">userArg</td><td>userArg given by <a class="el" href="classdxrt_1_1InferenceEngine.html#a88cea5324cbeb436034adb60cf31050f" title="Run inference engine using specific input pointer Synchronously.">Run()</a>; </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ab54d666cc65cef4d6ae2fe4c300d7ba1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab54d666cc65cef4d6ae2fe4c300d7ba1">&#9670;&nbsp;</a></span>Run() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;TensorPtrs&gt; dxrt::InferenceEngine::Run </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>inputBuffers</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>outputBuffers</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>userArgs</em> = <code>{}</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Runs the inference engine using a specific input pointer vector. </p>
<p>This function executes inference based on the provided input data pointer vector and returns a vector of output tensors. Users can optionally provide additional user-defined arguments and output pointer vectors. If userArgs is used, the number of elements in inputPtrs must be the same as the number of elements in userArgs. An InvalidArgumentException is thrown if the size of inputPtrs and userArgs are different. An InvalidArgumentException is thrown if the size of inputPtrs and OutputPtrs are different. An InvalidArgumentException is thrown if the size of inputPtrs is 0.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputBuffers</td><td>Vector of input data pointers used for inference. </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputBuffers</td><td>Vector of output data pointers. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArgs</td><td>Vector of user-defined arguments (e.g., original frame data, input metadata, etc.). (Optional)</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Vector of output tensors as smart pointer instances. </dd></dl>

</div>
</div>
<a id="a88cea5324cbeb436034adb60cf31050f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a88cea5324cbeb436034adb60cf31050f">&#9670;&nbsp;</a></span>Run() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::Run </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userArg</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>outputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Run inference engine using specific input pointer Synchronously. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>input data pointer to run inference </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArg</td><td>user-defined arguments as a void pointer(e.g. original frame data, metadata about input, ... ) </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputPtr</td><td>pointer to output data, if it is nullptr, output data is stored in buffer inside DXRT. <div class="fragment"><div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model.dxnn&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model.dxnn&quot;</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a> ie(modelPath);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
</div><!-- fragment --> </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>output tensors as vector of smart pointer instances </dd></dl>

</div>
</div>
<a id="ac9c8166a8c588860ea4a5fc29d743eb5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac9c8166a8c588860ea4a5fc29d743eb5">&#9670;&nbsp;</a></span>RunAsync() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::RunAsync </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>inputPtrs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userArg</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>outputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Non-blocking call to request asynchronous inference with automatic multi-input detection. This function automatically detects whether the input should be interpreted as multi-input single inference or batch inference based on the model requirements and input count. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtrs</td><td>Vector of input data pointers </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArg</td><td>user-defined arguments as a void pointer </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputPtr</td><td>pointer to output data, if it is nullptr, output data area is allocated by DXRT. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>job id that can be used to wait() function </dd></dl>

</div>
</div>
<a id="af3006c9a7183f7e42d1528aa8a6fea24"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af3006c9a7183f7e42d1528aa8a6fea24">&#9670;&nbsp;</a></span>RunAsync() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::RunAsync </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userArg</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>outputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Non-blocking call to request asynchronous inference by input pointer, and get job ID from inference engine. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>input data pointer to run inference </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArg</td><td>user-defined arguments as a void pointer(e.g. original frame data, metadata about input, ... ) </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputPtr</td><td>pointer to output data, if it is nullptr, output data area is allocated by DXRT. <div class="fragment"><div class="line"><span class="keyword">auto</span> modelPath = <span class="stringliteral">&quot;model&quot;</span>; <span class="comment">// assume compiled model path name is &quot;model&quot;</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a> option;</div>
<div class="line">option.<a class="code" href="classdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">devices</a> = {0,1,3};  <span class="comment">//use only 0,1,3 device</span></div>
<div class="line"><a class="code" href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a> ie(modelPath, option);</div>
<div class="line"><span class="keyword">auto</span> <a class="code" href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">outputs</a> = ie.Run();</div>
</div><!-- fragment --> </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>job id that can be used to wait() function </dd></dl>

</div>
</div>
<a id="a6a5b11e470a9dfeca6c60f644726ea59"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6a5b11e470a9dfeca6c60f644726ea59">&#9670;&nbsp;</a></span>RunAsyncMultiInput() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::RunAsyncMultiInput </td>
          <td>(</td>
          <td class="paramtype">const std::map&lt; std::string, void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>inputTensors</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userArg</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>outputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Run async inference with multiple input tensors for multi-input models. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputTensors</td><td>Map of tensor name to input data pointer </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArg</td><td>user-defined arguments as a void pointer </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputPtr</td><td>pointer to output data, if it is nullptr, output data is stored in buffer inside DXRT. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>job id that can be used to wait() function </dd></dl>

</div>
</div>
<a id="a8a61dde8d97418d77e5bfd348011517f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8a61dde8d97418d77e5bfd348011517f">&#9670;&nbsp;</a></span>RunAsyncMultiInput() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int dxrt::InferenceEngine::RunAsyncMultiInput </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>inputPtrs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userArg</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>outputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Run async inference with multiple input tensors (vector format) for multi-input models. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtrs</td><td>Vector of input data pointers in the order specified by <a class="el" href="classdxrt_1_1InferenceEngine.html#a41f090eff09ddb3e3963666eb507d7dc" title="Returns the names of all input tensors in the model.">GetInputTensorNames()</a> </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArg</td><td>user-defined arguments as a void pointer </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputPtr</td><td>pointer to output data, if it is nullptr, output data is stored in buffer inside DXRT. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>job id that can be used to wait() function </dd></dl>

</div>
</div>
<a id="ab0c6629f048341d62bfa3a400c90e827"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab0c6629f048341d62bfa3a400c90e827">&#9670;&nbsp;</a></span>RunBenchMark()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">float dxrt::InferenceEngine::RunBenchMark </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>run benchmark with loop n times (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000001">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a3846a2f47ea39933136e589aad036eb0" title="run benchmark with loop n times">RunBenchmark()</a> instead. </dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">num</td><td>number of inferences </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>input data pointer to run inference </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>average fps </dd></dl>

</div>
</div>
<a id="a3846a2f47ea39933136e589aad036eb0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3846a2f47ea39933136e589aad036eb0">&#9670;&nbsp;</a></span>RunBenchmark()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">float dxrt::InferenceEngine::RunBenchmark </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>run benchmark with loop n times </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">num</td><td>number of inferences </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>input data pointer to run inference </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>average fps </dd></dl>

</div>
</div>
<a id="aff7eaeecfb9fbf7aa1ea8e27d0572b92"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aff7eaeecfb9fbf7aa1ea8e27d0572b92">&#9670;&nbsp;</a></span>RunMultiInput() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::RunMultiInput </td>
          <td>(</td>
          <td class="paramtype">const std::map&lt; std::string, void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>inputTensors</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userArg</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>outputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Run inference engine with multiple input tensors for multi-input models. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputTensors</td><td>Map of tensor name to input data pointer </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArg</td><td>user-defined arguments as a void pointer </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputPtr</td><td>pointer to output data, if it is nullptr, output data is stored in buffer inside DXRT. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>output tensors as vector of smart pointer instances </dd></dl>

</div>
</div>
<a id="a4b5c6a8aa5db938b8b50b7d9c844b367"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4b5c6a8aa5db938b8b50b7d9c844b367">&#9670;&nbsp;</a></span>RunMultiInput() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::RunMultiInput </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>inputPtrs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userArg</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>outputPtr</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Run inference engine with multiple input tensors (vector format) for multi-input models. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtrs</td><td>Vector of input data pointers in the order specified by <a class="el" href="classdxrt_1_1InferenceEngine.html#a41f090eff09ddb3e3963666eb507d7dc" title="Returns the names of all input tensors in the model.">GetInputTensorNames()</a> </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">userArg</td><td>user-defined arguments as a void pointer </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputPtr</td><td>pointer to output data, if it is nullptr, output data is stored in buffer inside DXRT. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>output tensors as vector of smart pointer instances </dd></dl>

</div>
</div>
<a id="aa227ad7501e794726b9af17e88418cf1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa227ad7501e794726b9af17e88418cf1">&#9670;&nbsp;</a></span>supportsTensorCentricOffsets()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool dxrt::InferenceEngine::supportsTensorCentricOffsets </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Check if tensor-centric offset calculation is supported. </p>
<dl class="section return"><dt>Returns</dt><dd>true if supported, false otherwise </dd></dl>

</div>
</div>
<a id="a578a0098f9b49902fa4c81d95945baa4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a578a0098f9b49902fa4c81d95945baa4">&#9670;&nbsp;</a></span>task_order()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;std::string&gt; dxrt::InferenceEngine::task_order </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get model task order (Legacy API) </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000009">Deprecated:</a></b></dt><dd>Use <a class="el" href="classdxrt_1_1InferenceEngine.html#a8b759a7bb098d710e1c3052170057143" title="Get model task order.">GetTaskOrder()</a> instead. </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>task order </dd></dl>

</div>
</div>
<a id="a2cbf5e6220f5be2cd9132f2570129faf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2cbf5e6220f5be2cd9132f2570129faf">&#9670;&nbsp;</a></span>ValidateDevice() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::ValidateDevice </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>inputPtrs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>deviceId</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Validate inference of a specific NPU device with automatic multi-input detection. This function automatically detects whether the input should be interpreted as multi-input based on the model requirements and input count. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtrs</td><td>Vector of input data pointers for validation. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">deviceId</td><td>ID of the NPU device to validate. Default is 0 (first device). </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Output tensors as a vector of smart pointer instances, representing the validation results. </dd></dl>

</div>
</div>
<a id="aef4c9502a5d5a9d256d3899391376aba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aef4c9502a5d5a9d256d3899391376aba">&#9670;&nbsp;</a></span>ValidateDevice() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::ValidateDevice </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>inputPtr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>deviceId</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Validate inference of a specific NPU device connected to the host. This function runs a validation process using the provided input data on the specified NPU device. It can be used to ensure that the NPU device is operational and can process inference tasks correctly. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtr</td><td>Pointer to the input data used for validation. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">deviceId</td><td>ID of the NPU device to validate. Default is 0 (first device). </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Output tensors as a vector of smart pointer instances, representing the validation results. </dd></dl>

</div>
</div>
<a id="a5b6d78e715a91712548b905d2c730a0c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5b6d78e715a91712548b905d2c730a0c">&#9670;&nbsp;</a></span>ValidateDeviceMultiInput() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::ValidateDeviceMultiInput </td>
          <td>(</td>
          <td class="paramtype">const std::map&lt; std::string, void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>inputTensors</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>deviceId</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Validate NPU device with multiple input tensors for multi-input models. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputTensors</td><td>Map of tensor name to input data pointer </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">deviceId</td><td>ID of the NPU device to validate. Default is 0 (first device). </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Output tensors as a vector of smart pointer instances, representing the validation results. </dd></dl>

</div>
</div>
<a id="af252439b3ba07ea017792c4fe6d82c7a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af252439b3ba07ea017792c4fe6d82c7a">&#9670;&nbsp;</a></span>ValidateDeviceMultiInput() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::ValidateDeviceMultiInput </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; void * &gt; &amp;&#160;</td>
          <td class="paramname"><em>inputPtrs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>deviceId</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Validate NPU device with multiple input tensors (vector format) for multi-input models. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">inputPtrs</td><td>Vector of input data pointers in the order specified by <a class="el" href="classdxrt_1_1InferenceEngine.html#a41f090eff09ddb3e3963666eb507d7dc" title="Returns the names of all input tensors in the model.">GetInputTensorNames()</a> </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">deviceId</td><td>ID of the NPU device to validate. Default is 0 (first device). </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Output tensors as a vector of smart pointer instances, representing the validation results. </dd></dl>

</div>
</div>
<a id="a2c6f130d722950153287d4cf1efd5a5d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2c6f130d722950153287d4cf1efd5a5d">&#9670;&nbsp;</a></span>Wait()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">TensorPtrs dxrt::InferenceEngine::Wait </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>jobId</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Wait until an request is complete and returns output. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">jobId</td><td>job Id returned by <a class="el" href="classdxrt_1_1InferenceEngine.html#af3006c9a7183f7e42d1528aa8a6fea24" title="Non-blocking call to request asynchronous inference by input pointer, and get job ID from inference e...">RunAsync()</a> </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>output tensors as vector of smart pointer instances </dd></dl>

</div>
</div>
</div><!-- contents -->
<div class="ttc" id="aclassdxrt_1_1InferenceOption_html_ae98837ff566fe2b089a5b1bf07feb48c"><div class="ttname"><a href="classdxrt_1_1InferenceOption.html#ae98837ff566fe2b089a5b1bf07feb48c">dxrt::InferenceOption::devices</a></div><div class="ttdeci">std::vector&lt; int &gt; devices</div><div class="ttdoc">device ID list to use</div><div class="ttdef"><b>Definition:</b> inference_option.h:37</div></div>
<div class="ttc" id="aclassdxrt_1_1InferenceOption_html"><div class="ttname"><a href="classdxrt_1_1InferenceOption.html">dxrt::InferenceOption</a></div><div class="ttdoc">This struct specifies inference options applied to dxrt::InferenceEngine.</div><div class="ttdef"><b>Definition:</b> inference_option.h:21</div></div>
<div class="ttc" id="aclassdxrt_1_1InferenceEngine_html_a0893b724d6eed939556f1315ce05829d"><div class="ttname"><a href="classdxrt_1_1InferenceEngine.html#a0893b724d6eed939556f1315ce05829d">dxrt::InferenceEngine::outputs</a></div><div class="ttdeci">Tensors outputs(void *ptr=nullptr, uint64_t phyAddr=0)</div><div class="ttdoc">Get output tensor (Legacy API)</div><div class="ttdef"><b>Definition:</b> inference_engine.h:287</div></div>
<div class="ttc" id="aclassdxrt_1_1InferenceOption_html_a9fd270a5eeeb1a14ce5f133c16c30d55"><div class="ttname"><a href="classdxrt_1_1InferenceOption.html#a9fd270a5eeeb1a14ce5f133c16c30d55">dxrt::InferenceOption::boundOption</a></div><div class="ttdeci">uint32_t boundOption</div><div class="ttdoc">Select the NPU core inside the device.</div><div class="ttdef"><b>Definition:</b> inference_option.h:42</div></div>
<div class="ttc" id="aclassdxrt_1_1InferenceEngine_html"><div class="ttname"><a href="classdxrt_1_1InferenceEngine.html">dxrt::InferenceEngine</a></div><div class="ttdoc">This class abstracts runtime inference executor for user's compiled model.</div><div class="ttdef"><b>Definition:</b> inference_engine.h:64</div></div>
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
